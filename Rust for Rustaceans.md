# Rust for Rustaceans

## Rust

### 致谢

- 由于以前从未写过书，我对这个过程的期望很少。我天真地以为它会像写一系列博客文章，或者像写广泛的文档一样，但写一本书是完全不同的事情。我花费了无数个小时进行研究、规划、写作、重写、丢弃、反思和编辑。我非常感激我的女朋友Talia在我为这个项目工作的许多个深夜中所表现出的支持和耐心——没有你，写作体验会变得更加黯淡。这本书绝不可能没有令人难以置信的Rust开发者和社区的支持，他们开发了一门语言和生态系统，我继续发现与之互动是一种乐趣，并受到启发，以我最好的能力来传播和教授。同样感谢多年来观看我的直播的了不起的人们；如果没有你们持续的支持、鼓励和无尽的好奇心，我想我永远不会有机会首先写这本书。

- 如果没有David Tolnay作为本书的技术审查者，这本书也不会有一半的好处。David在找出理论和代码错误方面显然是无价的，但他的丰富经验、注重细节和爱好教学真正留下了深刻的印象。他的深思熟虑和富有洞察力的评论有时让我决定重写整个章节，但总是以比之前更好得多的方式进行。
- 同样感谢我的编辑Liz Chadwick和No Starch出版团队的其他成员。看到Liz在书的编写过程中的成长过程非常有趣；她在阅读过程中学习了Rust，并且当她的评论表明她真正理解了中级内容时，我感到非常高兴。每当她遇到不理解的地方时的讨论总是启发人，导致了更易理解和全面的解释。
- 我也不能不提到Steve Klabnik和Carol Nichols，他们是《Rust编程语言》的作者，这是我对Rust的第一次介绍。在我看来，这本书在很大程度上是他们书的续集，如果没有他们对Rust基础知识的出色解释和易于理解的工作，这本书是无法存在的。
最后，我要向你们致敬。是的，就是你们！写这本书是一个非常漫长的过程，而正是那些希望阅读它的人们的支持和鼓励使我一直坚持下去。正是像你们这样的人，无论是虚拟还是实体地阅读这本书，都希望提高自己的理解和技能，驱使我尽我所能为Rust教学资源的积累做出贡献。
谢谢大家！

### 介绍

- **无论使用哪种编程语言，入门教材所教授的知识与多年实践经验后所掌握的知识之间的差距总是很大。随着时间的推移，你会熟悉惯用法，对核心概念有更好的心智模型，了解哪些设计和模式有效，哪些无效，并发现周围生态系统中有用的库和工具。综合起来，这种经验使你能够在更短的时间内编写更好的代码**

- 通过这本书，我希望将我多年编写Rust代码的经验浓缩成一本易于理解的资源。《Rust for Rustaceans》是《Rust编程语言》（即Rust书）的延续，但适用于任何想要超越基础知识的Rust程序员，无论你是在哪里学习这门语言。本书深入探讨了诸如不安全代码、特质系统、no_std代码和宏等概念。它还涵盖了新领域，如异步I/O、测试、嵌入式开发和人性化的API设计。我旨在解释和揭示Rust的这些更高级和强大的特性，并使您能够构建更快、更人性化和更健壮的应用程序。

### What’s in the Book

- 本书既是指南，也是参考资料。各章节相对独立，因此您可以直接跳转到特别感兴趣的主题（或者当前困扰您的主题），或者从头到尾阅读本书以获得更全面的体验。尽管如此，我建议您从阅读第1章和第2章开始，因为它们为后续章节和日常Rust开发中的许多主题奠定了基础。以下是每个章节的简要介绍：
  - 第1章《基础知识》深入描述了Rust的基本概念，如变量、内存、所有权、借用和生命周期，您需要熟悉这些概念才能理解本书的其余部分。
  - 第2章《类型》类似地提供了关于Rust中类型和特质的更详尽解释，包括编译器如何推理它们、它们的特性和限制，以及一些高级应用。
  - 第3章《设计接口》介绍了如何设计直观、灵活和防止误用的API，包括如何命名事物、如何使用类型系统来强制执行API合约，以及何时使用泛型和特质对象。
  - 第4章《错误处理》探讨了两种主要类型的错误（枚举和不透明错误），以及何时使用每种类型的适当情况，以及如何定义、构造、传播和处理这些错误。
  - 第5章《项目结构》关注Rust项目的非代码部分，例如Cargo元数据和配置、crate特性和版本控制。
  - 第6章《测试》详细介绍了标准的Rust测试框架的工作原理，并介绍了一些超出标准单元测试和集成测试的测试工具和技术，例如模糊测试和性能测试。
  - 第7章《宏》涵盖了声明式和过程式宏，包括它们的编写方式、它们的用途以及一些注意事项。
  - 第8章《异步编程》介绍了同步和异步接口之间的区别，然后深入探讨了在Rust中如何表示异步性，包括Future和Pin的底层表示以及async和await的高级表示。该章还解释了异步执行器的作用以及它如何使整个异步机制协同工作。
  - 第9章《不安全代码》解释了unsafe关键字所解锁的强大功能以及随之而来的巨大责任。您将了解到在不安全代码中常见的陷阱，以及可以用来减少不正确不安全代码风险的工具和技术。
  - 第10章《并发（和并行）》介绍了Rust中如何表示并发以及为什么在正确性和性能方面很难做到正确。它涵盖了并发和异步性的关系（但并不相同），以及在接近硬件时并发如何工作，以及在尝试编写正确并发程序时如何保持理智。
  - 第11章《外部函数接口》教你如何使Rust与其他语言良好地协作，以及extern关键字等FFI原语的实际作用。
  - 第12章《无标准库的Rust》介绍了在无法使用完整标准库的情况下使用Rust的情况，例如在嵌入式设备或其他受限平台上，您只能使用核心和alloc模块提供的功能。
  - 第13章《Rust生态系统》不涵盖特定的Rust主题，而是旨在提供关于在Rust生态系统中工作的更广泛指导。它包含常见设计模式的描述，关于保持对语言添加和最佳实践的最新信息的建议，有用工具的提示以及我多年来积累的其他有用的琐事，这些信息在任何单一地方都没有描述。
该书有一个网站，网址为<https://rust-for-rustaceans.com，其中包含书中的资源链接、未来的勘误等。您还可以在No> Starch Press网站上的该书页面<https://nostarch.com/rust-rustaceans/找到这些信息。>
现在，所有这些都已经解释清楚了，只剩下一件事要做：

    ```rust
    fn main() {}
    ```

### 1 基础知识

- **当你深入研究Rust的更高级领域时，确保你对基础知识有扎实的理解非常重要。在Rust中，就像在任何编程语言中一样，各种关键字和概念的确切含义在你开始以更复杂的方式使用语言时变得重要**。在本章中，我们将详细介绍Rust的许多基本概念，试图更清楚地定义它们的含义、工作原理以及它们为什么恰好是这样。具体而言，我们将了解变量和值的区别，它们如何在内存中表示，以及程序所具有的不同内存区域。然后，我们将讨论一些关于所有权、借用和生命周期的细微差别，这些是您在继续阅读本书之前需要掌握的。
- 如果你愿意，你可以从头到尾阅读本章，或者你可以将其作为参考，复习你对其中一些概念不太确定的内容。我建议你只有在对本章内容感到完全舒适时再继续前进，因为对这些基本概念的误解会很快妨碍你理解更高级的主题，或者导致你错误地使用它们。

#### Talking About Memory

并非所有的内存都是相同的。在大多数编程环境中，你的程序可以访问堆栈、堆、寄存器、文本段、内存映射寄存器、内存映射文件，以及可能的非易失性RAM。你在特定情况下选择使用哪种内存区域会对你可以在其中存储什么、它能保持多长时间可访问以及你使用什么机制来访问它产生影响。这些内存区域的具体细节因平台而异，超出了本书的范围，但其中一些对于你如何推理Rust代码非常重要，因此值得关注。
covering here.

##### Memory Terminology

在我们深入研究内存区域之前，您首先需要了解值、变量和指针之间的区别。在Rust中，值是类型和该类型值域中的元素的组合。可以使用值的类型表示将值转换为字节序列，但单独来看，您可以将值视为您作为程序员所指的内容。例如，类型为u8的数字6是数学整数6的一个实例，其内存表示为字节0x06。类似地，字符串"Hello world"是所有字符串域中的一个值，其表示为其UTF-8编码。值的含义与存储这些字节的位置无关。
值存储在一个位置中，这是Rust术语中“可以容纳值的位置”。这个位置可以在堆栈、堆或其他位置上。存储值的最常见位置是变量，它是堆栈上的一个命名值槽。
指针是一个保存内存区域地址的值，因此指针指向一个位置。可以解引用指针以访问存储在其指向的内存位置中的值。我们可以将相同的指针存储在多个变量中，因此可以有多个间接引用相同内存位置和相同基础值的变量。
请考虑代码清单1-1中的代码，它说明了这三个元素。

```rust
let x = 42;
let y = 43;
let var1 = &x;
let mut var2 = &x;
1 var2 = &y;
```

Listing 1-1: Values, variables, and pointers

在这里，有四个不同的值：42（一个i32），43（一个i32），x的地址（一个指针）和y的地址（一个指针）。还有四个变量：x，y，var1和var2。后两个变量都保存指针类型的值，因为引用就是指针。虽然var1和var2最初存储相同的值，但它们存储了独立的副本；当我们改变var2中存储的值时，var1中的值不会改变。特别是，=运算符将右侧表达式的值存储在左侧命名的位置中。
在变量、值和指针之间的区别变得重要的一个有趣例子是这样一个语句：

```rust
let string = "Hello world";
```

即使我们将字符串值赋给变量string，实际上变量的值是指向字符串值"Hello world"中第一个字符的指针，而不是字符串值本身。此时你可能会说：“但是等等，那么字符串值存储在哪里？指针指向哪里？”如果是这样，你的观察力很敏锐，我们马上就会讨论这个问题。

**注意** 从技术上讲，变量string的值还包括字符串的长度。我们将在第2章讨论宽指针类型时详细介绍这一点。

##### 变量的深入理解

在高级模型中，我们不将变量视为保存字节的位置。相反，我们将它们仅视为在程序中实例化、移动和使用时赋予值的名称。当将一个值赋给一个变量时，该值从那时起就由该变量命名。当稍后访问一个变量时，你可以想象从该变量的先前访问到新访问之间画一条线，从而建立两个访问之间的依赖关系。如果变量中的值被移动，就不能再从它画线了。

- 在这个模型中，变量只存在于它持有合法值的时间内；你不能从一个值未初始化或已被移动的变量画线，所以它实际上不存在。使用这个模型，你的整个程序由许多这些依赖线组成，通常被称为流，每个流追踪一个特定值的生命周期。当存在分支时，流可以分叉和合并，每个分叉追踪该值的不同生命周期。编译器可以检查在程序的任何给定点，所有可以并行存在的流都是兼容的。例如，不能有两个并行流同时对一个值进行可变访问。也不能在没有拥有该值的流的情况下借用一个值。清单1-2展示了这两种情况的示例。

##### 高级模型

在高级模型中，我们不将变量视为保存字节的位置。相反，我们将它们仅视为在程序中实例化、移动和使用时赋予值的名称。当将一个值赋给一个变量时，该值从那时起就由该变量命名。当稍后访问一个变量时，你可以想象从该变量的先前访问到新访问之间画一条线，从而建立两个访问之间的依赖关系。如果变量中的值被移动，就不能再从它画线了。

- 在这个模型中，变量只存在于它持有合法值的时间内；你不能从一个值未初始化或已被移动的变量画线，所以它实际上不存在。使用这个模型，你的整个程序由许多这些依赖线组成，通常被称为流，每个流追踪一个特定值的生命周期。当存在分支时，流可以分叉和合并，每个分叉追踪该值的不同生命周期。编译器可以检查在程序的任何给定点，所有可以并行存在的流都是兼容的。例如，不能有两个并行流同时对一个值进行可变访问。也不能在没有拥有该值的流的情况下借用一个值。清单1-2展示了这两种情况的示例。

```rust
let mut x;
// this access would be illegal, nowhere to draw the flow from:
// assert_eq!(x, 42);
1 x = 42;
// this is okay, can draw a flow from the value assigned above:
2 let y = &x;
// this establishes a second, mutable flow from x:
3 x = 43;
// this continues the flow from y, which in turn draws from x.
// but that flow conflicts

```

清单1-2：借用检查器将捕获的非法流

- 首先，我们不能在初始化之前使用x，因为我们没有地方可以从中绘制流。只有当我们给x赋值时，我们才能从中绘制流。这段代码有两个流：一个是从1到3的独占(&mut)流，另一个是从1经过2到4的共享(&)流。借用检查器检查每个流的每个顶点，并检查是否存在其他不兼容的流同时存在。在这种情况下，当借用检查器检查3处的独占流时，它看到终止于4的共享流。由于不能同时对一个值进行独占和共享使用，借用检查器（正确地）拒绝了这段代码。请注意，如果4不存在，这段代码将编译成功！共享流将在2处终止，当检查3处的独占流时，不会存在冲突的流。
- 如果使用与先前变量相同的名称声明一个新变量，它们仍然被视为不同的变量。这被称为遮蔽，后面的变量通过相同的名称“遮蔽”了前面的变量。这两个变量共存，尽管后续的代码不再有办法命名前面的变量。这个模型大致符合编译器的工作方式，特别是借用检查器，实际上在编译器内部用于生成高效的代码。

##### 低级模型

变量是可能或可能不持有合法值的内存位置。您可以将变量视为“值槽”。当您对其赋值时，槽被填充，其旧值（如果有）被丢弃并替换。当您访问它时，编译器会检查槽是否为空，因为这意味着变量未初始化或其值已被移动。指向变量的指针指向变量的后备内存，并可以通过解引用来访问其值。例如，在语句let x: usize中，变量x是堆栈上的一个内存区域的名称，该区域可以容纳一个usize大小的值，尽管它没有定义明确的值（其槽为空）。如果您将一个值赋给该变量，例如x = 6，那么该内存区域将保存表示值6的位。当您对x进行赋值时，&x不会改变。如果您使用相同的名称声明多个变量，它们最终将具有不同的内存块支持。这个模型与C和C++以及许多其他低级语言使用的内存模型相匹配，并且在需要明确处理内存时非常有用。

##### 栈

栈是程序用作函数调用的临时空间的一段内存。每次调用函数时，栈的顶部都会分配一个连续的内存块，称为帧。在栈的底部附近是主函数的帧，当函数调用其他函数时，额外的帧会被推入栈中。一个函数的帧包含该函数内的所有变量，以及函数接受的任何参数。当函数返回时，它的栈帧会被回收。

- 构成函数局部变量值的字节不会立即被清除，但是不安全访问它们是不安全的，因为它们可能已经被后续的函数调用覆盖，这些函数的帧与被回收的帧重叠。即使它们没有被覆盖，它们可能包含不合法使用的值，例如在函数返回时移动的值。
- 栈帧以及它们最终消失的事实与Rust中的生命周期概念密切相关。存储在栈帧上的任何变量在该帧消失后无法访问，因此对它的任何引用的生命周期最长不能超过帧的生命周期。

##### 堆

堆是一个与程序当前调用栈无关的内存池。堆内存中的值会一直存在，直到显式释放。当您希望一个值的生命周期超过当前函数帧的生命周期时，这非常有用。如果该值是函数的返回值，调用函数可以在其堆栈上留出一些空间，以便被调用函数在返回之前将该值写入其中。但是，如果您想将该值发送到与当前线程可能完全不共享堆栈帧的其他线程，您可以将其存储在堆上。

- 堆允许您显式分配连续的内存段。当您这样做时，您会得到指向该内存段开头的指针。该内存段为您保留，直到您稍后将其释放；这个过程通常被称为释放，以C标准库中相应函数的名称命名。由于从堆分配的内存在函数返回时不会消失，因此您可以在一个地方为一个值分配内存，将指针传递给另一个线程，并且该线程可以安全地继续操作该值。或者，换句话说，当您在堆上分配内存时，得到的指针具有无约束的生命周期 - 只要您的程序保持其存活，它就可以使用。
- 在Rust中与堆交互的主要机制是Box类型。当您编写Box::new(value)时，该值被放置在堆上，而您得到的`（Box<T>）`是指向堆上该值的指针。当Box最终被丢弃时，该内存将被释放。
- 如果您忘记释放堆内存，它将永远存在，并且您的应用程序最终会占用机器上的所有内存。这被称为内存泄漏，通常是您要避免的情况。但是，有些情况下您明确希望泄漏内存。例如，假设您有一个只读配置，整个程序都应该能够访问。您可以在堆上分配它，并使用Box::leak显式泄漏它，以获得对它的'static引用。

##### 静态内存

静态内存实际上是一个统称，用于描述程序编译后所在文件中的几个相关区域。当程序执行时，这些区域会自动加载到程序的内存中。静态内存中的值在整个程序执行期间都存在。程序的静态内存包含程序的二进制代码，通常被映射为只读。当程序执行时，它会按照指令逐步遍历文本段中的二进制代码，并在调用函数时跳转。静态内存还保存了使用static关键字声明的变量的内存，以及代码中的某些常量值，比如字符串。

- 特殊的生命周期'static得名于静态内存区域，它将引用标记为“在静态内存存在的时间内有效”，即直到程序关闭。由于静态变量的内存在程序启动时分配，对静态内存中变量的引用在定义上是'static的，因为它在程序关闭之前不会被释放。反之则不成立——可能存在指向静态内存之外的'static引用，但这个名称仍然适用：一旦创建了具有静态生命周期的引用，对于程序的其余部分来说，它指向的内容就好像在静态内存中一样，可以在程序希望的任何时间内使用。

- 在使用Rust时，您会经常遇到'static生命周期，而不是真正的静态内存（例如通过static关键字）。这是因为在处理Rust时，'static经常出现在类型参数的trait约束中。类似于T: 'static这样的约束表示类型参数T能够在我们保留它的时间内存活，直到程序的剩余执行结束。实质上，这个约束要求T是拥有所有权且自给自足的，要么它不借用其他（非静态）值，要么它借用的任何值也是'static的，因此会一直存在直到程序结束。一个很好的'static约束的例子是std::thread::spawn函数，它创建一个新线程，要求传递给它的闭包是'static的。由于新线程可能比当前线程存在更长的时间，新线程不能引用旧线程堆栈上的任何内容。新线程只能引用在其整个生命周期内都存在的值，这可能是程序剩余时间的持续时间。

**注意** 你可能想知道const与static有什么区别。const关键字声明了以下项为常量。常量项可以在编译时完全计算，并且在编译期间，任何引用它们的代码都会被替换为常量的计算值。常量没有与之关联的内存或其他存储（它不是一个位置）。你可以将常量视为特定值的方便名称。

##### 所有权

Rust的内存模型以所有值都有一个单一所有者的概念为中心，也就是说，每个值都有一个负责最终释放它的位置（通常是一个作用域）。这是通过借用检查器来实现的。如果值被移动，例如通过将其赋值给一个新变量、将其推入向量或将其放置在堆上，那么该值的所有权将从旧位置移动到新位置。在那时，您不能再通过从原始所有者流动的变量访问该值，即使构成该值的位实际上仍然存在。相反，您必须通过引用其新位置的变量来访问移动的值。

- 有些类型是特例，不遵循这个规则。如果一个值的类型实现了特殊的Copy trait，即使它被重新分配到一个新的内存位置，该值也不被认为已经移动。相反，该值被复制，旧的和新的位置仍然可访问。实质上，在移动的目标位置构造了另一个相同的值的实例。Rust中的大多数原始类型，如整数和浮点类型，都是Copy的。要成为Copy，必须能够通过复制其位来复制类型的值。这排除了所有包含非Copy类型的类型，以及在值被丢弃时必须释放资源的任何类型。
- 为了看清楚，考虑一下如果Box这样的类型是Copy会发生什么。如果我们执行box2 = box1，那么box1和box2都会认为它们拥有为box分配的堆内存，并且它们都会在作用域结束时尝试释放它。释放内存两次可能会产生灾难性的后果。
当一个值的所有者不再使用它时，所有者有责任通过丢弃它来进行任何必要的清理。在Rust中，当保存值的变量不再在作用域内时，丢弃会自动发生。类型通常会递归地丢弃它们包含的值，因此丢弃一个复杂类型的变量可能导致许多值被丢弃。
- 由于Rust的离散所有权要求，我们不能意外地多次丢弃相同的值。持有对另一个值的引用的变量不拥有该值，因此当变量丢弃时，该值不会被丢弃。
清单1-3中的代码简要总结了所有权、移动和复制语义以及丢弃的规则。

```rust
let x1 = 42;
let y1 = Box::new(84);
{ // starts a new scope
1 let z = (x1, y1);
// z goes out of scope, and is dropped;
// it in turn drops the values from x1 and y1
2 }
// x1's value is Copy, so it was not moved into z
3 let x2 = x1;
// y1's value is not Copy, so it was moved into z
4 // let y2 = y1;
```

清单1-3：移动和复制语义
我们从两个值开始，一个是数字42，另一个是包含数字84的Box（一个堆分配的值）。前者是`Copy`，而后者不是。当我们将x1和y1放入元组z中时 1，x1被复制到z中，而y1被移动到z中。此时，x1仍然可以访问并可以再次使用 3。另一方面，一旦y1的值被移动 4，它就变得不可访问，任何尝试访问它的操作都会导致编译器错误。当z超出作用域时 2，它包含的元组值被丢弃，这同时也丢弃了从x1复制的值和从y1移动的值。当来自y1的`Box`被丢弃时，它也会释放用于存储y1值的堆内存。

##### 丢弃顺序

Rust会自动在值超出作用域时丢弃它们，比如在Listing 1-3的内部作用域中的x1和y1。丢弃的顺序规则相当简单：变量（包括函数参数）按照反向顺序丢弃，嵌套值按照源代码顺序丢弃。

- 这可能一开始听起来很奇怪——为什么会有这种差异？但如果我们仔细看，这其实很有道理。假设你写了一个函数，声明了一个字符串，然后将该字符串的引用插入到一个新的哈希表中。当函数返回时，必须先丢弃哈希表；如果先丢弃字符串，那么哈希表就会持有一个无效的引用！一般来说，后面的变量可能包含对前面值的引用，而反过来则不可能，这是由于Rust的生命周期规则。因此，Rust按照反向顺序丢弃变量。
- 现在，我们可以对嵌套值有相同的行为，比如元组、数组或结构体中的值，但这可能会让用户感到惊讶。如果你构造了一个包含两个值的数组，如果数组的最后一个元素先被丢弃，那就会显得很奇怪。同样的情况也适用于元组和结构体，最直观的行为是先丢弃第一个元组元素或字段，然后是第二个，依此类推。对于变量来说，没有必要在这种情况下反转丢弃顺序，因为Rust不允许在单个值中自引用。所以，Rust选择了直观的选项。

#### 借用和生命周期

Rust允许值的所有者通过引用将该值借给其他人，而不放弃所有权。引用是带有额外使用约定的指针，例如引用是否提供对引用值的独占访问，或者引用值是否还可以有其他引用指向它。

##### 共享引用

- 共享引用 &T 是一个可以共享的指针。可以存在任意数量的其他引用指向相同的值，并且每个共享引用都是可复制的，因此您可以轻松地创建更多的引用。
- 共享引用指向的值是不可变的；您不能修改或重新分配共享引用指向的值，也不能将共享引用转换为可变引用。
- Rust编译器可以假设共享引用指向的值在引用存在期间不会发生变化。例如，如果Rust编译器看到共享引用后面的值在函数中被多次读取，它有权只读取一次并重用该值。更具体地说，清单1-4中的断言应该永远不会失败。

```rust
fn cache(input: &i32, sum: &mut i32) {
  *sum = *input + *input;
  assert_eq!(*sum, 2 * *input);
}

```

清单1-4：Rust假设共享引用是不可变的。

编译器是否选择应用给定的优化几乎是无关紧要的。编译器的启发式算法会随时间而变化，因此通常希望根据编译器允许的操作来编写代码，而不是根据编译器在特定情况下的实际操作。

##### 可变引用

与共享引用相对应的是可变引用：&mut T。对于可变引用，Rust编译器再次可以充分利用引用所带来的约定：编译器假设没有其他线程通过共享引用或可变引用访问目标值。换句话说，它假设可变引用是独占的。这使得一些在其他语言中不容易实现的优化成为可能。例如，看一下清单1-5中的代码。

```rust
fn noalias(input: &i32, output: &mut i32) {
  if *input == 1 {
1   *output = 2;
  }
2 if *input != 1 {
    *output = 3;
  }
}
```

清单1-5：Rust假设可变引用是独占的。

- 在Rust中，编译器可以假设input和output不指向相同的内存。因此，在1处对output的重新赋值不会影响到2处的检查，整个函数可以编译为一个单独的if-else块。如果编译器不能依赖于独占可变性的约定，那么这个优化将是无效的，因为在类似`noalias(&x, &mut x)`的情况下，输入为1可能导致输出为3。
- 可变引用只允许您修改引用指向的内存位置。您是否可以修改超出直接引用的值取决于位于之间的类型提供的方法。通过一个例子可能更容易理解，所以请参考清单1-6。

```rust
let x = 42;
let mut y = &x; // y is of type &i32
let z = &mut y; // z is of type &mut &i32
```

清单1-6：可变性仅适用于直接引用的内存。

- 在这个例子中，你可以通过将指针y引用到不同的变量来改变指针的值（即不同的指针），但是你不能改变指针所指向的值（即x的值）。同样地，你可以通过z改变y的指针值，但是你不能改变z本身来持有不同的引用。
拥有一个值和拥有对它的可变引用之间的主要区别在于，当不再需要该值时，所有者负责丢弃该值。除此之外，通过可变引用，你可以做任何你拥有该值时可以做的事情，但有一个例外：如果你移动了可变引用后面的值，那么你必须留下另一个值来代替它。如果没有这样做，所有者仍然会认为它需要丢弃该值，但是没有值可供丢弃！

清单1-7展示了通过可变引用移动值的方式的示例。

```rust
fn replace_with_84(s: &mut Box<i32>) {
// this is not okay, as *s would be empty:
1 // let was = *s;
// but this is:
2 let was = std::mem::take(s);
// so is this:
3 *s = was;
// we can exchange values behind &mut:
let mut r = Box::new(84);
4 std::mem::swap(s, &mut r);
  assert_ne!(*r, 84);
}

let mut s = Box::new(42);
replace_with_84(&mut s);
5
```

清单1-7：通过可变引用访问必须留下一个值。

- 我添加了注释的行，表示非法操作。您不能简单地将值移出 1，因为调用者仍然认为它们拥有该值，并且会在5处再次释放它，导致双重释放。如果您只想留下一些有效的值，`std::mem::take 2`是一个很好的选择。它等同于`std::mem::replace(&mut value, Default::default())`；它将值从可变引用后面移出，但在其位置留下一个新的默认值。默认值是一个单独的拥有值，因此在作用域结束时，调用者可以安全地丢弃它。
- 或者，如果您不需要可变引用后面的旧值，可以用您已经拥有的值覆盖它 `3`，将其留给调用者稍后丢弃。当您这样做时，原来在可变引用后面的值将立即被丢弃。
- 最后，如果您有两个可变引用，可以交换它们的值 4，因为两个引用最终都会拥有一个合法的拥有值来释放。

##### 内部可变性

某些类型提供内部可变性，意味着它们允许您通过共享引用来修改值。这些类型通常依赖于附加机制（如原子CPU指令）或不变量来提供安全的可变性，而不依赖于独占引用的语义。这些类型通常分为两类：一类是通过共享引用获取可变引用的类型，另一类是通过共享引用替换值的类型。

- 第一类包括`Mutex`和`RefCell`等类型，它们包含安全机制，以确保对其提供可变引用的任何值一次只能存在一个可变引用（且没有共享引用）。在底层，这些类型（以及类似它们的类型）都依赖于一种称为`UnsafeCell`的类型，其名称应立即使您犹豫使用它。我们将在`第9章中更详细地介绍UnsafeCell`，但现在您应该知道它是通过共享引用进行变异的唯一正确方式。

- 提供内部可变性的其他类型类别是那些不提供对内部值的可变引用，而只提供用于就地操作该值的方法的类型。std::sync::atomic中的原子整数类型和std::cell::Cell类型属于此类别。您无法直接获取usize或i32类型的引用，但可以在给定时间点读取和替换其值。
**注意** 标准库中的Cell类型是通过不变量实现安全内部可变性的有趣示例。它不能在线程之间共享，并且从不提供对Cell中包含的值的引用。相反，所有方法要么完全替换值，要么返回包含的值的副本。由于无法存在对内部值的引用，因此始终可以安全地移动它。并且由于Cell不能在线程之间共享，即使通过共享引用进行变异，内部值也永远不会被同时变异。

##### 生命周期

如果您正在阅读本书，那么您可能已经熟悉生命周期的概念，可能是通过编译器关于生命周期规则违规的重复通知。这种程度的理解将对您编写的大多数Rust代码有所帮助，但是当我们深入研究Rust的更复杂部分时，您将需要一个更严格的心智模型来处理。

新手Rust开发人员通常被教导将生命周期视为与作用域对应的：当您引用某个变量时，生命周期开始，当该变量被移动或超出作用域时，生命周期结束。这通常是正确的，也通常很有用，但实际情况要复杂一些。
生命周期实际上是对某个引用必须有效的代码区域的名称。虽然生命周期通常与作用域重合，但它不一定要这样，我们将在本节后面看到。

**生命周期和借用检查器**
Rust生命周期的核心是借用检查器。每当使用一个带有某个生命周期'a的引用时，借用检查器会检查'a是否仍然有效。它通过追踪从使用点回溯到'a开始的路径（即引用被获取的地方），并检查沿该路径是否存在冲突的使用。这确保引用仍然指向一个可以安全访问的值。这类似于我们在本章前面讨论的高级“数据流”心智模型；编译器检查我们正在访问的引用的流动是否与其他并行流动冲突。
清单1-8展示了一个简单的代码示例，其中包含对引用x的生命周期注解。

```rust
let mut x = Box::new(42);
1 let r = &x; // 'a
if rand() > 0.5 {
2  *x = 84;
} else {
3   println!("{}", r); // 'a
}4
```

清单1-8：生命周期不需要连续。

- 当我们对x进行引用时，生命周期从1开始。在第一个分支2中，我们立即尝试通过将其值更改为84来修改x，这需要一个&mut x。借用检查器获取了对x的可变引用，并立即检查其使用情况。它发现在引用被获取和使用之间没有冲突的使用，因此接受了这段代码。如果您习惯于将生命周期视为作用域，这可能会让您感到惊讶，因为`r`在2处仍然在作用域内（它在4处超出作用域）。但是借用检查器足够聪明，能够意识到如果执行此分支，则r以后永远不会被使用，因此在这里对x进行可变访问是可以的。或者换句话说，从1开始创建的生命周期不延伸到此分支：从r到2没有流动，因此没有冲突的流动。然后，借用检查器在3处找到了对r的使用。它沿着路径返回到1，并没有发现冲突的使用（2不在该路径上），因此也接受了这个使用。
- 如果我们在清单1-8中的4处添加了对r的另一个使用，代码将无法编译。生命周期'a将从1一直持续到4（对r的最后一次使用），当借用检查器检查我们对r的新使用时，它将在2处发现冲突的使用。
- 生命周期可能变得非常复杂。在清单1-9中，您可以看到一个具有间断的生命周期示例，它在开始和最终结束之间是不有效的。

```rust

let mut x = Box::new(42);
1 let mut z = &x; // 'a
for i in 0..100 {
2  println!("{}", z); // 'a
3  x = Box::new(i);
4  z = &x; // 'a
}

println!("{}", z); // 'a
```

清单1-9：生命周期可以有空洞。

- 生命周期从1开始，当我们对x进行引用时。然后在3处移出x，结束了生命周期'a，因为它不再有效。借用检查器通过将'a结束于2处来接受此移动，这样在3处就没有冲突的流动了。然后，我们通过在4处更新z的引用来重新启动生命周期。无论代码现在是否循环回到2处还是继续到最后的打印语句，这两个使用现在都有一个有效的值可以流动，而且没有冲突的流动，因此借用检查器接受了这段代码！
- 再次强调，这与我们之前讨论的内存数据流模型完全一致。当x被移动时，z停止存在。当我们稍后重新分配z时，我们创建了一个完全新的变量，它只存在于那一点之后。恰好这个新变量也被命名为z。在这个模型中，这个例子并不奇怪。
**注意** 借用检查器是保守的，也必须是保守的。如果它不确定一个借用是否有效，它会拒绝它，因为允许一个无效的借用的后果可能是灾难性的。借用检查器不断变得更加智能，但有时它需要帮助来理解为什么一个借用是合法的。这就是为什么我们有不安全的Rust的一部分原因。
**泛型生命周期**
有时您需要在自己的类型中存储引用。这些引用需要有一个生命周期，以便在类型的各种方法中使用时，借用检查器可以检查它们的有效性。特别是，如果您希望类型的某个方法返回一个超出self引用的引用。
- Rust允许您使类型定义在一个或多个生命周期上泛型化，就像允许您使其在类型上泛型化一样。Steve Klabnik和Carol Nichols的《Rust编程语言》（No Starch Press，2018）对这个主题进行了详细介绍，所以我不会在这里重复基础知识。但是，当您编写更复杂的此类类型时，有两个关于此类类型和生命周期之间交互的细微差别需要注意。
- 首先，如果您的类型还实现了Drop，则删除您的类型将计为您的类型泛型生命周期或类型的使用。实际上，当删除您的类型的实例时，借用检查器将在删除之前检查您的类型的任何泛型生命周期是否仍然可以使用。这是必要的，以防您的删除代码确实使用了其中的任何引用。如果您的类型没有实现Drop，则删除该类型不计为使用，用户可以自由地忽略存储在该类型中的任何引用，只要他们不再使用它，就像我们在清单1-7中看到的那样。我们将在第9章中更详细地讨论有关删除的规则。
- 其次，虽然一个类型可以在多个生命周期上泛型化，但这样做通常只会使类型签名变得不必要复杂。通常情况下，一个类型在单个生命周期上泛型化就足够了，编译器将使用较短的生命周期作为插入到类型中的任何引用的生命周期。只有当您有一个包含多个引用的类型，并且其方法返回应与这些引用之一的生命周期相关联的引用时，才真正需要使用多个泛型生命周期参数。
- 考虑清单1-10中的类型，它为您提供了一个按特定其他字符串分隔的字符串的部分的迭代器。

```rust
struct StrSplit<'s, 'p> {
  delimiter: &'p str,
  document: &'s str,
}
impl<'s, 'p> Iterator for StrSplit<'s, 'p> {
  type Item = &'s str;
  fn next(&self) -> Option<Self::Item> {
     todo!()
  }
}
fn str_before(s: &str, c: char) -> Option<&str> {
   StrSplit { document: s, delimiter: &c.to_string() }.next()
}
```

清单1-10：需要在多个生命周期上泛型化的类型

- 当构造此类型时，您必须提供要搜索的分隔符和文档，两者都是对字符串值的引用。当您请求下一个字符串时，您会得到对文档的引用。考虑一下，如果在此类型中使用单个生命周期会发生什么。迭代器产生的值将与文档和分隔符的生命周期相关联。这将使得无法编写str_before函数：返回类型将具有与函数局部变量（通过to_string生成的String）相关联的生命周期，而借用检查器将拒绝该代码。

**生命周期的变异性**
变异性是程序员经常接触但很少知道名称的概念，因为它大多是不可见的。乍一看，变异性描述了哪些类型是其他类型的子类型，以及何时可以将子类型用于超类型（反之亦然）。广义上讲，如果类型A至少与类型B一样有用，则类型A是类型B的子类型。变异性是为什么在Java中，您可以将Turtle传递给接受Animal的函数，如果Turtle是Animal的子类型，或者为什么在Rust中，您可以将'静态str传递给接受' a str的函数。

- 虽然变异性通常隐藏在视线之外，但它经常出现，我们需要对其有一定的了解。`Turtle是Animal的子类型`，因为T`urtle比某个未指定的Animal`更“有用”-Turtle可以做任何Animal可以做的事情，可能还有更多。同样，'静态是'a的子类型，因为'静态的生命周期至少与任何'a一样长，因此更有用。或者更一般地说，如果'b：'a（'b的生命周期超过'a），那么'b是'a的子类型。这显然不是正式定义，但足够接近以实际使用。
- 所有类型都有一个变异性，它定义了可以在该类型的位置使用哪些其他类似类型。变异性有三种类型：协变、不变和逆变。如果您可以在类型的位置上使用子类型，那么该类型是协变的。例如，如果一个变量的类型是' a T，您可以为其提供类型为'静态T的值，因为' a T在'a上是协变的。' a T在T上也是协变的，因此您可以将`&Vec<&'静态str>传递给接受&Vec<&'a str>的函数`。
- 有些类型是不变的，这意味着您必须提供完全相同的类型。&mut T就是一个例子-如果一个函数接受&mut Vec<&'a str>，您不能将&mut `Vec<&'静态str>传递给它`。也就是说，&mut T在T上是不变的。如果可以，函数可以将一个短命的字符串放入Vec中，而调用者则会继续使用它，`认为它是Vec<&'静态str>`，因此包含的字符串是'静态的！任何提供可变性的类型通常都是不变的，原因是相同的-例如，`Cell<T>` 在T上是不变的。
- 最后一类逆变性出现在函数参数中。如果函数的参数越不“有用”，函数类型就越有用。如果将参数类型的变异性与其作为函数参数时的变异性进行对比，这一点就更清楚了：

```rust
let x: &'static str; // more useful, lives longer
let x: &'a str; // less useful, lives shorter
fn take_func1(&'static str) // stricter, so less useful
fn take_func2(&'a str) // less strict, more useful
```

- 这种翻转的关系表明Fn(T)在T上是逆变的。
- 那么为什么在涉及生命周期时需要学习变异性呢？
当您考虑泛型生命周期参数与借用检查器的交互时，变异性变得相关。考虑一个像清单1-11中所示的类型，它在单个字段中使用了多个生命周期。

```rust
struct MutStr<'a, 'b> {
s: &'a mut &'b str
}
let mut s = "hello";
1 *MutStr { s: &mut s }.s = "world";
println!("{}", s);
```

清单1-11：需要在多个生命周期上泛型化的类型

- 乍一看，这里使用两个生命周期似乎是不必要的——我们没有需要区分结构不同部分借用的方法，就像清单1-10中的StrSplit一样。但是，如果将这里的两个生命周期替换为单个'a，代码将无法编译！这完全是因为变异性。

**注意** 1处的语法可能看起来陌生。它等同于定义一个持有MutStr的变量x，然后写入*x.s = "world"，只是没有变量，因此MutStr立即被丢弃。

- 在1处，编译器必须确定生命周期参数（s）应设置为什么生命周期。如果有两个生命周期，'a 将设置为对s的借用的待确定生命周期，'b 将设置为'static，因为这是提供的字符串"hello"的生命周期。如果只有一个生命周期'a，编译器推断该生命周期必须是'static。
- 当我们稍后尝试通过共享引用访问字符串引用s以打印它时，编译器尝试缩短MutStr使用的s的可变借用，以允许对s的共享借用。
- 在双生命周期的情况下，'a 在println之前简单地结束，'b 保持不变。另一方面，在单生命周期的情况下，我们遇到了问题。编译器希望缩短对s的借用，但要做到这一点，它还必须缩短对str的借用。尽管 'static str 通常可以缩短为任何'a str（'a T在'a 上是协变的），但在这里它在&mut T后面，而&mut T在T上是不变的。不变性要求相关类型永远不会被替换为子类型或超类型，因此编译器缩短借用的尝试失败，并报告列表仍然被可变借用。糟糕！
- 由于不变性所施加的灵活性降低，您希望确保您的类型在尽可能多的泛型参数上保持协变（或在适当的情况下是逆变的）。如果这需要引入额外的生命周期参数，您需要仔细权衡添加另一个参数的认知成本与不变性的人体工程学成本。

#### 总结

本章的目标是建立一个坚实的共享基础，以便在接下来的章节中进行构建。到目前为止，我希望您感到对Rust的内存和所有权模型有了牢固的掌握，并且您可能从借用检查器中获得的错误似乎不再神秘。您可能已经知道我们在这里介绍的一些内容，但希望本章能给您一个更全面的整体形象，了解它们如何相互配合。在下一章中，我们将对类型进行类似的操作。我们将介绍类型在内存中的表示方式，了解泛型和特性如何生成运行代码，并查看Rust为更高级用例提供的一些特殊类型和特性构造。

### 2 类型

现在基础知识已经介绍完毕，我们将看一下Rust的类型系统。我们将跳过《Rust编程语言》中介绍的基础知识，而是直接深入探讨不同类型在内存中的布局、特性和特性限制、存在类型以及在跨crate边界使用类型的规则。

#### 类型在内存中的布局

每个Rust值都有一个类型。类型在Rust中有很多用途，正如我们在本章中所看到的，但其中最基本的作用之一是告诉您如何解释内存中的位。例如，位序列0b10111101（用十六进制表示为0xBD）本身并没有任何意义，直到您为其分配一个类型。当在类型u8下解释时，该位序列表示数字189。当在类型i8下解释时，它表示-67。当您定义自己的类型时，编译器的工作是确定定义类型的每个部分在内存中的表示位置。您的结构体的每个字段在位序列中的位置是什么？您的枚举的鉴别器存储在哪里？在编写更高级的Rust代码时，了解这个过程的工作原理非常重要，因为这些细节会影响代码的正确性和性能。

##### 对齐

- 在我们讨论如何确定类型的内存表示之前，我们首先需要讨论对齐的概念，它决定了类型的字节可以存储在哪里。一旦确定了类型的表示，您可能会认为可以将任意内存位置视为该类型并解释存储在那里的字节。虽然从理论上讲这是正确的，但在实践中，硬件也限制了给定类型可以放置的位置。最明显的例子是指针指向字节而不是位。如果您将类型T的值放置在计算机内存的第4位开始的位置，您将无法引用其位置；您只能创建一个指向字节0或字节1（第8位）的指针。因此，无论其类型如何，所有值都必须从字节边界开始。我们说所有值至少需要字节对齐-它们必须放置在8位的倍数地址上。
- 有些值的对齐规则比仅仅字节对齐更严格。在CPU和内存系统中，内存通常以大于单个字节的块进行访问。例如，在64位CPU上，大多数值以8字节（64位）的块进行访问，每个操作都从8字节对齐的地址开始。这被称为CPU的字长。然后，CPU使用一些巧妙的方法来处理读取和写入较小的值，或者跨越这些块边界的值。
- 在可能的情况下，您希望确保硬件可以在其“本机”对齐方式下运行。为了看到原因，考虑一下如果尝试读取从8字节块的中间开始的i64会发生什么（即，指向它的指针不是8字节对齐）。硬件将不得不进行两次读取-一次从第一个块的后半部分到达i64的开头，一次从第二个块的前半部分读取剩余的i64-然后将结果拼接在一起。这是非常低效的。由于操作分布在对底层内存的多次访问中，如果您从不同的线程同时写入的内存上读取，可能会得到奇怪的结果。您可能在另一个线程的写入发生之前读取前4个字节，然后在写入之后读取后4个字节，导致值损坏。
- 对于未对齐的数据进行操作被称为未对齐访问，可能导致性能下降和并发问题。因此，许多CPU操作要求或强烈建议其参数是自然对齐的。自然对齐的值是其对齐与其大小相匹配的值。因此，例如，对于8字节的加载，提供的地址必须是8字节对齐的。
- 由于对齐访问通常更快并提供更强的一致性语义，编译器会尽可能利用它们。它通过为每个类型分配一个根据其包含的类型计算出的对齐方式来实现这一点。内置值通常对齐到其大小，因此u8是字节对齐的，u16是2字节对齐的，u32是4字节对齐的，u64是8字节对齐的。复杂类型-包含其他类型的类型-通常被分配为其包含的任何类型的最大对齐方式。例如，包含u8、u16和u32的类型将是4字节对齐的，因为有u32的存在。

##### 布局

现在您了解了对齐，我们可以探讨编译器如何确定类型的内存表示，即布局。默认情况下，正如您很快将看到的，Rust编译器对于如何布局类型几乎没有提供任何保证，这对于理解底层原理来说是一个很差的起点。幸运的是，Rust提供了一个repr属性，您可以将其添加到类型定义中，以请求该类型的特定内存表示。您最常见的可能是repr(C)。顾名思义，它以与C或C++编译器布局相兼容的方式布局类型。这在使用外部函数接口与其他语言进行交互的Rust代码中非常有用，我们将在第11章中讨论外部函数接口时详细介绍，因为Rust将生成与其他语言编译器期望的布局相匹配的布局。由于C布局是可预测的且不会更改，repr(C)在不安全的上下文中也非常有用，如果您使用原始指针与该类型一起工作，或者如果您需要在两个已知具有相同字段的不同类型之间进行类型转换。当然，它也非常适合进入布局算法的第一步。

**注意** 另一个有用的表示是repr(transparent)，它只能用于具有单个字段的类型，并保证外部类型的布局与内部类型完全相同。这在与“newtype”模式结合使用时非常方便，其中您可能希望按照内存表示操作某些struct A和struct NewA(A)。

- 那么，让我们看看编译器如何在内存中布局repr(C)中的特定类型：清单2-1中的Foo类型。您认为编译器会如何在内存中布局它？

```rust
#[repr(C)]
struct Foo {
  tiny: bool,
  normal: u32,
  small: u8,
  long: u64,
  short: u16,
}
 ```

清单2-1：对齐影响布局。

- 首先，编译器看到了字段tiny，其逻辑大小为1位（true或false）。但是由于CPU和内存是以字节为单位操作的，tiny在内存表示中被分配了1字节。接下来，normal是一个4字节的类型，所以我们希望它是4字节对齐的。但是，即使Foo是对齐的，我们分配给tiny的1字节也会使normal错过它的对齐。为了纠正这个问题，编译器在tiny和normal之间的内存表示中插入了3字节的填充-这些字节具有不确定的值，在用户代码中被忽略-。填充中没有值，但它占用了空间。
- 对于下一个字段small，对齐很简单：它是一个1字节的值，并且当前结构体中的字节偏移量为1 + 3 + 4 = 8。这已经是字节对齐的，所以small可以紧跟在normal之后。但是，对于long，我们又遇到了问题。现在，我们已经进入了9字节的Foo。如果Foo是对齐的，那么long不是我们希望的8字节对齐，所以我们必须再插入另外7字节的填充来重新对齐long。这也方便地确保了我们需要的2字节对齐，以容纳最后一个字段short，总共为26字节。现在，我们已经遍历了所有字段，还需要确定Foo本身的对齐方式。规则是使用Foo的任何字段的最大对齐方式，这将是8字节，因为有long。因此，为了确保如果将Foo放置在数组中（例如）时Foo保持对齐，编译器添加了最后的6字节填充，使Foo的大小成为其对齐的倍数，即32字节。
- 现在我们准备摆脱C的遗留问题，并考虑一下如果我们在清单2-1中没有使用repr(C)会发生什么。C表示的主要限制之一是它要求我们按照原始结构定义中的顺序放置所有字段。默认的Rust表示repr(Rust)消除了这个限制，以及其他一些较小的限制，例如对于具有相同字段的类型的确定性字段排序。也就是说，即使两个不同的类型共享所有相同的字段，类型相同，顺序相同，也不能保证在使用默认的Rust布局时它们的布局相同！
- 由于我们现在可以重新排序字段，我们可以按照大小递减的顺序放置它们。这意味着我们不再需要Foo字段之间的填充；字段本身用于实现所需的对齐！现在，Foo只是其字段的大小：只有16字节。这是Rust默认情况下不会给出关于类型在内存中布局的许多保证的原因之一：通过给编译器更多的自由重新排列事物，我们可以生成更高效的代码。
- 实际上，还有第三种布局类型的方式，那就是告诉编译器我们不希望在字段之间有任何填充。这样做意味着我们愿意承受使用未对齐访问的性能损失。最常见的用例是当每个额外的字节内存的影响都可以感知到时，例如如果您有大量类型的实例，如果您的内存非常有限，或者如果您正在通过像网络连接这样的低带宽介质发送内存表示。要选择此行为，您可以使用#[repr(packed)]注解您的类型。请记住，这可能导致更慢的代码，并且在极端情况下，如果尝试执行CPU仅对齐参数支持的操作，可能会导致程序崩溃。
- 有时，您希望为特定字段或类型提供比其技术要求更大的对齐方式。您可以使用属性#[repr(align(n))]来实现这一点。这样做的一个常见用例是确保存储在内存中连续的不同值（例如在数组中）最终位于CPU的不同缓存行中。这样，您可以避免伪共享，伪共享可能导致并发程序的性能严重下降。伪共享发生在两个不同的CPU访问共享缓存行的不同值时；虽然它们理论上可以并行操作，但它们最终都会争夺更新缓存中的同一条目。我们将在第10章中更详细地讨论并发性。

##### 复杂类型

- 您可能会好奇编译器如何在内存中表示其他Rust类型。这里是一个快速参考：

  - **元组** 以与元组值相同顺序的相同类型字段的结构体表示。
  - **数组** 作为包含类型的连续序列表示，元素之间没有填充。
  - **联合** 布局独立选择每个变体。对齐方式是所有变体中的最大值。
  - **枚举** 与联合相同，但有一个额外的隐藏共享字段，用于存储枚举变体的鉴别器。鉴别器是代码用来确定给定值包含的枚举变体的值。鉴别器字段的大小取决于变体的数量。
  
##### 动态大小类型和宽指针

您可能在Rust文档的各个奇怪角落和错误消息中遇到过标记特征Sized。通常，它会出现，因为编译器希望您提供一个Sized类型，但您（显然）没有提供。在Rust中，大多数类型都会自动实现Sized，也就是说，它们在编译时具有已知的大小，但有两种常见类型不是：特征对象和切片。如果您有一个dyn Iterator或[u8]，它们没有明确定义的大小。它们的大小取决于程序运行时才能知道的一些信息，而不是在编译时，这就是为什么它们被称为动态大小类型（DST）。没有人事先知道您的函数接收到的dyn Iterator是这个200字节的结构还是那个8字节的结构。这带来了一个问题：通常，编译器必须知道某个东西的大小才能生成有效的代码，例如对于类型为(i32，dyn Iterator，[u8]，i32)的元组分配多少空间，或者如果您的代码尝试访问第四个字段时要使用的偏移量。但是，如果类型不是Sized，那么这些信息是不可用的。

编译器几乎在任何地方都要求类型是Sized的。结构字段、函数参数、返回值、变量类型和数组类型都必须是Sized的。这个限制是如此常见，以至于您编写的每个类型约束都包括T: Sized，除非您使用T: ?Sized明确地选择退出（?表示“可能不是”）。但是，如果您有一个DST并且希望对其执行某些操作，这样做并不是很有帮助，例如，如果您真的希望函数接受特征对象或切片作为参数。
解决无大小和有大小类型之间差距的方法是将无大小类型放在宽指针（也称为胖指针）后面。宽指针与普通指针非常相似，但它包含一个额外的字大小字段，该字段提供了编译器生成与指针一起工作的合理代码所需的附加信息。当您对DST引用时，编译器会自动为您构造一个宽指针。对于切片，额外的信息只是切片的长度。对于特征对象-好吧，我们稍后再说。关键是，宽指针是Sized的。具体而言，它是usize的两倍大小（usize是目标平台上一个字的大小）：一个usize用于保存指针，一个usize用于保存“完成”类型所需的额外信息。
**注意** Box和Arc也支持存储宽指针，这就是为什么它们都支持T: ?Sized的原因。

#### 特质和特质约束

特质是Rust类型系统的关键部分，它们是使类型能够在定义时互操作的粘合剂。《Rust编程语言》对如何定义和使用特质进行了很好的介绍，所以我不会在这里详细介绍。相反，我们将看一下特质的一些更技术性的方面：它们是如何实现的、您必须遵守的限制以及一些更奇特的特质用法。

##### 编译和分派

到目前为止，您可能已经在Rust中编写了相当数量的泛型代码。您在类型和方法上使用了泛型类型参数，甚至可能偶尔使用了一些特质约束。但是，您是否曾经想过在编译泛型代码时实际发生了什么，或者在对dyn Trait调用特质方法时会发生什么？

当您编写一个对T泛型的类型或函数时，实际上是在告诉编译器为每个类型T创建一个副本的类型或函数。当您构造一个Vec或HashMap<String, bool>时，编译器实际上是将泛型类型及其所有实现块复制粘贴，并将每个泛型参数的所有实例替换为您提供的具体类型。它为每个T替换为i32创建了一个完整的Vec类型的副本，并为每个K替换为String和每个V替换为bool创建了一个完整的HashMap类型的副本。
**注意** 实际上，编译器并不真正进行完整的复制粘贴。它只复制您使用的代码的部分，因此如果您从未在Vec上调用find，那么find的代码将不会被复制和编译。

泛型函数也是如此。考虑清单2-2中的代码，它展示了一个泛型方法。

```rust
impl String {
pub fn contains(&self, p: impl Pattern) -> bool {
     p.is_contained_in(self)
  }
}
```

清单2-2：使用静态分派的泛型方法

- 对于每种不同的模式类型，都会创建该方法的副本（回想一下，impl Trait是<T: Trait>的简写）。我们需要为每种impl Pattern类型创建一个不同的函数体副本，因为我们需要知道is_contained_in函数的地址以便调用它。CPU需要被告知跳转到哪里并继续执行。对于任何给定的模式，编译器知道该地址是该模式类型实现该特质方法的地方的地址。但是，我们无法为任何类型使用一个地址，因此我们需要为每种类型创建一个副本，每个副本都有自己的地址跳转。这被称为静态分派，因为对于方法的任何给定副本，我们“分派到”的地址是已知的静态地址。

**注意** 您可能已经注意到，在这个上下文中，“静态”一词有点多义。静态通常用于指代在编译时已知的任何内容，或者可以被视为已知的内容，因为它可以写入静态内存，正如我们在第1章中讨论的那样。

- 从泛型类型到许多非泛型类型的过程称为单态化，这也是泛型Rust代码通常与非泛型代码一样高效的原因之一。当编译器开始优化代码时，它就好像没有泛型存在一样！每个实例都是单独优化的，并且所有类型都是已知的。因此，代码的效率与直接调用传入的模式的is_contained_in方法一样高效，而不需要任何特质。编译器对所涉及的类型有完全的了解，甚至可以选择内联is_contained_in的实现。
- 单态化也有代价：所有这些类型的实例化需要单独编译，如果编译器无法优化它们，这可能会增加编译时间。每个单态化函数也会产生自己的机器代码块，这可能会使程序变得更大。由于不同实例化的泛型类型方法之间不共享指令，CPU的指令缓存也不那么有效，因为现在需要保存多个实际上相同指令的副本。
  
  **非泛型内部函数**

- 通常，泛型方法中的大部分代码与类型无关。例如，考虑HashMap::insert的实现。计算提供的键的哈希值的代码取决于映射的键类型，但遍历映射的桶以找到插入点的代码可能与类型无关。在这种情况下，共享生成的非泛型方法的机器代码将更高效，只有在实际需要时才生成不同的副本。

- 您可以使用以下模式来处理这种情况：在泛型方法内部声明一个非泛型的辅助函数，执行共享操作。这样，编译器只需为您复制粘贴与类型相关的代码，而辅助函数可以共享使用。
- 将函数设置为内部函数的好处是，您不会在模块中污染一个单一目的的函数。相反，您可以在方法之外声明这样的辅助函数；只需注意不要将其设置为泛型impl块下的方法，否则它仍将被单态化。

- 静态分派的替代方案是动态分派，它使代码能够在不知道泛型类型的情况下调用特质方法。我之前说过，在清单2-2中需要多个方法实例的原因是，否则您的程序将不知道调用给定模式上的特质方法is_contained_in的地址。好吧，使用动态分派，调用方只需告诉您。如果您将impl Pattern替换为&dyn Pattern，您告诉调用方他们必须为此参数提供两个信息：模式的地址和is_contained_in方法的地址。在实践中，调用方给我们提供了一个指向称为虚方法表或vtable的内存块的指针，该内存块保存了有关该类型的所有特质方法的实现的地址，其中之一是is_contained_in。当方法内部的代码想要在提供的模式上调用特质方法时，它会在vtable中查找该模式的is_contained_in实现的地址，然后调用该地址处的函数。这使我们能够在不管调用方想要使用什么类型的情况下使用相同的函数体。
**注意** 每个vtable还包含有关具体类型的布局和对齐方式的信息，因为始终需要这些信息来处理类型。如果您想要一个显式vtable的示例，请查看std::task::RawWakerVTable类型。
- 当我们使用dyn关键字选择动态分派时，您会注意到我们必须在其前面加上&符号。原因是我们不再在编译时知道调用方传递的模式类型的大小，因此我们不知道为其分配多少空间。换句话说，dyn Trait是!Sized，其中!表示不。为了使其Sized，以便我们可以将其作为参数接受，我们将其放在指针后面（我们知道其大小）。由于我们还需要传递方法地址表，因此该指针变成了宽指针，其中额外的字保存了指向vtable的指针。您可以使用任何能够保存宽指针的类型进行动态分派，例如&mut、Box和Arc。清单2-3显示了清单2-2的动态分派等效形式。

```rust
impl String {
  pub fn contains(&self, p: &dyn Pattern) -> bool {
    p.is_contained_in(&*self)
  }
}
```

清单2-3：使用动态分派的通用方法

- 实现特质的类型和其vtable的组合被称为特质对象。大多数特质都可以转换为特质对象，但并非所有特质都可以。例如，Clone特质的clone方法返回Self，因此无法转换为特质对象。如果我们接受一个dyn Clone特质对象，然后调用clone方法，编译器将不知道返回的类型是什么。或者，考虑标准库中的Extend特质，它的extend方法对提供的迭代器的类型是泛型的（因此可能有多个实例）。如果您调用一个接受dyn Extend的方法，那么在特质对象的vtable中就没有一个单一的地址可以放置extend；必须为extend可能被调用的每种类型都有一个条目。这些是不可对象安全的特质的示例，因此可能无法转换为特质对象。为了是对象安全的，特质的所有方法都不能是泛型的，也不能使用Self类型。此外，特质不能有任何静态方法（即，第一个参数不解引用为Self的方法），因为无法知道要调用的方法的哪个实例。例如，不清楚FromIterator::from_iter(&[0])应该执行哪段代码。
- 在阅读有关特质对象的文档时，您可能会看到对特质绑定Self: Sized的提及。这样的绑定意味着Self没有通过特质对象使用（因为它将是!Sized）。您可以将该绑定放在特质上，以要求该特质永远不使用动态分派，或者可以将其放在特定方法上，以使该方法在通过特质对象访问特质时不可用。具有where Self: Sized绑定的方法在检查特质是否对象安全时被豁免。
- 动态分派可以减少编译时间，因为不再需要编译类型和方法的多个副本，并且可以提高CPU指令缓存的效率。然而，它也阻止编译器针对特定使用的类型进行优化。使用动态分派，清单2-2中的find方法只能通过vtable插入对函数的调用-编译器无法执行任何额外的优化，因为它不知道该函数调用的另一侧的代码是什么。此外，对特质对象的每个方法调用都需要在vtable中进行查找，这会增加一小部分开销，而不是直接调用方法。

- 当你在静态分派和动态分派之间做选择时，很少有明确的正确答案。总的来说，在库中使用静态分派，在二进制文件中使用动态分派是比较合适的。在库中，你希望允许用户根据他们的需求选择最适合他们的分派方式。如果你使用动态分派，他们也被迫这样做，而如果你使用静态分派，他们可以选择是否使用动态分派。另一方面，在二进制文件中，你编写的是最终代码，所以除了你编写的代码之外，没有其他需求需要考虑。动态分派通常允许你编写更清晰的代码，省略了泛型参数，并且编译速度更快，尽管性能上可能会有一些损失，所以通常对于二进制文件来说是更好的选择。

##### 泛型特质

Rust的特质可以通过两种方式进行泛型化：使用泛型类型参数，例如`trait Foo<T>`，或者使用关联类型，例如trait Foo { type Bar; }。这两者之间的区别并不立即显而易见，但幸运的是，有一个简单的经验法则：如果您期望给定类型的特质只有一个实现，请使用关联类型；否则，请使用泛型类型参数。

- 这样做的原因是，关联类型通常更容易使用，但不允许多个实现。因此，简而言之，建议您尽可能使用关联类型。
- 对于泛型特质，用户必须始终指定所有泛型参数并重复任何参数的约束。这可能很快变得混乱且难以维护。如果您向特质添加了一个泛型参数，那么该特质的所有用户也必须进行更新以反映这些更改。由于对于给定类型可能存在多个特质的实现，编译器可能很难确定您要使用的特质实例，从而导致糟糕的消除歧义的函数调用，例如`FromIterator::<u32>::from_iter`。但好处是，您可以为同一类型多次实现特质，例如，您可以为您的类型实现多个右侧类型的PartialEq，或者您可以同时为`FromIterator<T>`和FromIterator<&T>实现，其中T: Clone，这正是泛型特质提供的灵活性。
- 另一方面，对于关联类型，编译器只需要知道实现特质的类型，然后跟随所有关联类型（因为只有一个实现）。这意味着所有约束都可以存在于特质本身中，不需要在使用时重复。反过来，这允许特质添加进一步的关联类型而不影响其用户。由于类型决定了特质的所有关联类型，您永远不必使用前面段落中显示的统一函数调用语法进行消除歧义。然而，您不能为多个Target类型实现Deref，也不能为多个不同的Item类型实现Iterator。

##### 一致性和孤儿规则

Rust对于可以在哪里实现特质以及可以在哪些类型上实现它们有一些相当严格的规则。这些规则存在的目的是保持一致性属性：对于任何给定的类型和方法，对于该类型使用的方法的实现只有一个正确的选择。为了看到这一点的重要性，考虑一下如果我可以为标准库中的bool类型编写自己的Display特质实现会发生什么。现在，对于任何试图打印bool值并包含我的crate的代码，编译器将不知道是选择我编写的实现还是标准库中的实现。两种选择都不正确，也没有比另一种更好，编译器显然无法随机选择。如果没有涉及标准库，而是我们有两个相互依赖的crate，并且它们都为某个共享类型实现了一个特质，那么同样的问题也会发生。一致性属性确保编译器永远不会陷入这些情况，也永远不必做出这些选择：总是只有一个明显的选择。

- 维护一致性的一种简单方法是确保只有定义特质的crate才能为该特质编写实现；如果其他人无法实现该特质，那么就不会在其他地方出现冲突的实现。然而，在实践中，这太过限制性，会使特质变得无用，因为除非您将自己的类型包含在定义crate中，否则无法为自己的类型实现诸如std::fmt::Debug和serde::Serialize之类的特质。相反的极端观点是，只能为自己的类型实现特质，解决了这个问题，但引入了另一个问题：定义特质的crate现在无法为标准库或其他流行crate中的类型提供该特质的实现！理想情况下，我们希望找到一些规则，以在希望下游crate为其自己的类型实现上游特质之间取得平衡，同时又希望上游crate能够添加自己特质的实现而不会破坏下游代码。

**注意** 上游是指您的代码依赖的内容，下游是指依赖于您的代码的内容。通常，这些术语在直接的crate依赖关系意义上使用，但它们也可以用于指代代码库的官方分支的权威分支 - 如果您对Rust编译器进行分支，官方Rust编译器就是您的“上游”。

- 在Rust中，确立这种平衡的规则是孤儿规则。简单地说，孤儿规则规定，只有当特质或类型是本地crate的时候，才能为类型实现特质。因此，您可以为自己的类型实现Debug，也可以为bool实现MyNeatTrait，但不能为bool实现Debug。如果尝试这样做，您的代码将无法编译，并且编译器将告诉您存在冲突的实现。
- 这可以让您走得很远；它允许您为第三方类型实现自己的特质，并为自己的类型实现第三方特质。然而，孤儿规则并不是故事的终点。还有一些额外的含义、注意事项和例外情况需要注意。

**全局实现**
孤儿规则允许您使用类似impl`<T> MyTrait for T where T:`的代码来为一系列类型实现特质。这是一种全局实现 - 它不仅限于特定的类型，而是适用于广泛的类型范围。只有定义特质的crate才允许编写全局实现，并且向现有特质添加全局实现被认为是一种破坏性的更改。如果不是这样，包含impl MyTrait for Foo的下游crate可能会因为您更新定义MyTrait的crate而突然停止编译，并出现关于冲突实现的错误。
**基本类型**
某些类型非常重要，以至于必须允许任何人在其上实现特质，即使这似乎违反了孤儿规则。这些类型使用#[fundamental]属性进行标记，目前包括&、&mut和Box。对于孤儿规则而言，基本类型可以说不存在 - 在检查孤儿规则之前，它们实际上被擦除了，以便您可以为&MyType实现IntoIterator，例如。只有孤儿规则的话，这个实现是不允许的，因为它为外部类型实现了外部特质 - IntoIterator和&都来自标准库。添加基本类型的全局实现也被认为是一种破坏性的更改。
**覆盖实现**
有一些有限的情况下，我们希望允许为外部类型实现外部特质，而孤儿规则通常不允许这样做。这种情况的最简单示例是当您想要编写类似`impl From<MyType> for Vec<i32>`的代码时。在这里，From特质是外部的，Vec类型也是外部的，但没有违反一致性的危险。这是因为只有通过标准库中的全局实现（标准库无法命名MyType）才能添加冲突的实现，而这本身就是一种破坏性的更改。
为了允许这些类型的实现，孤儿规则包含了一个狭窄的例外，只允许在非常特定的情况下为外部类型实现外部特质。具体而言，只有当至少一个Ti是本地类型，并且在第一个这样的Ti之前的所有T都不是泛型类型P1..=Pn时，才允许给定的impl<P1..=Pn> ForeignTrait<T1..=Tn> for T0。泛型类型参数（Ps）允许出现在T0..Ti中，只要它们被某些中间类型覆盖即可。如果T作为其他类型的类型参数出现（例如`Vec<T>`），则T被视为已覆盖，但如果T独立存在（只是T）或仅出现在基本类型&后面，那么T就不被视为已覆盖。因此，Listing 2-4中的所有实现都是有效的。

```rust
impl<T> From<T> for MyType
impl<T> From<T> for MyType<T>
impl<T> From<MyType> for Vec<T>
impl<T> ForeignTrait<MyType, T> for Vec<T>

```

清单2-4：为外部类型实现外部特质的有效实现

然而，清单2-5中的实现是无效的。

```rust
impl<T> ForeignTrait for T
impl<T> From<T> for T
impl<T> From<Vec<T>> for T
impl<T> From<MyType<T>> for T
impl<T> From<T> for Vec<T>
impl<T> ForeignTrait<T, MyType> for Vec<T>
```

清单2-5：为外部类型实现外部特质的无效实现

- 孤儿规则的这种放宽使得在为现有特质添加新实现时，什么构成破坏性更改变得复杂。特别是，只有当新实现包含至少一个新的本地类型，并且该新的本地类型满足前面描述的例外规则时，才不会产生破坏性更改。添加任何其他新实现都是破坏性更改。

**注意** 注意，`impl<T> ForeignTrait<LocalType, T> for ForeignType`是有效的，但`impl<T> ForeignTrait<T, LocalType> for ForeignType`是无效的！这可能看起来是武断的，但如果没有这个规则，您可以编写impl`<T> ForeignTrait<T, LocalType> for ForeignType`，而另一个crate可以编写`impl<T> ForeignTrait<TheirType, T> for ForeignType`，只有当两个crate合并在一起时才会出现冲突。孤儿规则要求您的本地类型出现在类型参数之前，从而打破了这种关系，并确保如果两个crate在隔离环境中遵守一致性，它们在合并时也会遵守一致性。

##### 特质约束

标准库中充斥着特质约束，无论是HashMap中的键必须实现Hash + Eq，还是传递给thread::spawn的函数必须是FnOnce + Send + 'static。当您自己编写通用代码时，它几乎肯定会包含特质约束，否则您的代码对其泛型类型无法做太多事情。随着您编写更复杂的通用实现，您会发现自己对特质约束需要更多的准确性，因此让我们看一下一些实现这一目标的方法。

- 首先，特质约束不必采用T: Trait的形式，其中T是您的实现或类型泛型的某种类型。约束可以是任意类型限制，甚至不需要包括泛型参数、参数类型或本地类型。您可以编写像where String: Clone这样的特质约束，即使String: Clone始终为真且不包含本地类型。您还可以编写像`where io::Error: From<MyError<T>>`这样的特质约束；您的泛型类型参数不仅需要出现在左侧，这不仅允许您表达更复杂的约束，还可以避免不必要地重复约束。例如，如果您的方法想要构造一个HashMap<K, V, S>，其中键是某个泛型类型T，值是usize，而不是像where T: Hash + Eq, S: BuildHasher + Default这样写出约束，您可以写出where HashMap<T, usize, S>: FromIterator。这样可以避免查找您最终使用的方法的确切约束要求，并更清楚地传达代码的“真实”要求。正如您所看到的，如果底层特质方法的约束较复杂，它还可以显著减少约束的复杂性。

  **派生特质**

  虽然#[derive(Trait)]非常方便，但在特质约束的上下文中，您应该注意一个细微之处，即它通常是如何实现的。许多#[derive(Trait)]展开成`impl Trait for Foo<T> where T: Trait`。这通常是您想要的，但并非总是如此。例如，考虑以这种方式为`Foo<T>派生Clone，而Foo包含Arc<T>`。无论T是否实现Clone，Arc都实现了Clone，但由于派生的约束，只有当T实现Clone时，Foo才会实现Clone！这通常不是太大的问题，但它确实增加了一个不需要的约束。如果我们将类型重命名为Shared，问题可能会变得更清晰一些。想象一下，当编译器告诉用户他们无法克隆`Shared<NotClone>`时，他们会多么困惑！在撰写本文时，标准库提供的#[derive(Clone)]就是这样工作的，尽管这可能会在将来发生变化

- 有时，您希望对您的泛型类型的关联类型进行约束。例如，考虑迭代器方法flatten，它接受一个产生本身实现Iterator的项的迭代器，并生成这些内部迭代器的项的迭代器。它生成的类型Flatten是泛型的，其中I是外部迭代器的类型。如果I实现了Iterator，并且I产生的项本身实现了IntoIterator，则Flatten实现了Iterator。为了使您能够编写这样的约束，Rust允许您使用语法Type::AssocType来引用类型的关联类型。例如，我们可以使用I::Item来引用I的Item类型。如果一个类型具有多个相同名称的关联类型，例如如果提供关联类型的特质本身是泛型的（因此有许多实现），则可以使用语法`<Type as Trait>::AssocType`进行消歧义。使用这种方式，您不仅可以为外部迭代器类型编写约束，还可以为该外部迭代器的项类型编写约束。

- 在广泛使用泛型的代码中，您可能会发现自己需要编写一个关于对类型的引用的约束。通常情况下，这是可以接受的，因为您往往还会有一个可以用作这些引用的生命周期参数。然而，在某些情况下，您希望约束说“对于任何生命周期，此引用都实现此特质。”这种类型的约束被称为高阶特质约束，它在与Fn特质结合使用时特别有用。例如，假设您希望对一个函数进行泛型化，该函数接受对T的引用并返回对T内部的引用。如果您编写F: Fn(&T) -> &U，您需要为这些引用提供一个生命周期，但您真正想要说的是“任何生命周期，只要输出与输入相同。”使用高阶生命周期，您可以编写F: for<'a> Fn(&'a T) -> &'a U，表示对于任何生命周期'a，约束必须成立。Rust编译器足够聪明，当您编写类似于Fn约束的引用时，它会自动添加for，这涵盖了此功能的大多数用例。在撰写本文时，标准库仅在三个地方使用了显式形式，但确实存在，因此值得了解。

- 将所有这些内容结合起来，考虑清单2-6中的代码，它可以用于为任何可以迭代且元素具有Debug特性的类型实现Debug。
  
```rust
impl Debug for AnyIterable
where for<'a> &'a Self: IntoIterator,
for<'a> <&'a Self as IntoIterator>::Item: Debug {
fn fmt(&self, f: &mut Formatter) -> Result<(), Error> {
f.debug_list().entries(self).finish()
}}

```

清单2-6：适用于任何可迭代集合的过度泛化的Debug实现

- 您可以将此实现复制粘贴到几乎任何集合类型中，它将“正常工作”。当然，您可能希望有一个更智能的调试实现，但这很好地说明了特质约束的强大之处。

##### 标记特质

通常，我们使用特质来表示多个类型可以支持的功能；可以通过调用hash对Hash类型进行哈希，可以通过调用clone对Clone类型进行克隆，可以通过调用fmt对Debug类型进行格式化以进行调试。但并非所有特质都以这种方式发挥功能。一些特质，称为标记特质，相反地指示实现类型的属性。标记特质没有方法或关联类型，只是告诉您特定类型可以或不能以某种方式使用。例如，如果一个类型实现了Send标记特质，那么在线程边界上发送它是安全的。如果它没有实现这个标记特质，那么发送它是不安全的。这个行为没有关联的方法；它只是关于类型的一个事实。标准库中有许多这样的特质，包括Send、Sync、Copy、Sized和Unpin，它们中的大多数（除了Copy）也是自动特质；编译器会自动为类型实现它们，除非类型包含了不实现标记特质的内容。

- 标记特质在Rust中起着重要的作用：它们允许您编写捕捉代码中未直接表达的语义要求的约束。在需要类型是Send的代码中没有调用send。相反，代码假设给定的类型可以在单独的线程中使用，没有标记特质，编译器无法检查这个假设。程序员需要记住这个假设并仔细阅读代码，这是我们都知道不希望依赖的事情。这条路上充满了数据竞争、段错误和其他运行时问题。
- 类似于标记特质的是标记类型。它们是不包含数据且没有方法的单元类型（例如struct MyMarker;）。标记类型在标记类型处于特定状态时非常有用。当您希望防止用户误用API时，它们非常方便。例如，考虑一个类型SshConnection，它可能已经进行了身份验证，也可能还没有进行身份验证。您可以为SshConnection添加一个泛型类型参数，然后创建两个标记类型：Unauthenticated和Authenticated。当用户首次连接时，他们会得到`SshConnection<Unauthenticated>`。在其impl块中，您只提供一个方法：connect。connect方法返回一个`SshConnection<Authenticated>`，只有在该impl块中，您才提供运行命令等其他方法。我们将在第3章进一步讨论这种模式。

#### 存在类型

在Rust中，您很少需要为函数体中声明的变量指定类型，也不需要为调用的泛型参数的类型指定类型。这是因为类型推断，编译器根据类型出现的代码所评估的类型来决定使用的类型。编译器通常只会对变量和闭包的参数（和返回类型）进行类型推断；顶层定义，如函数、类型、特质和特质实现块，都需要您明确命名所有类型。这样做的原因有几个，但主要原因是当您至少有一些已知的起点来开始推断时，类型推断会更容易。然而，并不总是容易，甚至不可能完全命名一个类型！例如，如果您从函数返回一个闭包，或者从特质方法返回一个异步块，它的类型没有一个您可以在代码中输入的名称。

- 为了处理这种情况，Rust支持存在类型。很有可能，您已经看到了存在类型的作用。所有标记为async fn或返回类型为impl Trait的函数都具有存在返回类型：签名不给出返回值的真实类型，只是给出函数返回某个实现了一组特质的类型的一些提示，调用者可以依赖于该返回类型实现这些特质，而不是其他任何东西。

**注意** 严格来说，调用者不仅依赖于返回类型，还依赖于其他内容。编译器还会通过impl Trait在返回位置传播自动特质，如Send和Sync。我们将在下一章中更详细地讨论这个问题。

- 这种行为赋予了存在类型其名称：我们断言存在某个与签名匹配的具体类型，并且我们将其寻找的任务交给编译器。编译器通常会通过对函数体应用类型推断来找到这个类型。

- 并非所有的impl Trait实例都使用存在类型。如果您在函数的参数位置使用impl Trait，它实际上只是该函数的未命名泛型参数的简写。例如，fn foo(s: impl ToString)在大多数情况下只是fn foo<S: ToString>(s: S)的语法糖。
- 存在类型在实现具有关联类型的特质时特别有用。例如，想象一下，您正在实现IntoIterator特质。它有一个关联类型IntoIter，它保存了可以将所讨论的类型转换为的迭代器的类型。使用存在类型，您不需要定义一个单独的迭代器类型来用于IntoIter。相反，您可以将关联类型指定为impl Iterator<Item = Self::Item>，并在fn into_iter(self)中编写一个求值为迭代器的表达式，例如通过对某个现有迭代器类型进行映射和过滤。

- 存在类型不仅提供了方便，还提供了一个超越方便的功能：它们允许您执行零成本的类型擦除。您可以使用存在类型隐藏底层具体类型，而不是仅仅因为它们出现在公共签名中而导出辅助类型 - 迭代器和futures是常见的例子。您的接口的用户只会看到相关类型实现的特质，而具体类型则作为实现细节留下。这不仅简化了接口，而且还使您可以随意更改该实现，而不会破坏未来的下游代码。

#### `总结`

本章对Rust类型系统进行了全面的回顾。我们既看了编译器如何在内存中显现类型，又看了它如何推理类型本身。这是后续章节中编写不安全代码、复杂应用程序接口和异步代码的重要背景材料。您还会发现，本章中的大部分类型推理都与您设计Rust代码接口的方式有关，我们将在下一章中介绍这一点。

### 3 设计接口

**每个项目，无论大小，都有一个API。实际上，通常有几个API。其中一些是面向用户的，比如HTTP端点或命令行界面，而另一些是面向开发者的，比如库的公共接口。除此之外，Rust的crate还有许多内部接口：每个类型、特质和模块边界都有自己的微型API，与代码的其余部分进行交互。随着代码库的规模和复杂性的增长，您会发现值得投入一些思考和关注，以设计良好的内部API，以使使用和维护代码的体验尽可能愉快。在本章中，我们将讨论一些在Rust中编写符合惯用法的接口的最重要的考虑因素，无论这些接口的用户是您自己的代码还是使用您的库的其他开发者。这些原则基本上可以归结为四个：接口应该是不令人惊讶的、灵活的、明显的和受限制的。我将依次讨论这些原则，为编写可靠和可用的接口提供一些指导。**

- 我强烈建议在阅读本章后查看Rust API指南[（https://rust-lang.github.io/api-guidelines/）]。其中有一个优秀的清单，您可以按照其中的每个建议进行详细的操作。本章中的许多建议也可以通过cargo clippy工具进行检查，如果您尚未运行该工具，请开始运行。我还鼓励您阅读Rust RFC 1105[（https://rust-lang.github.io/rfcs/1105-api-evolution.html）]和The Cargo Book中关于SemVer兼容性的章节[（https://doc.rust-lang.org/cargo/reference/semver.html）]，它们涵盖了在Rust中什么是破坏性更改和什么不是破坏性更改。

#### 不令人惊讶

最小惊讶原则，也被称为最小惊讶法则，在软件工程中经常出现，对于Rust接口也同样适用。在可能的情况下，您的接口应该足够直观，以至于用户在猜测时通常能够猜对。当然，并不是您的应用程序的所有内容都会立即以这种方式直观，但任何可以不令人惊讶的地方都应该是如此。核心思想是紧密围绕用户可能已经了解的事物，以便他们不必以与他们习惯不同的方式重新学习概念。这样，您可以为他们节省脑力来解决实际上与您的接口有关的问题。

- 有多种方法可以使您的接口可预测。在这里，我们将看看如何使用命名、常见特质和人性化特质技巧来帮助用户。

##### 命名规范

用户通过名称首次接触到您的接口；他们会立即从类型、方法、变量、字段和库的名称中推断出一些信息。如果您的接口重用了其他（可能是常见的）接口中的方法和类型的名称，用户将知道他们可以对您的方法和类型做出某些假设。一个名为iter的方法可能会接受&self，并且可能会返回一个迭代器。一个名为into_inner的方法可能会接受self，并且可能会返回某种包装类型。一个名为SomethingError的类型可能会实现std::error::Error，并出现在各种结果中。通过为相同的目的重用常见名称，您使用户更容易猜测事物的功能，并使他们更容易理解您的接口中与其他接口不同的地方。

- 与此相关的是，具有相同名称的事物实际上应该以相同的方式工作。否则，例如，如果您的iter方法接受self，或者您的SomethingError类型没有实现Error，用户可能会根据他们对接口的期望编写错误的代码。他们会感到惊讶和沮丧，并且不得不花时间研究您的接口与他们的期望有何不同。当我们可以避免给用户带来这种摩擦时，我们应该这样做。

##### 类型的常见特质

在Rust中，用户通常会假设接口中的一切“只是工作的”。他们希望能够使用{:?}打印任何类型，并将任何东西发送到另一个线程，他们期望每个类型都是可克隆的。在可能的情况下，我们应该避免让用户感到惊讶，并且尽早实现大多数标准特质，即使我们目前并不需要它们。

- 由于第2章讨论的一致性规则，编译器不允许用户在需要时实现这些特质。用户不能为您接口中的外部类型实现外部特质（如Clone）。相反，他们需要在自己的类型中包装您的接口类型，即使这样，如果没有访问类型的内部，编写合理的实现可能也很困难。
- 首先，这些标准特质中的第一个是Debug特质。几乎每种类型都可以实现Debug特质，即使只是打印类型的名称也可以。在接口中实现Debug特质的最佳方式通常是使用#[derive(Debug)]，但请记住，所有派生的特质都会自动为任何泛型参数添加相同的约束。您也可以通过利用fmt::Formatter上的各种debug_辅助函数来编写自己的实现。
- 紧随其后的是 Rust 的自动 trait Send 和 Sync（以及在较小程度上是 Unpin）。如果一个类型没有实现这些 trait，那应该有一个非常好的理由。一个没有实现 Send 的类型不能放置在 Mutex 中，也不能在包含线程池的应用程序中进行传递使用。一个没有实现 Sync 的类型不能通过 Arc 进行共享，也不能放置在静态变量中。用户已经习惯了在这些上下文中类型可以正常工作，特别是在几乎所有东西都在线程池上运行的异步世界中，如果您不确保您的类型实现了这些 trait，用户会感到沮丧。如果您的类型无法实现这些 trait，请确保将此事实以及原因在文档中进行充分说明！
- 接下来，您应该实现的几乎通用的特质是Clone和Default。这些特质可以很容易地派生或实现，并且对大多数类型来说都是有意义的。如果您的类型无法实现这些特质，请确保在文档中明确说明，因为用户通常希望能够轻松创建更多（和新的）类型实例。如果他们不能，他们会感到惊讶。
- 在期望特质的层次结构中，比较特质：PartialEq、PartialOrd、Hash、Eq和Ord更进一步。特别值得注意的是PartialEq特质，因为用户最终会有两个希望使用==或assert_eq!来比较的实例。即使对于同一类型的相同实例，您的类型可能会比较相等，实现PartialEq也是值得的，以便让用户使用assert_eq!。
- PartialOrd和Hash更为特殊，可能不适用于所有情况，但在可能的情况下，您也应该实现它们。对于用户可能将其用作映射中的键或使用std::collection集合类型进行去重的类型，这一点尤其重要，因为它们往往需要这些约束。Eq和Ord对于实现类型的比较操作有额外的语义要求，超出了PartialEq和PartialOrd的要求。这些要求在这些特质的文档中有详细说明，只有在您确信这些语义实际适用于您的类型时才应该实现它们。
- 对于大多数类型来说，实现serde crate的Serialize和Deserialize特质是有意义的。这些特质可以很容易地派生，serde_derive crate甚至提供了覆盖仅适用于一个字段或枚举变体的序列化的机制。由于serde是一个第三方crate，您可能不希望添加对它的必需依赖。因此，大多数库选择提供一个serde功能，只有在用户选择时才添加对serde的支持。
- 你可能想知道为什么我没有在这一节中包括可派生的 trait Copy。有两个因素使 Copy 与其他提到的 trait 有所不同。首先，用户通常不希望类型是 Copy；相反，他们倾向于认为如果他们想要两个副本，他们必须调用 clone。Copy 改变了移动给定类型的值的语义，这可能会让用户感到惊讶。这与第二个观察结果相关：一个类型很容易停止是 Copy，因为 Copy 类型受到严格限制。一个最初简单的类型很容易最终需要持有一个 String 或其他非 Copy 类型。如果发生这种情况，并且您必须删除 Copy 实现，那将是一个不兼容的变更。相比之下，您很少需要删除 Clone 实现，因此这是一个较轻的承诺。

##### 人性化的特质实现

Rust 不会自动为实现了特质的类型的引用实现特质。换句话说，您通常不能使用 &Bar 调用 fn foo<T: Trait>(t: T)，即使 Bar: Trait。这是因为 Trait 可能包含需要 &mut self 或 self 的方法，显然无法在 &Bar 上调用。然而，对于看到 Trait 只有 &self 方法的用户来说，这种行为可能非常令人惊讶！

- 因此，当您定义一个新的特质时，通常会根据需要为&T where T: Trait，&mut T where T: Trait和Box<T> where T: Trait提供全局实现。根据Trait的方法接收者，您可能只能实现其中一些。标准库中的许多特质具有类似的实现，这样可以减少用户的惊讶。
- 迭代器是另一种情况，您通常会希望在对类型的引用上专门添加特质实现。对于任何可迭代的类型，考虑在适用的情况下为&MyType和&mut MyType实现IntoIterator。这使得使用借用实例的循环也能够正常工作，就像用户期望的那样。

##### 包装类型

Rust在经典意义上没有对象继承。然而，Deref特质及其类似物AsRef提供了一种类似继承的机制。这些特质允许您拥有一个类型为T的值，并通过直接在T类型的值上调用它们来调用类型为U的一些方法，前提是T: Deref<Target = U>。这对用户来说感觉像是魔法，通常非常好用。

- 如果您提供了一个相对透明的包装类型（比如Arc），很有可能您希望实现Deref，以便用户可以通过使用点运算符在内部类型上直接调用方法。如果访问内部类型不需要任何复杂或潜在缓慢的逻辑，您还应该考虑实现AsRef，这样用户就可以轻松地将&WrapperType用作&InnerType。对于大多数包装类型，您还应该在可能的情况下实现`From<InnerType>`和`Into<InnerType>`，以便用户可以轻松地添加或删除您的包装。
- 您可能也遇到过Borrow特质，它与Deref和AsRef非常相似，但实际上是一个稍微不同的东西。具体而言，Borrow专为一个更狭窄的用例而设计：允许调用者提供同一类型的多个本质上相同的变体中的任何一个。它可能本可以被称为Equivalent。例如，对于一个`HashSet<String>`，Borrow允许调用者提供&str或&String。虽然使用AsRef也可以实现相同的效果，但如果没有Borrow的额外要求，即目标类型与实现类型完全相同地实现了Hash、Eq和Ord，那将是不安全的。Borrow还为T、&T和&mut T提供了`Borrow<T>`的全局实现，这使得在trait约束中方便地接受给定类型的拥有或引用值。一般来说，Borrow仅适用于当您的类型与另一个类型本质上等效时，而Deref和AsRef则适用于更广泛地实现您的类型可以“扮演”的任何内容。

  **解引用和固有方法**

- 当存在在T上接受self的方法时，点运算符和Deref的魔力可能会变得令人困惑和意外。例如，给定一个值t: T，不清楚t.frobnicate()是对T还是底层的U进行frobnicate！
- 因此，允许您在某个事先未知的内部类型上透明调用方法的类型应避免使用固有方法。对于Vec来说，它有一个push方法是可以的，即使它解引用为一个slice，因为您知道slice不会很快获得push方法。但是，如果您的类型解引用为一个用户可控制的类型，您添加的任何固有方法也可能存在于该用户可控制的类型上，从而引发问题。在这些情况下，更倾向于使用形式为fn frobnicate(t: T)的静态方法。这样，t.frobnicate()总是调用U::frobnicate，而T::frobnicate(t)可以用于对T本身进行frobnicate。
  
#### 灵活性

您编写的每段代码都包含一个合同，无论是隐式还是显式。合同由一组要求和一组承诺组成。要求是对代码使用方式的限制，而承诺是关于代码使用方式的保证。在设计新接口时，您需要仔细考虑这个合同。一个好的经验法则是避免施加不必要的限制，并只做出您能够遵守的承诺。添加限制或删除承诺通常需要进行重大的语义版本更改，并可能会破坏其他地方的代码。另一方面，放宽限制或提供额外的承诺通常是向后兼容的。

- 在Rust中，限制通常以特质约束和参数类型的形式出现，而承诺通常以特质实现和返回类型的形式出现。例如，比较列表3-1中的三个函数签名。

```rust
fn frobnicate1(s: String) -> String
fn frobnicate2(s: &str) -> Cow<'_, str>
fn frobnicate3(s: impl AsRef<str>) -> impl AsRef<str>

```

列表3-1：具有不同合同的相似函数签名

- 这三个函数签名都接受一个字符串并返回一个字符串，但它们在合同上有很大的区别。
- 第一个函数要求调用者拥有字符串，以String类型的形式，它承诺将返回一个拥有的String。由于合同要求调用者分配并要求我们返回一个拥有的String，我们无法以向后兼容的方式使这个函数无需分配。
- 第二个函数放宽了合同：调用者可以提供任何字符串的引用，因此用户不再需要分配或放弃对String的所有权。它还承诺返回一个std::borrow::Cow，这意味着它可以返回一个字符串引用或一个拥有的String，具体取决于它是否需要拥有该字符串。这里的承诺是函数将始终返回一个Cow，这意味着我们不能将其更改为使用其他优化的字符串表示。调用者还必须明确提供一个&str，因此如果他们有一个预先存在的自己的String，他们必须将其解引用为&str来调用我们的函数。
- 第三个函数放宽了这些限制。它只要求用户传入一个可以产生字符串引用的类型，并且只承诺返回值可以产生字符串引用。
- 这些函数签名中没有一个比其他函数更好。如果您需要在函数中拥有一个字符串的所有权，您可以使用第一个参数类型来避免额外的字符串复制。如果您希望允许调用者利用分配和返回的拥有字符串的情况，具有返回类型为Cow的第二个函数可能是一个不错的选择。然而，我希望您从中得出的结论是，您应该仔细考虑您的接口绑定给您的合同，因为事后更改可能会带来破坏性的影响。
- 在本节的其余部分，我将给出一些常见的接口设计决策示例，以及它们对您的接口合同的影响。

##### 泛型参数

您的接口对用户的一个明显要求是他们必须为您的代码提供的类型。如果您的函数明确接受一个Foo，用户必须拥有并提供一个Foo。没有其他办法。在大多数情况下，使用泛型而不是具体类型会更有回报，允许调用者传递符合函数实际需求的任何类型，而不仅仅是特定类型。将列表3-1中的&str更改为`impl AsRef<str>`就是这种放松要求的示例。通过这种方式放宽要求的一种方法是从参数完全泛型且没有约束开始，然后只需按照编译器错误提示添加所需的约束。

- 然而，如果过度使用这种方法，将使每个函数的每个参数都成为自己的泛型类型，这既难以阅读又难以理解。没有确切的硬性规则来确定何时应该或不应该使给定的参数成为泛型类型，所以请根据自己的判断。一个好的经验法则是，如果您可以想到其他类型的用户可能合理且经常地想要使用而不是您最初使用的具体类型，则将参数设置为泛型。
- 你可能还记得第2章中提到的泛型代码通过单态化为每个使用的类型组合而复制。考虑到这一点，让许多参数成为泛型可能会让你担心过度增大你的二进制文件。在第2章中，我们还讨论了如何使用动态分发来减轻这个问题，通常只会带来可忽略的性能损失，在这里也适用。对于你已经以引用方式接受的参数（记住dyn Trait不是Sized，并且你需要一个宽指针来使用它们），你可以轻松地用使用动态分发的参数替换你的泛型参数。例如，你可以使用`&dyn AsRef<str>`来取代`impl AsRef<str>`。
- 在你开始这样做之前，有几件事情你应该考虑。首先，你代表你的用户做出了这个选择，他们无法选择不使用动态分发。如果你知道你应用动态分发的代码永远不会对性能敏感，那可能没问题。但是如果有一个用户想在他们的高性能应用程序中使用你的库，那么在一个热循环中调用的函数中使用动态分发可能会成为一个绊脚石。其次，在撰写本文时，只有当你有一个简单的特质约束，比如`T: AsRef<str>` 或 `impl AsRef<str>`时，使用动态分发才能工作。对于更复杂的约束，Rust 不知道如何构建动态分发的虚函数表，所以你不能使用 &dyn Hash + Eq 这样的约束。最后，请记住，对于泛型，调用者总是可以通过传递一个特质对象来选择动态分发。反之则不成立：如果你接受一个特质对象，那就是调用者必须提供的。
- 可能会诱人的是从具体类型开始定义接口，然后逐渐将其转换为泛型。这样做是可行的，但请记住，这样的更改不一定向后兼容。为了理解原因，想象一下将函数从`fn foo(v: &Vec<usize>)`更改为fn foo(v: impl AsRef<[usize]>). 虽然每个`&Vec<usize>`都实现了AsRef<[usize]>，但类型推断仍然可能对用户造成问题。考虑一下如果调用者使用foo(&iter.collect())调用foo会发生什么。在原始版本中，编译器可以确定它应该收集到一个Vec中，但现在它只知道它需要收集到某种实现了AsRef<[usize]>的类型中。而且可能有多个这样的类型，所以随着这个更改，调用者的代码将无法编译！

##### 对象安全性

当您定义一个新的trait时，无论该trait是否是对象安全的（请参见第2章“编译和调度”的末尾），都是该trait合同的一个未写明的部分。如果该trait是对象安全的，用户可以使用dyn Trait将实现该trait的不同类型视为单个公共类型。如果不是，编译器将禁止对该trait使用dyn Trait。即使这会稍微降低使用它们的人性化程度（例如，使用impl AsRef而不是&str），您也应该更喜欢使您的trait成为对象安全的，因为对象安全使您的trait能够以新的方式使用。如果您的trait必须具有泛型方法，请考虑将其泛型参数放在trait本身上，或者如果其泛型参数也可以使用动态分发来保持trait的对象安全性。或者，您可以为该方法添加一个where Self: Sized的trait约束，这样只能使用trait的具体实例来调用该方法（而不是通过dyn Trait）。您可以在Iterator和Read traits中看到这种模式的示例，它们是对象安全的，但在具体实例上提供了一些额外的便利方法。

- 对于如何牺牲多少来保持对象安全性的问题，没有一个单一的答案。我的建议是考虑您的trait将如何使用，以及用户是否希望将其用作trait对象。如果您认为用户可能希望同时使用许多不同的trait实例，那么您应该更加努力提供对象安全性，而如果您认为这种用例并不太有意义，那么就不需要那么努力。例如，对于FromIterator trait来说，动态分发并不有用，因为它的一个方法不接受self，所以您根本无法构造一个trait对象。同样地，std::io::Seek作为一个单独的trait对象几乎没有用处，因为您只能对这样的trait对象进行seek操作，而无法进行读取或写入。
**DROP TRAIT OBJECTS**
您可能认为Drop特质作为特质对象也是无用的，因为您只能对其进行丢弃操作。但事实证明，有一些库只想能够丢弃任意类型。例如，一个提供延迟丢弃值的库，例如用于并发垃圾回收或延迟清理，只关心值是否可以被丢弃，而不关心其他任何内容。有趣的是，Drop的故事并不止于此；由于Rust需要能够丢弃特质对象，每个虚函数表都包含了drop方法。实际上，每个dyn Trait也是一个dyn Drop。请记住，对象安全性是您的公共接口的一部分！
如果以与向后兼容的方式修改特质，例如通过添加具有默认实现的方法，但这使得特质不再是对象安全的，您需要提升主要语义版本号。

##### 借用 vs 拥有

对于您在Rust中定义的几乎每个函数、特质和类型，您都必须决定它是拥有数据还是仅持有对数据的引用。您所做的决定将对接口的人性化和性能产生深远的影响。
幸运的是，这些决策往往是自然而然的。
如果您编写的代码需要拥有数据的所有权，例如调用需要self的方法或将数据移动到另一个线程中，它必须存储拥有的数据。当您的代码必须拥有数据时，通常也应该要求调用者提供拥有的数据，而不是通过引用并进行克隆。这样做可以让调用者控制分配，并明确了使用相关接口的成本。

- 另一方面，如果您的代码不需要拥有数据，应该使用引用进行操作。一个常见的例外是对于像i32、bool或f64这样的小类型，直接存储和复制与通过引用存储一样廉价。但是要小心假设这对所有Copy类型都成立；[u8; 8192]是Copy的，但是在各个地方存储和复制它会很昂贵。
- 当然，在现实世界中，事情往往没有那么清晰明了。
有时候，您事先不知道您的代码是否需要拥有数据。例如，String::from_utf8_lossy只有在传递给它的字节序列包含无效的UTF-8序列时才需要拥有该字节序列的所有权。在这种情况下，Cow类型是您的朋友：它允许您在数据允许的情况下操作引用，并在必要时生成拥有的值。
- 有时候，引用的生命周期会使接口变得非常复杂，以至于使用起来非常麻烦。如果您的用户在使用您的接口时遇到编译问题，那就意味着您可能需要（即使是不必要地）获取某些数据的所有权。如果这样做，首先从便宜的可克隆数据或不涉及性能敏感的数据开始，然后再决定是否需要堆分配可能是一个巨大的字节块。

##### 可失败和阻塞的析构函数

以I/O为中心的类型在被丢弃时通常需要执行清理操作。这可能包括将写入刷新到磁盘、关闭文件或优雅地终止与远程主机的连接。执行这些清理操作的自然位置是类型的Drop实现。不幸的是，一旦一个值被丢弃，我们就没有办法向用户传达错误，除非通过panic。在异步代码中也会出现类似的问题，我们希望在有待处理的工作时完成。当调用drop时，执行器可能正在关闭，我们无法再做更多的工作。我们可以尝试启动另一个执行器，但这会带来一系列问题，比如在异步代码中阻塞，正如我们将在第8章中看到的那样。

- 对于这些问题，没有完美的解决方案，无论我们做什么，一些应用程序都不可避免地会回退到我们的Drop实现。因此，我们需要通过Drop提供尽力而为的清理。如果清理出现错误，至少我们尝试过-我们会忽略错误并继续执行。如果执行器仍然可用，我们可能会生成一个未来任务来进行清理，但如果它从未运行，我们已经尽力而为了。
- 然而，我们应该为希望不留下任何悬空线程的用户提供更好的选择。我们可以通过提供显式的析构函数来实现这一点。通常，这采用一个接受 self 所有权并公开与销毁相关的任何错误（使用 -> Result<_,_>) 或异步性（使用 async fn）的方法的形式。然后，细心的用户可以使用该方法来优雅地关闭任何相关资源。
**注意** 确保在文档中突出显示显式析构函数！
- 像往常一样，存在权衡。一旦添加了显式析构函数，您将遇到两个问题。首先，由于您的类型实现了Drop，您无法在析构函数中移动该类型的任何字段。这是因为在显式析构函数运行之后，仍然会调用Drop::drop，并且它需要&mut self，这要求self的任何部分都没有被移动。其次，drop接受的是&mut self，而不是self，因此您的Drop实现不能简单地调用显式析构函数并忽略其结果（因为它不拥有self）。有几种方法可以解决这些问题，但没有一种是完美的。
- 第一种方法是将顶层类型作为一个新类型包装器，包装一个Option，而Option又持有一些内部类型，该内部类型持有所有字段。然后，在两个析构函数中都可以使用Option::take，并且仅在内部类型尚未被取走时调用内部类型的显式析构函数。由于内部类型没有实现Drop，因此您可以拥有所有字段的所有权。这种方法的缺点是，您希望在顶层类型上提供的所有方法现在都必须包含代码，以通过Option（您知道始终是Some，因为尚未调用drop）访问内部类型的字段。

- 第二种解决方法是使每个字段可取。您可以通过将其替换为None（这就是Option::take的作用）来“取走”一个Option，但您也可以对许多其他类型执行此操作。例如，您可以通过将它们替换为廉价构造的默认值（std::mem::take在这里很有用）来取走一个Vec或HashMap。如果您的类型具有合理的“空”值，这种方法非常有效，但如果您必须将几乎每个字段都包装在Option中，然后修改对这些字段的每次访问以匹配unwrap，这将变得乏味。
- 第三种选择是将数据保存在ManuallyDrop类型中，该类型解引用为内部类型，因此不需要解包。您还可以在析构时使用ManuallyDrop::take来获取所有权。这种方法的主要缺点是ManuallyDrop::take是不安全的。没有安全机制来确保您在调用take之后不会尝试使用ManuallyDrop内部的值，或者不会多次调用take。如果这样做，您的程序将悄无声息地表现出未定义的行为，并发生糟糕的事情。
- 最终，您应该选择最适合您应用程序的方法。我倾向于选择第二个选项，并只在您发现自己陷入Option的困境时切换到其他选项。如果代码足够简单，您可以轻松检查代码的安全性，并且对自己的能力有信心，那么ManuallyDrop解决方案是非常好的选择。

#### 显而易见的

虽然一些用户可能熟悉支撑接口的实现的某些方面，但他们不太可能了解其所有规则和限制。他们不会知道在调用bar之后调用foo是不允许的，或者只有在月亮处于47度角且过去18秒内没有人打喷嚏时才能安全地调用不安全的方法baz。只有当接口明确表明发生了奇怪的事情时，他们才会查阅文档或仔细阅读类型签名。因此，对于用户来说，理解您的接口尽可能容易，尽可能难以错误使用是至关重要的。为此，您可以利用文档和类型系统这两种主要技术。让我们依次看看每种技术。
**注意** 您还可以利用命名来向用户暗示接口背后可能有更多内容。如果用户看到一个名为dangerous的方法，他们很可能会阅读其文档。

##### 文档

使接口透明的第一步是编写良好的文档。
我可以写一本专门讲如何编写文档的书，
但让我们在这里专注于与Rust相关的建议。

- 首先，清楚地记录您的代码可能会做一些意外的事情的情况，
或者它依赖于用户做一些超出类型签名规定的事情。Panic是这两种情况的一个很好的例子：
如果您的代码可能会panic，请记录这一事实，以及它可能在哪些情况下会panic。
同样，如果您的代码可能返回错误，请记录它返回错误的情况。
对于不安全的函数，请记录调用者必须保证的条件，以使调用安全。
- 其次，在crate和模块级别包含端到端的使用示例。这些示例比特定类型或方法的示例更重要，
因为它们让用户了解整个接口的结构。通过对接口的高层次理解，
开发人员可能很快意识到特定方法和类型的作用以及它们应该在哪里使用。
端到端的示例还为用户提供了定制使用的起点，他们可以复制粘贴示例，然后根据自己的需求进行修改。
这种“边做边学”的方式往往比让他们从组件中拼凑出来更有效。
**注意** 非常特定于方法的示例，显示len方法确实返回长度的示例，
不太可能告诉用户有关您的代码的新信息。

- 第三，组织您的文档。将所有类型、特质和函数放在单个顶层模块中，会让用户很难找到开始的地方。利用模块将语义相关的项分组在一起。然后，使用内部文档链接将这些项相互链接起来。如果类型A的文档中提到了特质B，那么应该在那里链接到该特质。如果让用户能够轻松探索您的接口，他们就不太可能错过重要的关联或依赖关系。还要考虑使用#[doc(hidden)]标记那些不打算公开但出于遗留原因仍然需要的接口部分，以避免在文档中混乱。
- 最后，在可能的情况下丰富您的文档。链接到解释概念、数据结构、算法或其他方面的外部资源，这些资源可能在其他地方有很好的解释。如果有相关的RFC、博客文章和白皮书，那就很好。使用#[doc(cfg(..))]来突出显示仅在某些配置下可用的项，这样用户就能快速意识到为什么文档中列出的某个方法不可用。使用#[doc(alias = "...")]使类型和方法可以通过其他名称进行搜索和发现。在顶层文档中，指导用户常用的模块、特性、类型、特质和方法。

##### 类型系统泛型

类型系统是确保接口明显、自说明和抗误用的优秀工具。您有几种技术可以使接口很难被误用，从而更有可能被正确使用。
其中之一是语义类型化，您可以添加类型来表示值的含义，而不仅仅是其原始类型。这里的经典示例是布尔值：如果您的函数接受三个布尔参数，很有可能某个用户会搞错值的顺序，并在出现严重问题后才意识到。然而，如果它接受三个不同的两变量枚举类型的参数，用户在没有编译器警告的情况下无法搞错顺序：如果他们试图将DryRun::Yes传递给overwrite参数，那将不起作用，将Overwrite::No传递给dry_run参数也是如此。您还可以将语义类型化应用于布尔值以外的情况。例如，围绕数值类型的新类型可以为包含的值提供单位，或者可以将原始指针参数限制为仅限于由另一个方法返回的指针。

- 一个密切相关的技术是使用零大小的类型来表示关于类型实例的特定事实。例如，考虑一个名为Rocket的类型，它表示真实火箭的状态。Rocket上的一些操作（方法）应该在任何状态下都可用，但有些操作只在特定情况下有意义。例如，如果火箭已经发射，就不可能再次发射。同样，如果火箭尚未发射，可能不应该能够分离燃料箱。我们可以将它们建模为枚举变体，但这样所有的方法都会在每个阶段都可用，并且我们需要引入可能的panic。
- 相反，如图3-2所示，我们可以在Rocket上引入一个泛型参数Stage，并使用它来限制在何时可用哪些方法。

```rust
1 struct Grounded;
struct Launched;
// and so on
struct Rocket<Stage = Grounded> {
2 stage: std::marker::PhantomData<Stage>,
}
3 impl Default for Rocket<Grounded> {}
impl Rocket<Grounded> {
pub fn launch(self) -> Rocket<Launched> { }
}
4 impl Rocket<Launched> {
pub fn accelerate(&mut self) { }
pub fn decelerate(&mut self) { }
}
5 impl<Stage> Rocket<Stage> {
pub fn color(&self) -> Color { }
pub fn weight(&self) -> Kilograms { }
}
```

清单3-2：使用标记类型限制实现

- 我们引入了单元类型来表示火箭的每个阶段1。实际上，我们不需要存储阶段-只需要存储它提供的元信息-因此我们使用PhantomData 2将其存储在后面，以确保它在编译时被消除。然后，我们只在Rocket持有特定类型参数时编写实现块。您只能在地面上（目前）构建火箭，并且只能从地面上发射它3。只有在火箭发射后，您才能控制其速度4。无论火箭处于什么状态，您总是可以执行某些操作，这些操作我们放在一个通用的实现块中5。您会注意到，以这种方式设计接口，用户根本无法在错误的时间调用方法-我们已经将使用规则编码到类型本身中，并使非法状态无法表示。

- 这个概念也适用于许多其他领域；如果您的函数忽略一个指针参数，除非给定的布尔参数为true，最好将这两个参数合并在一起。使用一个枚举类型，其中一个变体表示false（没有指针），另一个变体表示true并持有一个指针，既不会让调用者也不会让实现者误解两者之间的关系。这是一个强大的想法，我强烈鼓励您使用它。
- 在使接口明显的过程中，另一个小而有用的工具是#[must_use]注解。将其添加到任何类型、特质或函数上，如果用户的代码接收到该类型或特质的元素，或者调用该函数，并且没有明确处理它，编译器将发出警告。您可能已经在Result的上下文中见过这个注解：如果一个函数返回一个Result，而您没有在某个地方分配其返回值，您将得到一个编译器警告。但要小心不要过度使用这个注解-只有在用户在不使用返回值时很可能犯错误时才添加它。

#### 受限制的

随着时间的推移，一些用户将依赖于您接口的每个属性，无论是错误还是功能。这对于公开可用的库尤其如此，因为您无法控制用户。因此，在进行用户可见的更改之前，您应该仔细考虑。无论是添加新类型、字段、方法还是特质实现，还是更改现有的内容，您都希望确保更改不会破坏现有用户的代码，并且您计划保留该更改一段时间。频繁的不向后兼容的更改（在语义版本控制中的主要版本增加）肯定会引起用户的不满。

- 许多不向后兼容的更改是显而易见的，比如重命名公共类型或删除公共方法，但有些更改更加微妙，并且与Rust的工作方式紧密相关。在这里，我们将介绍一些棘手的微妙更改以及如何为其进行规划。您将看到，您需要在某些情况下权衡这些更改，以及您希望接口有多灵活-有时，必须做出一些让步。

##### 类型修改

删除或重命名公共类型几乎肯定会破坏某些用户的代码。为了应对这个问题，您应该尽可能利用Rust的可见性修饰符，如pub(crate)和pub(in path)。您拥有的公共类型越少，您就越有自由在以后更改事物而不破坏现有代码。
用户代码可以以更多方式依赖于您的类型，而不仅仅是名称。考虑清单3-3中的公共类型以及给定的代码使用。

```rust
// in your interface
pub struct Unit;
// in user code
let u = lib::Unit;
```

清单3-3：一个看似无害的公共类型
现在考虑一下如果给Unit添加一个私有字段会发生什么。即使您添加的字段是私有的，这个更改仍然会破坏用户的代码，因为他们依赖的构造函数已经消失了。类似地，考虑清单3-4中的代码和用法。

```rust
// in your interface
pub struct Unit { pub field: bool };
// in user code
fn is_true(u: lib::Unit) -> bool {
matches!(u, Unit { field: true })
}
```

清单3-4：访问单个公共字段的用户代码
同样，如果给Unit添加一个私有字段，这将破坏用户的代码，因为Rust的完整模式匹配检查逻辑能够看到用户无法看到的接口的部分。它识别出还有更多的字段，即使用户代码无法访问它们，并拒绝用户的模式作为不完整的。如果我们将元组结构体转换为具有命名字段的常规结构体，也会出现类似的问题：即使字段本身完全相同，任何旧的模式对于新的类型定义将不再有效。
Rust提供了#[non_exhaustive]属性来帮助缓解这些问题。您可以将其添加到任何类型定义中，编译器将禁止在该类型上使用隐式构造函数（例如lib::Unit { field1: true }）和非穷尽模式匹配（即没有尾随, ..的模式）。如果您怀疑将来可能修改某个特定类型，这是一个很好的属性添加。但是，它会限制用户代码，例如剥夺用户依赖穷尽模式匹配的能力，因此如果您认为某个类型可能保持稳定，请避免添加该属性。

##### 特质实现

正如您在第2章中所记得的，Rust的一致性规则禁止为给定类型实现给定特质的多个实现。由于我们不知道下游代码可能添加了什么实现，因此为现有特质添加一个全局实现通常是一个破坏性的更改。对于为现有类型实现外部特质或为外部类型实现现有特质，情况也是如此-在这两种情况下，外部特质或类型的所有者可能同时添加一个冲突的实现，因此这必须是一个破坏性的更改。
删除特质实现是一个破坏性的更改，但为新类型实现特质从来不会成为问题，因为没有一个crate可以有与该类型冲突的实现。
也许令人意外的是，您还需要小心为现有类型实现任何特质。为了理解原因，请考虑清单3-5中的代码。

```rust
// crate1 1.0
pub struct Unit;
put trait Foo1 { fn foo(&self) }
// note that Foo1 is not implemented for Unit
// crate2; depends on crate1 1.0
use crate1::{Unit, Foo1};
trait Foo2 { fn foo(&self) }
impl Foo2 for Unit { .. }
fn main() {
Unit.foo();
}
```

清单3-5：为现有类型实现特质可能会引起问题。

- 如果您在crate1中添加了对Unit的impl Foo1的实现，而没有将其标记为破坏性更改，那么下游代码将突然停止编译，因为对foo的调用现在变得模糊不清。即使是对新的公共特质的实现，如果下游crate使用通配符导入（use crate1::*），这种情况也可能发生。如果您提供了一个预导模块，并指示用户使用通配符导入，那么您尤其需要牢记这一点。
- 对现有特质的大多数更改也是破坏性的更改，比如改变方法签名或添加新方法。改变方法签名会破坏所有实现，并且可能会破坏特质的许多使用方式，而添加新方法只是“仅仅”破坏所有实现。但是，添加具有默认实现的新方法是可以的，因为现有的实现将继续适用。
- 我在这里使用“通常”和“大多数”这样的词，因为作为接口作者，我们有一个工具可以帮助我们规避一些规则：封闭特质。封闭特质是一种只能被其他 crate 使用而不能被实现的特质。这立即使得一些破坏性更改变得非破坏性。例如，您可以向封闭特质添加一个新方法，因为您知道在当前 crate 之外没有其他实现需要考虑。同样地，您可以为新的外部类型实现封闭特质，因为您知道定义该类型的外部 crate 不可能添加冲突的实现。
- 封闭特质最常用于派生特质-为实现特定其他特质的类型提供全局实现的特质。只有在不合理的情况下，才应该封闭一个特质，使外部 crate 无法实现它；这严重限制了特质的实用性，因为下游 crate 将无法为其自己的类型实现该特质。您还可以使用封闭特质来限制可以用作类型参数的类型，例如将清单3-2中的 Rocket 示例中的 Stage 类型限制为仅限 Grounded 和 Launched 类型。清单3-6展示了如何封闭一个特质，以及如何在定义 crate 中为其添加实现。

```rust
pub trait CanUseCannotImplement: sealed::Sealed 1 { .. }
mod sealed {
pub trait Sealed {}
2 impl<T> Sealed for T where T: TraitBounds {}
}
impl<T> CanUseCannotImplement for T where T: TraitBounds {}
```

清单3-6：如何封闭一个特质并为其添加实现
诀窍是将一个私有的、空的特质作为要封闭的特质的超特质添加进去1。由于超特质在一个私有模块中，其他 crate 无法访问它，因此也无法实现它。封闭特质要求底层类型实现 Sealed，因此只有我们明确允许的类型2才能最终实现该特质。
**注意**：如果您以这种方式封闭一个特质，请确保记录下这个事实，以免用户尝试自己实现该特质而感到沮丧！

##### 隐藏的约定

有时，您对代码的某一部分所做的更改会以微妙的方式影响接口中的其他部分的约定。这种情况主要发生在重新导出和自动特质中。
**重新导出**

如果您的接口的任何部分暴露了外部类型，那么对其中一个外部类型的任何更改也是对您的接口的更改。例如，考虑一下如果您将依赖项的新主要版本作为迭代器类型暴露在您的接口中会发生什么。依赖于您接口的用户可能还直接依赖于该依赖项，并期望您接口提供的类型与该依赖项中同名的类型相同。但是，如果您更改了依赖项的主要版本，即使类型的名称相同，这也不再成立。

清单3-7展示了一个例子。

```rust
// your crate: bestiter
pub fn iter<T>() -> itercrate::Empty<T> { .. }
// their crate
struct EmptyIterator { it: itercrate::Empty<()> }

EmptyIterator { it: bestiter::iter() }
```

清单3-7：重新导出使外部 crate 成为接口契约的一部分。

- 如果您的 crate 从 itercrate 1.0 移动到 itercrate 2.0，但其他方面没有改变，那么此清单中的代码将不再编译。即使没有更改任何类型，编译器也认为（正确地）itercrate1.0::Empty 和 itercrate2.0::Empty 是不同的类型。因此，您无法将后者赋值给前者，这使得这个接口的更改是破坏性的。
- 为了缓解这类问题，通常最好使用新类型模式对外部类型进行封装，然后仅暴露您认为有用的外部类型的部分。在许多情况下，您可以通过使用impl Trait来为调用方提供仅最小的合约，从而完全避免使用新类型包装器。通过承诺更少，您可以减少破坏性的更改。
**语义版本控制技巧**
- - itercrate的例子可能让您感到不舒服。如果Empty类型没有改变，那么为什么编译器不允许使用它的任何代码继续工作，无论代码是使用版本1.0还是2.0？答案是...复杂的。归根结底，这是因为Rust编译器不会假设只因为两个类型具有相同的字段，它们就是相同的类型。举个简单的例子，假设itercrate 2.0为Empty添加了`#[derive(Copy)]`。现在，这个类型在使用1.0或2.0时具有不同的移动语义！而针对其中一个版本编写的代码将无法与另一个版本一起工作。
- - 这个问题往往在大型、广泛使用的库中出现，在这些库中，随着时间的推移，破坏性的更改很可能发生在 crate 的某个地方。不幸的是，语义化版本控制是在 crate 级别上进行的，而不是类型级别上进行的，因此任何地方的破坏性更改都是全局的破坏性更改。
- - 但并非一切都失去了。几年前，David Tolnay（serde的作者，以及其他大量Rust贡献的作者）提出了一个巧妙的技巧来处理这种情况。他称之为“语义版本技巧”。这个想法很简单：如果某个类型T在破坏性更改（从1.0到2.0）之后保持不变，那么在发布2.0之后，您可以发布一个新的1.0次要版本，该版本依赖于2.0，并用来自2.0的T重新导出替换T。

- - 通过这样做，您确保在两个主要版本之间只有一个类型T。这反过来意味着依赖于1.0的任何crate都可以使用来自2.0的T，反之亦然。而且，由于这仅适用于您使用此技巧明确选择的类型，实际上是破坏性的更改将继续存在。

##### 自动特质

Rust拥有一些特质，根据类型的内容自动为每个类型实现。对于本讨论而言，最相关的是Send和Sync，尽管Unpin、Sized和UnwindSafe特质也存在类似的问题。根据它们的本质，这些特质为您的接口中几乎每个类型添加了一个隐藏的承诺。这些特质甚至会传播到像impl Trait这样的类型擦除类型中。

- 这些特质的实现（通常）由编译器自动添加，但这也意味着如果它们不再适用，它们将不会自动添加。因此，如果您有一个包含私有类型B的公共类型A，并且您更改B以使其不再是Send，那么A现在也不再是Send。这是一个破坏性的更改！
- 这些更改很难跟踪，并且通常直到接口的用户抱怨他们的代码不再工作时才会发现。为了在发生之前捕捉到这些情况，最好在测试套件中包含一些简单的测试，检查所有类型是否按照您的预期实现了这些特质。清单3-8给出了一个这样的测试的示例。

```rust
fn is_normal<T: Sized + Send + Sync + Unpin>() {}
#[test]
fn normal_types() {
is_normal::<MyType>();
}
```

清单3-8：测试类型是否实现了一组特质

- **注意**，这个测试不运行任何代码，只是测试代码是否编译通过。如果MyType不再实现Sync，测试代码将无法编译，您将知道刚刚进行的更改破坏了自动特质的实现。
**隐藏文档中的项目**
`#[doc(hidden)] 属性`允许您在文档中隐藏公共项目，而不会使其对知道其存在的代码不可访问。这通常用于公开宏所需的方法和类型，但不用于用户代码。隐藏的项目与接口契约的交互方式存在一些争议。一般来说，标记为#[doc(hidden)]的项目只在其公共影响方面被视为接口契约的一部分；例如，如果用户代码可能包含一个隐藏的类型，那么该类型是否是Send是契约的一部分，而其名称则不是。隐藏的内在方法和封闭特质上的隐藏特质方法通常不是接口契约的一部分，但您应确保在这些方法的文档中明确说明这一点。是的，隐藏的项目仍然应该被记录在文档中！

#### Summary

在本章中，我们探讨了设计Rust接口的许多方面，无论是用于外部使用还是作为您的crate中不同模块之间的抽象边界。我们涵盖了许多具体的陷阱和技巧，但最终，高级原则应该指导您的思考：您的接口应该是不令人惊讶、灵活、明显和受限制的。在下一章中，我们将深入探讨如何在Rust代码中表示和处理错误。

### 4.错误处理

对于除了最简单的程序之外，您将会有可能失败的方法。在本章中，我们将探讨表示、处理和传播这些失败的不同方式，以及每种方式的优点和缺点。我们将首先探讨不同的错误表示方式，包括枚举和擦除，然后研究一些需要不同表示技术的特殊错误情况。接下来，我们将看一些处理错误的方法以及错误处理的未来发展方向。
值得注意的是，Rust中的错误处理最佳实践仍然是一个活跃的讨论话题，在撰写本文时，生态系统尚未就单一统一的方法达成一致。因此，本章将重点介绍基本原则和技术，而不是推荐特定的crate或模式。

#### 表示错误

当你编写可能失败的代码时，最重要的问题是要问自己的是用户将如何与返回的任何错误进行交互。用户是否需要知道确切发生了哪个错误以及出了什么问题的细节，还是他们只需记录错误发生并尽力继续进行？为了理解这一点，我们必须看看错误的性质是否可能影响调用者在接收到错误时的操作。这反过来将决定我们如何表示不同的错误。

- 表示错误有两种主要选项：枚举和擦除。也就是说，您可以让错误类型枚举可能的错误条件，以便调用者可以区分它们，或者您可以只向调用者提供一个不透明的错误。让我们依次讨论这两个选项。

##### 枚举

对于我们的示例，我们将使用一个库函数，该函数将字节从某个输入流复制到某个输出流，类似于std::io::copy。用户提供两个流，一个用于读取，一个用于写入，您将字节从一个流复制到另一个流。在此过程中，任何一个流都有可能失败，此时复制过程必须停止并向用户返回错误。在这种情况下，用户可能想知道是输入流还是输出流发生了错误。例如，在Web服务器中，如果在向客户端流式传输文件时发生输入流错误，可能是因为磁盘被弹出，而如果输出流发生错误，可能是客户端断开连接。后者可能是服务器应该忽略的错误，因为可以继续将数据复制到新的连接，而前者可能需要整个服务器关闭！

这是一个我们希望枚举错误的情况。用户需要能够区分不同的错误情况，以便能够适当地做出响应，因此我们使用了一个名为CopyError的枚举，其中每个变体表示错误的不同根本原因，就像在清单4-1中所示。

```rust
pub enum CopyError {
In(std::io::Error),
Out(std::io::Error),
}
```

清单4-1：一个枚举的错误类型

- 每个变体还包括遇到的错误，以尽可能提供给调用者有关出错的详细信息。
- 当创建自己的错误类型时，您需要采取一些步骤，使错误类型与Rust生态系统的其他部分良好地协同工作。首先，您的错误类型应实现std::error::Error trait，该trait为调用者提供了用于检查错误类型的常用方法。最感兴趣的主要方法是Error::source，它提供了一种查找错误根本原因的机制。这通常用于打印回溯，显示一直追溯到错误的根本原因。对于我们的CopyError类型，source的实现很简单：我们匹配self并提取并返回内部的std::io::Error。
- 第二，您的类型应该实现Display和Debug，以便调用者可以有意义地打印您的错误。如果您实现了Error trait，则需要这样做。一般来说，您的Display实现应该提供一个简短的描述，说明出了什么问题，可以轻松地与其他错误消息合并。显示格式应该是小写的，不带尾部标点符号，以便它可以很好地适应其他更大的错误报告。Debug应该提供更详细的错误信息，包括可能有助于追踪错误原因的辅助信息，例如端口号、请求标识符、文件路径等。通常，使用#[derive(Debug)]就足够了。
**注意** 在旧的 Rust 代码中，您可能会看到对 Error::description 方法的引用，但这已被弃用，推荐使用 Display。
- 第三，如果可能的话，您的类型应该实现Send和Sync，以便用户能够在线程边界上共享错误。如果您的错误类型不是线程安全的，那么在多线程环境中几乎不可能使用您的crate。实现Send和Sync的错误类型也更容易与非常常见的std::io::Error类型一起使用，该类型能够包装实现Error、Send和Sync的错误。当然，并非所有的错误类型都可以合理地实现Send和Sync，比如如果它们与特定的线程本地资源相关联，那就没关系。您可能也不会将这些错误发送到线程边界之外。然而，在将Rc<String>和RefCell<bool>类型放入错误中之前，这是需要注意的事项。
- 最后，如果可能的话，您的错误类型应该是 'static。这样做的最直接好处是，它使调用者更容易将您的错误传播到调用堆栈上，而不会遇到生命周期问题。它还使您的错误类型更容易与类型擦除的错误类型一起使用，我们很快就会看到。

##### 不透明错误

现在让我们考虑一个不同的例子：一个图像解码库。您将一堆字节提供给库进行解码，并且库会为您提供各种图像操作方法。如果解码失败，用户需要能够找出如何解决问题，因此必须了解失败的原因。但是，是否重要的是原因是图像头中的大小字段无效，还是压缩算法无法解压缩块？可能不重要 - 即使应用程序知道确切的原因，也无法有意义地从任何一种情况中恢复。在这种情况下，作为库的作者，您可能希望提供单个不透明的错误类型。这也使得您的库更易于使用，因为在任何地方都只使用一个错误类型。此错误类型应实现Send、Debug、Display和Error（包括适当的source方法），但除此之外，调用者不需要了解更多信息。您可能在内部表示更细粒度的错误状态，但没有必要将其暴露给库的用户。这样做只会增加API的大小和复杂性，没有任何好处。

- 确定不透明错误类型的具体形式主要取决于您。它可以是一个具有所有私有字段的类型，仅公开有限的用于显示和检查错误的方法，或者它可以是一个严重类型擦除的错误类型，如Box<dyn Error + Send + Sync + 'static>，它只透露它是一个错误，通常不允许用户进行内省。决定将错误类型设置为多么不透明主要取决于错误是否除了其描述之外还有其他有趣的内容。使用`Box<dyn Error>`，您只能将错误向上传递给用户。如果它确实没有任何有价值的信息要向用户呈现，那可能没问题，例如，如果它只是一个动态错误消息或来自程序内部更深层次的大量不相关错误之一。但是，如果错误具有一些有趣的方面，例如行号或状态码，您可能希望通过一个具体但不透明的类型来公开它。
**注意** 一般来说，社区共识是错误应该很少发生，因此不应该对“正常路径”增加太多成本。因此，错误通常被放置在指针类型（如Box或Arc）后面。这样，它们不太可能对它们所包含的整体Result类型的大小产生太大影响。
- 使用类型擦除错误的一个好处是，它允许您轻松地组合来自不同来源的错误，而无需引入额外的错误类型。也就是说，类型擦除错误通常可以很好地组合，并允许您表示一组开放的错误。如果您编写的函数的返回类型是`Box<dyn Error + ...>`，那么您可以在该函数内部使用`?`处理不同类型的错误，这些错误可能来自各种不同的错误，并且它们都将转换为相同的错误类型。
- 在擦除的上下文中，Box<dyn Error + Send + Sync + 'static> 上的 'static 约束值得花更多时间来讨论。我在前一节中提到，它对于让调用者传播错误而不必担心失败的方法的生命周期边界非常有用，但它还有一个更重要的目的：访问 downcasting。Downcasting 是将一个类型的项转换为更具体类型的过程。这是 Rust 提供的少数几种可以在运行时访问类型信息的情况之一；它是动态语言通常提供的更一般类型反射的有限情况。在错误的上下文中，downcasting 允许用户将 dyn Error 转换为原始的具体错误类型，前提是该 dyn Error 最初就是该类型。例如，用户可能希望在收到的错误是 std::io::Error 类型的 std::io::ErrorKind::WouldBlock 时采取特定的操作，但在其他情况下不采取相同的操作。如果用户得到了 dyn Error，他们可以使用 Error::downcast_ref 尝试将错误向下转换为 std::io::Error。downcast_ref 方法返回一个 Option，告诉用户 downcast 是否成功。这里的关键观察是：downcast_ref 仅在参数是 'static 的情况下起作用。如果我们返回一个不是 'static 的不透明错误，我们就剥夺了用户进行这种错误内省的能力，即使他们希望这样做。
- 在生态系统中，关于库的类型擦除错误（或更一般地说，类型擦除类型）是否属于其公共和稳定的API存在一些争议。也就是说，如果您的库中的方法foo返回一个`Box<dyn Error>`作为lib::MyError，将foo更改为返回不同的错误类型是否会导致破坏性更改？类型签名没有改变，但用户可能已经编写了假设他们可以使用downcast将该错误转换回lib::MyError的代码。我对此问题的观点是，您选择返回`Box<dyn Error>`（而不是lib::MyError）是有原因的，并且除非明确记录，否则不保证downcasting方面的任何特定内容。

**注意**，虽然`Box<dyn Error + ...>` 是一种吸引人的类型擦除错误类型，但它本身并没有实现 Error。因此，在实现了 Error 的库中，考虑添加自己的 BoxError 类型以进行类型擦除。你可能会想知道 Error::downcast_ref 如何安全地工作。也就是说，它如何知道提供的 dyn Error 参数是否确实是给定的类型 T？标准库甚至有一个叫做 Any 的 trait，它对任何类型都实现了，并且为 dyn Any 实现了 downcast_ref ——这怎么可能没问题呢？答案在于编译器支持的类型 std::any::TypeId，它允许你为任何类型获取一个唯一的标识符。Error trait 有一个隐藏的提供的方法叫做 type_id，默认实现是返回 `TypeId::of::<Self>()`。类似地，Any 有一个针对 T 的通用实现 impl Any for T，在该实现中，它的 type_id 返回相同的值。在这些 impl 块的上下文中，Self 的具体类型是已知的，所以这个 type_id 就是真实类型的类型标识符。这提供了 downcast_ref 需要的所有信息。downcast_ref 调用 self.type_id，通过动态大小类型的虚函数表（参见第2章）转发到底层类型的实现，并将其与提供的 downcast 类型的类型标识符进行比较。如果它们匹配，那么 dyn Error 或 dyn Any 后面的类型确实是 T，从一个引用到另一个引用的转换是安全的。

##### 特殊错误情况

有些函数可能会失败，但如果它们失败了，就无法返回任何有意义的错误。从概念上讲，这些函数的返回类型是 Result<T, ()>。在某些代码库中，您可能会看到它表示为 Option<T>。虽然这两种选择都是合法的函数返回类型，但它们传达了不同的语义含义，通常应避免将 Result<T, ()> “简化”为 Option<T>。Err(()) 表示操作失败，应该重试、报告或以其他方式进行特殊处理。另一方面，None 只表示函数没有返回值；通常不被视为异常情况或需要处理的情况。您可以在 Result 类型上看到 #[must_use] 注解 - 当您获得一个 Result 时，语言期望您重视处理两种情况，而对于 Option，实际上不需要处理任何一种情况。

**注意** 还要记住，`()` 不实现 `Error` 特质。这意味着它不能被类型擦除为 `Box<dyn Error>`，并且在使用 `?` 时可能会有些麻烦。因此，在这些情况下，通常最好定义自己的单元结构类型，为其实现 `Error`，并将其作为错误类型，而不是使用 `()`。

- 有些函数，比如那些启动一个持续运行的服务器循环的函数，只会返回错误；除非发生错误，它们会一直运行。其他一些函数永远不会出错，但仍然需要返回一个Result，例如为了匹配一个trait的签名。对于这样的函数，Rust提供了never类型，使用!语法来表示。never类型表示一个永远不会生成的值。你不能自己构造这种类型的实例——唯一的方法是进入一个无限循环或者panic，或者通过一些其他特殊的操作，编译器知道这些操作永远不会返回。对于Result来说，当你有一个你知道永远不会被使用的Ok或Err时，你可以将其设置为!类型。如果你编写一个返回Result<T, !>的函数，你将无法返回Err，因为唯一的方法是进入永远不会返回的代码。因为编译器知道任何包含!的变体永远不会被生成，所以它也可以根据这一点对你的代码进行优化，比如在Result<T, !>上的unwrap不会生成panic代码。而且当你进行模式匹配时，编译器知道任何包含!的变体甚至不需要列出。非常巧妙！
- 最后一个奇特的错误类型是std::thread::Result。这是它的定义：

```rust
type Result<T> = Result<T, Box<dyn Any + Send + 'static>>;

```

错误类型是类型擦除的，但它并不像我们之前看到的那样被擦除为dyn Error。相反，它是一个dyn Any，它只保证错误是某种类型，没有更多的信息...这并不是一个很好的保证。这个奇怪的错误类型之所以存在，是因为std::thread::Result的错误变体只在发生panic时产生；具体来说，如果您尝试加入一个已经panic的线程。在这种情况下，加入线程除了忽略错误或使用unwrap自己发生panic之外，似乎没有其他什么可以做的。本质上，错误类型是“一个panic”，值是“传递给panic!的任何参数”，它确实可以是任何类型（尽管通常是格式化的字符串）。

##### 传播错误

Rust的?运算符是unwrap或提前返回的简写形式，用于轻松处理错误。但它还有一些其他值得了解的技巧。首先，?通过From trait执行类型转换。在返回Result<T, E>的函数中，您可以在任何Result<T, X>上使用?，其中E: From<X>。这就是使通过Box<dyn Error>进行错误擦除如此吸引人的特性；您可以随处使用?，而不用担心特定的错误类型，它通常会“正常工作”。

**FROM AND INTO**

标准库中有许多转换特质，但其中两个核心特质是 From 和 Into。如果我们有了 From，为什么还需要 Into，反之亦然，这可能让你感到奇怪。有几个原因，但让我们从历史原因开始：由于第2章中讨论的一致性规则，早期的 Rust 不可能只有一个特质。或者更具体地说，一致性规则曾经是什么样子。

- 假设您想在您的crate中实现某个本地类型与标准库中某个类型之间的双向转换。您可以很容易地编写`impl<T> From<Vec<T>> for MyType<T>`和`impl<T> Into<Vec<T>> for MyType<T>`，但如果您只有From或Into，您将不得不编写`impl<T> From<MyType<T>> for Vec<T>`或`impl<T> Into<MyType<T>> for Vec<T>`。然而，编译器过去会拒绝这些实现！只有在Rust 1.41.0之后，当为覆盖类型添加了一项例外规则到一致性规则中时，它们才合法。在那个改变之前，必须同时具备这两个特质。由于大部分Rust代码是在Rust 1.41.0之前编写的，现在无法删除任何一个特质。

- 除了这个历史事实之外，还有一些良好的人机工程学原因，即使我们今天可以从头开始，也要同时拥有这两个特质。在不同的情况下，使用其中一个特质通常要容易得多。例如，如果您正在编写一个接受可以转换为Foo的类型的方法，您是希望编写fn(impl Into<Foo>)还是fn<T>(T) where Foo: From<T>？相反地，要将字符串转换为语法标识符，您是希望编写Ident::from("foo")还是<_ as Into<Ident>>::into("foo")？这两个特质都有各自的用途，我们最好同时拥有它们。
- 鉴于我们两者都有，您可能想知道在您的代码中应该使用哪个。事实证明，答案非常简单：实现From，并在边界中使用Into。原因是Into对于任何实现了From的T都有一个默认实现，所以无论类型是否显式实现了From或Into，它都实现了Into！
- 当然，就像简单的事情经常发生的那样，故事并没有就此结束。由于编译器在使用Into作为约束时通常需要“穿过”泛型实现，判断一个类型是否实现了Into比实现From更复杂。在某些情况下，编译器并不聪明到足以解决这个难题。因此，在撰写本文时，?运算符使用的是From而不是Into。大多数情况下，这并没有什么区别，因为大多数类型都实现了From，但这意味着旧库中实现了Into而不是From的错误类型可能无法与?一起使用。随着编译器变得更加智能，?可能会“升级”为使用Into，这样问题就会消失，但目前我们只能这样。

- ?的第二个要注意的方面是，这个操作符实际上只是一个被暂时称为Try的特质的语法糖。在撰写本文时，Try特质尚未稳定，但在您阅读本文时，很可能已经确定了它或类似的东西。由于细节尚未完全确定，我只会给出Try如何工作的概要，而不是完整的方法签名。在本质上，Try定义了一个包装类型，其状态要么是进一步计算有用的（即happy path），要么是不可用的。其中一些人可能会正确地想到monad，尽管我们不会在这里探讨这种联系。例如，在Result<T, E>的情况下，如果你有一个Ok(t)，你可以通过解包t继续happy path。另一方面，如果你有一个Err(e)，你希望立即停止执行并产生错误值，因为你没有t，所以无法进行进一步的计算。
- 有趣的是，Try适用于更多类型，而不仅仅是Result。例如，Option<T>遵循相同的模式 - 如果你有一个Some(t)，你可以继续在happy path上，而如果你有一个None，你希望返回None而不是继续。这种模式扩展到更复杂的类型，比如Poll<Result<T, E>>，其happy path类型是Poll<T>，这使得?适用于更多情况。当Try稳定下来时，我们可能会看到?开始与各种类型一起工作，使我们的happy path代码更加优雅。

- ?运算符已经可以在可失败函数、doctest和fn main中使用。然而，为了发挥其全部潜力，我们还需要一种方式来限定这种错误处理的范围。例如，考虑列表4-2中的函数。

```rust
fn do_the_thing() -> Result<(), Error> {
    let thing = Thing::setup()?;
    // .. code that uses thing and ? ..
    thing.cleanup();
    Ok(())
}
```

清单4-2：使用?运算符的多步骤可失败函数
这不会按预期工作。在setup和cleanup之间的任何?都会导致整个函数提前返回，从而跳过清理代码！这是try块旨在解决的问题。try块的行为几乎类似于单次迭代循环，其中?使用break而不是return，并且块的最后一个表达式具有隐式的break。我们现在可以修复清单4-2中的代码，以始终执行清理操作，如清单4-3所示。

```rust
fn do_the_thing() -> Result<(), Error> {
    let thing = Thing::setup()?;
    let r = try {
        // .. code that uses thing and ? ..
    };
    thing.cleanup();
    r
}
```

清单4-3：一个多步骤的可失败函数，总是在完成后进行清理

在撰写本文时，try块还不稳定，但是对于它们的有用性已经达成了足够的共识，它们很可能以类似于此处描述的形式出现。

#### 总结

本章介绍了在Rust中构建错误类型的两种主要方法：枚举和擦除。我们讨论了何时使用每种方法以及每种方法的优点和缺点。我们还深入探讨了?运算符的一些幕后细节，并考虑了?在未来可能变得更加有用的情况。在下一章中，我们将从代码中退后一步，看看如何组织一个Rust项目。我们将讨论特性标志、依赖管理和版本控制，以及如何使用工作区和子模块来管理更复杂的crate。我们在下一页见！

### 5.项目结构

本章提供了一些关于组织Rust项目的思路。对于简单的项目，由cargo new设置的结构可能是你很少考虑的事情。你可以添加一些模块来拆分代码，添加一些依赖来增加功能，但仅此而已。然而，随着项目的规模和复杂性的增长，你会发现需要超越这个范围。也许你的crate的编译时间过长，或者你需要条件依赖，或者你需要更好的持续集成策略。在本章中，我们将介绍一些Rust语言和Cargo提供的工具，这些工具可以更轻松地管理这些问题。

#### 特性

特性是Rust中定制项目的主要工具。在其核心，特性只是一个构建标志，crate可以传递给它们的依赖项，以添加可选功能。特性本身没有语义意义-相反，您可以选择为您的crate定义特性的含义。

- 通常，我们使用特性有三种方式：启用可选依赖项，有条件地包含 crate 的其他组件，以及增强代码的行为。请注意，所有这些用法都是累加的；特性可以增加 crate 的功能，但通常不应该删除模块或替换类型或函数签名。这源于一个原则，即如果开发人员对他们的 Cargo.toml 进行简单的更改，比如添加一个新的依赖项或启用一个特性，那么这不应该导致他们的 crate 停止编译。如果一个 crate 有互斥的特性，这个原则很快就会被抛弃——如果 crate A 依赖于 crate C 的一个特性，而 crate B 依赖于 C 的另一个互斥特性，那么添加对 crate B 的依赖将会破坏 crate A！因此，我们通常遵循这样的原则：如果 crate A 在某个特性集上编译通过了 crate C，那么在 crate C 上启用所有特性时，它也应该能够编译通过。
- Cargo非常强调这个原则。例如，如果两个crate（A和B）都依赖于crate C，但它们分别在C上启用了不同的特性，Cargo只会编译一次crate C，并使用A或B所需的所有特性。也就是说，它会对A和B对C请求的特性取并集。因此，向Rust crate添加互斥特性通常很困难；很有可能有两个依赖项会依赖于具有不同特性的crate，如果这些特性是互斥的，那么下游crate将无法构建。

**注意** 我强烈建议您配置持续集成基础设施，以检查您的crate在其特性的任何组合下是否编译通过。一个帮助您完成这个任务的工具是cargo-hack，您可以在<https://github.com/taiki-e/cargo-hack/>找到。

##### 定义和包含特性

特性是在Cargo.toml中定义的。清单5-1展示了一个名为foo的crate的示例，其中包含一个简单的特性，用于启用可选依赖syn。

```toml
[package]
name = "foo"
...
[features]
derive = ["syn"]
[dependencies]
syn = { version = "1", optional = true }
```

清单5-1：启用可选依赖项的特性
当Cargo编译这个crate时，默认情况下不会编译syn crate，这样可以减少编译时间（通常会显著减少）。只有当下游crate需要使用由derive特性启用的API，并明确选择启用它时，syn crate才会被编译。清单5-2展示了这样一个下游crate bar如何启用derive特性，并包含syn依赖项。

```toml
[package]
name = "bar"
...
[dependencies]
foo = { version = "1", features = ["derive"] }
```

清单5-2：启用依赖项的特性

- 有些特性使用得非常频繁，因此更合理的做法是让一个crate选择性地禁用它们，而不是选择性地启用它们。为了支持这一点，Cargo允许您为一个crate定义一组默认特性。同样地，它也允许您选择性地禁用依赖项的默认特性。清单5-3展示了foo如何使其derive特性默认启用，并选择性地禁用syn的一些默认特性，而只启用它所需的用于derive特性的特性。

```toml
[package]
name = "foo"
...
[features]
derive = ["syn"]
default = ["derive"]
[dependencies.syn]
version = "1"
default-features = false
features = ["derive", "parsing", "printing"]
optional = true
```

清单5-3：添加和选择性禁用默认特性，以及可选依赖项

- 在这里，如果一个crate依赖于foo，并且没有明确选择禁用默认特性，它将编译foo的syn依赖项。反过来，syn将只使用列出的三个特性进行构建，而不使用其他特性。通过这种方式选择禁用默认特性，并仅选择您需要的特性，是减少编译时间的好方法！

**将可选依赖项作为特性**

当您定义一个特性时，等号后面的列表本身就是一个特性列表。这可能一开始听起来有点奇怪-在清单5-3中，syn是一个依赖项，而不是一个特性。事实证明，Cargo将每个可选依赖项都作为具有相同名称的特性。如果您尝试添加与可选依赖项同名的特性，您将看到这一点；Cargo不会允许这样做。在Cargo中，特性和依赖项的不同命名空间的支持正在进行中，但在撰写本文时尚未稳定。与此同时，如果您想要一个以依赖项命名的特性，您可以使用package = ""重命名依赖项，以避免名称冲突。特性启用的特性列表还可以包括依赖项的特性。

- 例如，您可以编写 derive = ["syn/derive"] 来使您的 derive 特性启用 syn 依赖项的 derive 特性。

##### 在您的crate中使用特性

在使用特性时，您需要确保您的代码仅在依赖可用时使用。如果您的特性启用了特定的组件，您需要确保如果特性未启用，则不包含该组件。

- 你可以使用条件编译来实现这一点，它允许你使用注解来给出特定代码在何种条件下应该或不应该被编译。条件编译主要通过 #[cfg] 属性来表达。还有一个紧密相关的 cfg! 宏，它允许你根据类似的条件来改变运行时行为。你可以用条件编译做很多有趣的事情，我们将在本章后面看到，但最基本的形式是 #[cfg(feature = "some-feature")]，它使得源代码中的下一个“东西”只在启用了 some-feature 特性时才被编译。类似地，如果 cfg!(feature = "some-feature") 等价于 if true，只有在启用了 derive 特性时（否则为 false）。
- `#[cfg]`属性比cfg!宏更常用，因为宏根据特性修改运行时行为，这可能会导致难以确保特性是可添加的。您可以将#[cfg]放在某些Rust项（如函数和类型定义、impl块、模块和use语句）以及某些其他结构（如结构体字段、函数参数和语句）之前。但是，#[cfg]属性不能随意放置；Rust语言团队精心限制了它的出现位置，以避免条件编译引发过于奇怪和难以调试的情况。
- 请记住，修改API的某些公共部分可能会意外地使特性不可添加，从而使一些用户无法编译您的crate。在这里，您通常可以使用向后兼容更改的规则作为经验法则-例如，如果您将枚举变体或公共结构字段条件化为一个特性，那么该类型也必须用#[non_exhaustive]进行注解。否则，如果由于依赖树中的第二个crate添加了该特性，没有启用该特性的依赖crate可能无法再编译。
**注意** 如果您正在编写一个大型的crate，您预计用户只需要其中的一部分功能，那么您应该考虑通过特性来保护较大的组件（通常是模块）。这样，用户可以选择性地启用他们真正需要的部分，并支付编译的成本。

#### 工作区

在Rust中，crate扮演着多种角色——它们是依赖关系图中的顶点，是特质一致性的边界，也是编译特性的作用域。因此，每个crate都被视为一个单独的编译单元；Rust编译器将一个crate几乎视为一个大的源文件，作为一个整体进行编译，并最终转换为单个二进制输出（可以是可执行文件或库）。

- 虽然这简化了编译器的许多方面，但也意味着处理大型 crate 可能会很困难。如果您对应用程序的某个部分进行了单元测试、注释或类型的更改，编译器必须重新评估整个 crate，以确定是否有任何更改。在内部，编译器实现了许多机制来加速这个过程，比如增量重新编译和并行代码生成，但最终您的 crate 的大小是确定项目编译时间的一个重要因素。
- 因此，随着项目的增长，您可能希望将其拆分为相互依赖的多个crate。Cargo正好有一个方便的功能来实现这一点：工作区。工作区是一组crate（通常称为子crate），它们由顶层的Cargo.toml文件绑定在一起，如清单5-4所示。

```toml
[workspace]
members = [
"foo",
"bar/one",
"bar/two",
]
```

清单5-4：工作区的 Cargo.toml

- members 数组是一个包含工作区中每个 crate 的目录列表。这些 crate 都有自己的 Cargo.toml 文件在自己的子目录中，但它们共享一个单独的 Cargo.lock 文件和一个单独的输出目录。crate 的名称不需要与 members 中的条目匹配。通常情况下，工作区中的 crate 具有相同的名称前缀，通常选择为“主”crate 的名称。例如，在 tokio crate 中，成员被称为 tokio、tokio-test、tokio-macros 等等。
- 工作区最重要的特性之一是您可以在工作区的根目录中通过调用 cargo 与所有工作区成员进行交互。想要检查它们是否都编译通过？cargo check 将检查它们全部。想要运行所有测试？cargo test 将测试它们全部。虽然不像将所有内容放在一个 crate 中那样方便，所以不要将所有内容拆分成微小的 crate，但这是一个相当好的近似。
**注意** 在工作区中，Cargo 命令通常会做“正确的事情”。如果您需要消除歧义，例如如果两个工作区 crate 都有相同名称的二进制文件，请使用 -p 标志（用于 package）。如果您在特定工作区 crate 的子目录中，可以传递 --workspace 来执行整个工作区的命令。
- 一旦您在工作区级别的 Cargo.toml 文件中设置了工作区成员的数组，您可以使用路径依赖关系来使您的 crate 互相依赖，如清单5-5所示。

```toml
# bar/two/Cargo.toml
[dependencies]
one = { path = "../one" }
# bar/one/Cargo.toml
[dependencies]
foo = { path = "../../foo" }
```

清单5-5：工作区中的crate之间的依赖关系

- 现在，如果您对bar/two中的crate进行更改，那么只有该crate会重新编译，因为foo和bar/one没有更改。甚至从头编译项目可能会更快，因为编译器不需要评估整个项目源代码以寻找优化机会。

**指定工作区内部的依赖关系**
在工作区中，最明显的指定一个crate依赖于另一个crate的方法是使用路径指定符，如清单5-5所示。然而，如果您的子crate是为公共使用而设计的，您可能希望使用版本指定符。

- 假设您有一个crate依赖于清单5-5中bar工作区中的一个crate的Git版本，使用one = { git = ". . ." }，以及一个已发布的foo版本（也来自bar），使用foo = "1.0.0"。Cargo将会忠实地获取one的Git仓库，该仓库包含整个bar工作区，并且会看到one依赖于foo，位于工作区内的../../foo。但是Cargo不知道已发布的版本foo = "1.0.0"和Git仓库中的foo是同一个crate！它将它们视为两个独立的依赖项，只是碰巧具有相同的名称。

- 你可能已经看到了这个问题的所在。如果你尝试使用来自 foo（1.0.0）的任何类型与一个接受来自 foo 的类型的 API，编译器将拒绝该代码。尽管这些类型具有相同的名称，但编译器无法知道它们是相同的底层类型。用户会感到非常困惑，因为编译器会显示类似“expected foo::Type，got foo::Type”的错误信息。
- 缓解这个问题的最佳方法是仅在子crate依赖于未发布的更改时使用路径依赖关系。只要一个crate使用foo 1.0.0，它应该在其依赖项中列出foo = "1.0.0"。只有当您对foo进行了更改，并且one需要这些更改时，才应将one更改为使用路径依赖关系。一旦您发布了one可以依赖的foo的新版本，您应该再次删除路径依赖关系。
- 这种方法也有其缺点。现在，如果您更改了 foo，然后运行 one 的测试，您会发现 one 将使用旧的 foo 进行测试，这可能不是您期望的结果。您可能希望配置您的持续集成基础设施，以便对每个子 crate 进行测试，既使用其他子 crate 的最新发布版本，又使用所有子 crate 配置为使用路径依赖项。

#### Project Configuration

运行 cargo new 会为您设置一个最小的 Cargo.toml，其中包含 crate 的名称、版本号、一些作者信息以及一个空的依赖项列表。这可以让您走得很远，但随着项目的发展，您可能希望在 Cargo.toml 中添加一些有用的内容。

##### Crate 元数据

将所有Cargo支持的元数据指令添加到Cargo.toml中是最明显的事情。除了显而易见的字段，如描述和主页，还可以包含其他信息，例如crate的README路径（readme）、与cargo run一起运行的默认二进制文件（default-run）以及额外的关键字和类别，以帮助crates.io对您的crate进行分类。
对于项目布局更复杂的crate，设置include和exclude元数据字段也非常有用。它们决定了应该包含和发布哪些文件。默认情况下，Cargo会包含crate目录中的所有文件，除非在.gitignore文件中列出，但如果您还有大型测试夹具、无关的脚本或其他辅助数据与您想要进行版本控制的相同目录中，这可能不是您想要的。正如它们的名称所示，include和exclude允许您仅包含特定的文件集或排除与给定模式匹配的文件。

**注意** 如果您有一个 crate 不应该被发布，或者只应该发布到某些特定的替代注册表（即不发布到 crates.io），您可以将 publish 指令设置为 false 或允许的注册表列表。
您可以使用的元数据指令列表不断增长，因此请定期查看 Cargo 参考文档的 Manifest Format 页面（<https://doc.rust-lang.org/cargo/reference/manifest.html>）。

##### Build Configuration

Cargo.toml还可以让您控制Cargo如何构建您的crate。
最明显的工具是build参数，它允许您为您的crate编写完全自定义的构建程序（我们将在第11章中重新讨论此问题）。
然而，Cargo还提供了两种较小但非常有用的机制，我们将在这里探讨：patches和profiles。

`**[patch]**`
Cargo.toml的[patch]部分允许您指定一个不同的源来替换依赖项，无论修补的依赖项在依赖关系中的位置如何。当您需要针对某个传递依赖项的修改版本进行编译，以测试错误修复、性能改进或即将发布的新次要版本时，这非常有价值。清单5-6展示了一个临时使用一组依赖项的变体的示例。

```toml

[patch.crates-io]

# use a local (presumably modified) source

regex = { path = "/home/jon/regex" }

# use a modification on a git branch

serde = { git = "<https://github.com/serde-rs/serde.git>", branch = "faster" }

# patch a git dependency

[patch.'https://github.com/jonhoo/project.git']
project = { path = "/home/jon/project" }
```

清单5-6：使用[patch]在Cargo.toml中覆盖依赖项源
即使您修补了一个依赖项，Cargo也会检查crate的版本，以防止意外地修补错误的主要版本crate。如果由于某种原因您在传递依赖项中依赖于多个主要版本的相同crate，您可以通过为每个版本提供不同的标识符来修补它们，如清单5-7所示。

```toml

[patch.crates-io]
nom4 = { path = "/home/jon/nom4", package = "nom" }
nom5 = { path = "/home/jon/nom5", package = "nom" }

```

清单5-7：使用[patch]在Cargo.toml中覆盖同一crate的多个版本

- Cargo将查看每个路径中的Cargo.toml文件，意识到/nom4包含主要版本4，/nom5包含主要版本5，并相应地修补这两个版本。package关键字告诉Cargo在这两种情况下都查找名为nom的crate，而不是像默认情况下那样使用依赖标识符（左侧部分）。您也可以在常规依赖项中使用package来重命名依赖项！
- 请记住，在发布crate时，补丁不会被考虑在上传的包中。依赖于您的crate的crate将仅使用自己的[patch]部分（可能为空），而不是您的crate的[patch]部分！
**CRATES VS. PACKAGES**
您可能想知道包和crate之间的区别是什么。
这两个术语在非正式的上下文中经常可以互换使用，但它们在Rust编译器、Cargo、crates.io或其他情况下具有不同的定义。我个人认为crate是一个Rust模块层次结构，从一个根.rs文件开始（您可以在其中使用crate级别的属性，如#![feature]）-通常是类似lib.rs或main.rs的文件。相比之下，一个package是一组crate和元数据，因此基本上是由Cargo.toml文件描述的所有内容。这可能包括一个库crate、多个二进制crate、一些集成测试crate，甚至可能是多个工作区成员，它们本身就有Cargo.toml文件。

`**[profile]**`
[profile]部分允许您向Rust编译器传递附加选项，以改变它编译crate的方式。这些选项主要分为三类：性能选项、调试选项和以用户定义方式改变代码行为的选项。它们在调试模式和发布模式下有不同的默认值（还有其他模式）。

- 三个主要的性能选项是opt-level、codegen-units和lto。opt-level选项通过告诉编译器在运行时对程序进行多大程度的优化（0表示“没有优化”，3表示“尽可能多的优化”）来调整运行时性能。设置越高，代码优化的程度就越高，这可能使代码运行更快。然而，额外的优化会增加编译时间的成本，这就是为什么通常只在发布构建中启用优化的原因。
**注意** 您还可以将 opt-level 设置为 "s"，以优化二进制文件的大小，这在嵌入式平台上可能很重要。

- codegen-units选项与编译时性能有关。它告诉编译器可以将单个crate的编译拆分成多少个独立的编译任务（代码生成单元）。将大型crate的编译拆分成更多的片段，编译速度将更快，因为可以使用更多的线程并行编译crate。不幸的是，为了实现这种加速，线程需要更多或更少独立地工作，这意味着代码优化会受到影响。例如，想象一下，在一个线程中编译的crate段可以从另一个段中内联一些代码-由于这两个段是独立的，内联无法发生！因此，这个设置是编译时性能和运行时性能之间的权衡。默认情况下，Rust在调试模式下使用无限数量的codegen units（基本上是“尽快编译”），在发布模式下使用较小的数量（在撰写本文时为16个）。

- lto设置用于切换链接时优化（LTO），它使得编译器（或链接器，如果你想更加技术化）能够联合优化程序的各个编译单元，这些单元最初是分别编译的。关于LTO的详细细节超出了本书的范围，但基本思想是每个编译单元的输出都包含了该单元中的代码的信息。在所有单元编译完成后，链接器对所有单元进行另一次遍历，并使用额外的信息来优化组合编译代码。这个额外的遍历增加了编译时间，但可以恢复由于将编译拆分为较小部分而可能丢失的大部分运行时性能。特别是，LTO可以为可能受益于跨crate优化的性能敏感程序提供显著的性能提升。但要注意，跨crate的LTO可能会大大增加编译时间。
- Rust默认情况下在每个crate内部对所有codegen单元执行LTO，以弥补使用多个codegen单元造成的优化损失。由于LTO仅在每个crate内部执行，而不是跨crate执行，因此这个额外的步骤并不太繁重，而且增加的编译时间应该比使用大量codegen单元节省的时间更少。Rust还提供了一种称为thin LTO的技术，它允许LTO过程在很大程度上并行化，但代价是可能会错过一些“完整”LTO过程发现的优化。

**注意** LTO 在许多情况下也可以用于优化跨外部函数接口边界。有关更多详细信息，请参阅 linker-plugin-lto rustc 标志。

- [profile] 部分还支持一些用于调试的标志，例如 debug、debug-assertions 和 overflow-checks。debug 标志告诉编译器在编译的二进制文件中包含调试符号。这会增加二进制文件的大小，但意味着在回溯和性能分析中可以获得函数名等信息，而不仅仅是指令地址。debug-assertions 标志启用 debug_assert! 宏和其他相关的调试代码，这些代码在其他情况下不会被编译（通过 cfg(debug_assertions)）。这些代码可能会使程序运行变慢，但可以更容易地在运行时捕获可疑行为。overflow-checks 标志，顾名思义，启用整数操作的溢出检查。这会减慢它们的速度（注意到一个趋势了吗？），但可以帮助您尽早发现棘手的错误。默认情况下，在调试模式下启用这些标志，在发布模式下禁用。

`[profile.*.panic]`

- [profile] 部分还有一个值得单独讨论的标志：panic。该选项决定当程序中的代码调用 panic! 时（无论是直接调用还是通过 unwrap 等间接调用），会发生什么。您可以将 panic 设置为 unwind（在大多数平台上默认值）或 abort。我们将在第9章中更详细地讨论 panic 和 unwinding，但在这里我会简要总结一下。
- 在 Rust 中，通常情况下，当程序发生 panic 时，引发 panic 的线程会开始进行堆栈展开（unwinding）。您可以将堆栈展开视为从当前函数递归返回到该线程堆栈的底部。也就是说，如果 main 调用了 foo，foo 调用了 bar，bar 调用了 baz，在 baz 中发生 panic 将会强制从 baz、bar、foo，最后从 main 中返回，导致程序退出。进行堆栈展开的线程将正常地丢弃堆栈上的所有值，这使得这些值有机会清理资源、报告错误等。即使发生 panic，这使得运行系统有机会以优雅的方式退出。
- 当一个线程发生 panic 并进行堆栈展开时，其他线程继续运行，不受影响。只有当（如果）运行 main 函数的线程退出时，程序才会终止。也就是说，panic 通常只限于发生 panic 的线程。
- 这意味着堆栈展开是一把双刃剑；程序在一些失败的组件中艰难前行，这可能导致各种奇怪的行为。例如，想象一下，在更新互斥锁中的状态时，一个线程发生了 panic。随后尝试获取该互斥锁的任何线程现在必须准备好处理状态可能处于部分更新、不一致的状态的事实。因此，一些同步原语（如互斥锁）将记住它们上次访问时是否发生了 panic，并将此信息传递给随后尝试访问该原语的任何线程。如果一个线程遇到这样的状态，通常也会发生 panic，从而导致级联，最终终止整个程序。但这可能比继续以损坏的状态运行要好！
- 支持堆栈展开所需的记账工作是不免费的，并且通常需要编译器和目标平台的特殊支持。例如，许多嵌入式平台根本无法高效地展开堆栈。因此，Rust支持一种不同的panic模式：abort，当发生panic时，整个程序立即退出。在这种模式下，没有线程可以进行任何清理工作。这可能看起来很严重，确实如此，但它确保程序永远不会在半工作状态下运行，并且错误会立即显现出来。

**警告** panic 设置是全局的 - 如果将其设置为 abort，则所有依赖项也将使用 abort 进行编译。

- 您可能已经注意到，当一个线程发生 panic 时，它倾向于打印一个回溯：导致 panic 发生的函数调用的轨迹。这也是一种展开，尽管它与此处讨论的 panic 展开行为是分开的。即使在 panic=abort 的情况下，您也可以通过向 rustc 传递 -Cforce-unwind-tables 来获得回溯，这使得 rustc 包含了在 panic 时回溯堆栈的必要信息，同时终止程序的执行。
**配置文件覆盖**
您可以使用配置文件覆盖来为特定的依赖项或特定的配置文件设置配置选项。例如，清单5-8展示了如何在调试模式下为serde crate启用激进的优化，并为所有其他crate启用适度的优化，使用[profile.<profile-name>.package.<crate-name>]语法。

```toml

[profile.dev.package.serde]
opt-level = 3
[profile.dev.package."*"]
opt-level = 2
```

清单5-8：为特定依赖项或特定模式覆盖配置文件选项

- 如果某个依赖项在调试模式下非常慢（例如解压缩或视频编码），而您需要对其进行优化，以便测试套件不需要花费数天的时间才能完成，那么这种优化覆盖可能非常有用。您还可以在Cargo配置文件的~/.cargo/config中使用[profile.dev]（或类似的）部分指定全局配置文件默认值。
- 当您为特定依赖项设置优化参数时，请记住这些参数仅适用于作为该crate的一部分编译的代码；如果在此示例中的serde中有您在crate中使用的通用方法或类型，那么该方法或类型的代码将在您的crate中进行单态化和优化，并且将应用您crate的配置文件设置，而不是serde的配置文件覆盖中的设置。

#### 条件编译

大多数您编写的 Rust 代码都是通用的，无论在哪个 CPU 或操作系统上运行，它都能正常工作。但有时，您可能需要在 Windows 上、在 ARM 芯片上或在针对特定平台应用程序二进制接口（ABI）编译时执行一些特殊操作。或者，当某个 CPU 指令可用时，您可能希望编写一个优化版本的特定函数，或者在连续集成（CI）环境中运行时禁用一些慢但无趣的设置代码。为了满足这些情况，Rust 提供了条件编译的机制，即只有在编译环境满足某些条件时，才会编译特定的代码段。

- 我们使用cfg关键字来表示条件编译，你在本章的“在你的crate中使用特性”中已经见过它。它通常以#[cfg(condition)]属性的形式出现，表示只有在条件为真时才编译下一个项。Rust还有#[cfg_attr(condition, attribute)]，如果条件成立，则编译为#[attribute]，否则不做任何操作。您还可以使用cfg!(condition)宏将cfg条件评估为布尔表达式。
- 每个cfg结构都接受一个由选项组成的条件，例如feature = "some-feature"，以及组合器all、any和not，它们的功能符合您的预期。选项可以是简单的名称，例如unix，也可以是用于特性条件的键/值对。
- 有许多有趣的选项可以使编译依赖于它们。让我们从最常见的到最不常见的逐个介绍：

##### 功能选项

- 您已经看到了这些的示例。功能选项采用feature = "name-of-feature"的形式，如果启用了指定的功能，则被视为true。您可以使用组合器在单个条件中检查多个功能。例如，any(feature = "f1", feature = "f2")在启用了功能f1或功能f2时为true。

##### 操作系统选项

- 这些使用键/值语法，键为target_os，值为windows、macos和linux。您还可以使用target_family指定操作系统的系列，它接受值windows或unix。它们非常常见，已经有了自己的命名简写形式，因此您可以直接使用cfg(windows)和cfg(unix)。例如，如果您希望特定的代码段仅在macOS和Windows上编译，您可以编写：#[cfg(any(windows, target_os = "macos"))]。

##### 上下文选项

- 这些选项允许您根据特定的编译上下文来定制代码。其中最常见的是test选项，只有在crate在测试配置文件下编译时才为true。请记住，test仅针对正在测试的crate设置，而不针对其任何依赖项设置。这也意味着在运行集成测试时，您的crate中不会设置test；实际上，是集成测试在测试配置文件下编译，而您的crate会正常编译（即没有设置test）。相同的规则也适用于doc和doctest选项，它们仅在构建文档或编译doctest时设置。还有一个debug_assertions选项，默认情况下在调试模式下设置。

工具选项

- 一些工具，如clippy和Miri，设置自定义选项（稍后详细介绍），可以在运行这些工具时自定义编译。通常，这些选项以工具的名称命名。例如，如果您不希望某个特定的计算密集型测试在Miri下运行，您可以给它添加属性#[cfg_attr(miri, ignore)]。

##### Architecture options

- 这些选项允许您根据编译器所针对的CPU指令集进行编译。您可以使用target_arch指定特定的架构，它接受值如x86、mips和aarch64，或者您可以使用target_feature指定特定的平台特性，它接受值如avx或sse2。对于非常底层的代码，您可能还会发现target_endian和target_pointer_width选项很有用。

##### Compiler options

- 这些选项允许您根据编译的平台ABI来调整代码，可以通过target_env来获取，它接受值如gnu、msvc和musl。出于历史原因，这个值通常为空，特别是在GNU平台上。通常只有在需要直接与环境ABI进行接口的情况下，才需要此选项，例如在使用#[link]链接到特定ABI的符号名称时。虽然cfg条件通常用于自定义代码，但有些条件也可以用于自定义依赖项。例如，依赖项winrt通常只在Windows上有意义，而nix crate可能只在基于Unix的平台上有用。清单5-9展示了如何使用cfg条件来实现这一点：

```toml

[target.'cfg(windows)'.dependencies]
winrt = "0.7"
[target.'cfg(unix)'.dependencies]
nix = "0.17"
```

清单5-9：条件依赖项

- 在这里，我们指定 winrt 版本 0.7 应该只在 cfg(windows)（即在 Windows 上）下作为依赖项，而 nix 版本 0.17 则只在 cfg(unix)（即在 Linux、macOS 和其他基于 Unix 的平台上）下作为依赖项。需要记住的一件事是，在构建过程的早期阶段，只有某些 cfg 选项可用，因此无法使用此语法基于特性和上下文来引入依赖项。但是，您可以使用仅依赖于目标规范或架构的任何 cfg，以及由调用 rustc 的工具明确设置的任何选项（如 cfg(miri)）。
**注意** 在我们讨论依赖规范的同时，我强烈建议您设置CI基础设施，使用cargo-deny和cargo-audit等工具对您的依赖项进行基本审核。这些工具可以检测到以下情况：您在传递依赖关系中依赖于多个主要版本的给定依赖项，您依赖于未维护或已知存在安全漏洞的crate，或者您使用了您可能希望避免的许可证。使用这样的工具是提高代码库质量的一种自动化方式！

- 添加自定义的条件编译选项也非常简单。您只需要确保在 rustc 编译您的 crate 时传递 --cfg=myoption。最简单的方法是将 --cfg 添加到 RUSTFLAGS 环境变量中。这在 CI 中非常有用，您可以根据测试套件是在 CI 上运行还是在开发机器上运行来自定义测试套件：在 CI 设置中添加 --cfg=ci 到 RUSTFLAGS，然后在代码中使用 cfg(ci) 和 cfg(not(ci))。以这种方式设置的选项也可以在 Cargo.toml 的依赖项中使用。

#### 版本控制

所有的 Rust crate 都有版本号，并且预期遵循 Cargo 对语义化版本控制的实现。语义化版本控制规定了哪些变化需要增加哪些版本号，以及哪些版本被认为是兼容的，以及以何种方式兼容。RFC 1105 标准本身值得一读（它并不是非常技术性），但总结起来，它区分了三种类型的变化：破坏性变化需要进行主版本号的更改；新增功能需要进行次版本号的更改；修复错误只需要进行补丁版本号的更改。RFC 1105 在概述了在 Rust 中什么是破坏性变化方面做得相当不错，我们在本书的其他地方也提到了一些相关内容。

- 我不会在这里详细介绍不同类型变化的确切语义。相反，我想强调一些在决定如何为自己的 crate 进行版本控制时需要注意的 Rust 生态系统中不太直观的版本号使用方式。

#### 最低支持的 Rust 版本

第一个 Rust 特性是最低支持的 Rust 版本（MSRV）。在 Rust 社区中，关于项目在 MSRV 和版本控制方面应该遵循什么策略存在很多争议，而且没有一个真正好的答案。问题的核心是，一些 Rust 用户受限于使用较旧的 Rust 版本，通常是在企业环境中，他们几乎没有选择。如果我们不断利用新稳定的 API，这些用户将无法编译我们 crate 的最新版本，将被落下。

- 有两种技术可以让创建者为处于这种情况下的用户提供一些便利。第一种是建立一个最低支持的 Rust 版本（MSRV）策略，承诺新版本的 crate 将始终与过去 X 个月内的任何稳定版本编译通过。确切的数字有所不同，但通常为 6 或 12 个月。根据 Rust 的六周发布周期，这对应于最新的四个或八个稳定版本。项目中引入的任何新代码都必须与 MSRV 编译器（通常由 CI 检查）一起编译通过，或者在 MSRV 策略允许的情况下暂时保留，直到可以合并。这有时可能会有些麻烦，因为这意味着这些 crate 无法充分利用语言的最新功能，但它将为您的用户带来便利。
- 第二种技术是确保每当 MSRV 更改时，增加您的 crate 的次要版本号。因此，如果您发布了版本为 2.7.0 的 crate，并将其 MSRV 从 Rust 1.44 增加到 Rust 1.45，那么依赖于您的 crate 并停留在 1.44 上的项目可以使用依赖版本说明符 version = "2, <2.7" 来保持项目正常运行，直到可以迁移到 Rust 1.45。重要的是，您要增加次要版本号，而不仅仅是修订版本号，这样您仍然可以通过进行另一个修订版本的关键安全修复来为先前的 MSRV 发布提供支持，如果有必要的话。
- 一些项目非常重视对最低支持的 Rust 版本（MSRV）的支持，将 MSRV 的更改视为破坏性更改并增加主版本号。这意味着下游项目必须明确选择接受 MSRV 的更改，而不是选择不接受，但这也意味着没有严格的 MSRV 要求的用户将无法看到未来的错误修复，除非更新其依赖项，这可能需要他们进行破坏性更改。正如我所说，这些解决方案都有缺点。
- 在当前的 Rust 生态系统中，强制最低支持的 Rust 版本（MSRV）是具有挑战性的。只有少数一部分的 crate 提供了任何 MSRV 的保证，即使你的依赖项提供了，你也需要不断监控它们以了解何时增加了 MSRV。当它们增加了 MSRV 时，你需要使用之前提到的限制版本范围进行新的 crate 发布，以确保你的 MSRV 不会改变。这可能会迫使你放弃依赖项的安全性和性能更新，因为你必须继续使用旧版本，直到你的 MSRV 策略允许更新。而这个决定也会影响到你的依赖者。有一些提案将 MSRV 检查集成到 Cargo 中，但截至目前为止，还没有可行的稳定版本。

#### 最小依赖版本

当您首次添加依赖项时，往往不清楚应该给出什么版本说明符。程序员通常选择最新版本或当前主要版本，但很可能这两种选择都是错误的。我所说的“错误”并不意味着您的 crate 无法编译，而是选择这样做可能会给您 crate 的用户带来麻烦。让我们看看为什么每种情况都存在问题。

- 首先，考虑一种情况，您添加了对 hugs = "1.7.3" 的依赖，这是最新发布的版本。现在想象一下，某个开发人员依赖于您的 crate，但他们还依赖于另一个 crate，名为 foo，它本身依赖于 hugs。进一步想象一下，foo 的作者非常注意他们的 MSRV 策略，所以他们依赖于 hugs = "1, <1.6"。在这种情况下，您将遇到麻烦。当 Cargo 看到 hugs = "1.7.3" 时，它只考虑 >=1.7 的版本。但是然后它看到 foo 对 hugs 的依赖要求 <1.6，所以它放弃了，并报告说没有满足所有要求的 hugs 版本。
**注意** 在实践中，有许多原因可能导致一个 crate 明确不希望使用较新的依赖项版本。最常见的原因是为了保持最低支持的 Rust 版本（MSRV），满足企业审计要求（较新的版本可能包含未经审计的代码），以及确保可重现构建，只使用精确列出的版本。
- 这是不幸的，因为很可能您的 crate 可以与 hugs 1.5.6 编译通过。甚至可能它可以与任何 1.X 版本编译通过！但是通过使用最新的版本号，您告诉 Cargo 仅考虑在该次要版本及以上的版本。那么解决方案是使用 hugs = "1" 吗？不，也不完全正确。可能您的代码确实依赖于仅在 hugs 1.6 中添加的某些内容，因此 1.6.2 是可以的，但 1.5.6 不行。如果您只在使用较新版本的情况下编译 crate，您可能不会注意到这一点，但是如果依赖图中的某个 crate 指定了 hugs = "1, <1.5"，则您的 crate 将无法编译！
- 正确的策略是列出具有您的 crate 所依赖的所有内容的最早版本，并确保即使在向 crate 添加新代码时也保持这种情况。但是，除了浏览更改日志或通过试错之外，如何确定这一点呢？您最好的选择是使用 Cargo 的不稳定 -Zminimal-versions 标志，它使您的 crate 使用所有依赖项的最低可接受版本，而不是最高版本。然后，将所有依赖项设置为最新的主版本号，尝试编译，并为任何不兼容的依赖项添加一个次要版本号。反复尝试，直到一切都能正常编译，这样您就得到了最低版本要求！
- 值得注意的是，与最低支持的 Rust 版本（MSRV）一样，最小版本检查也面临着生态系统采用的问题。尽管您可能已经正确设置了所有版本规范，但您所依赖的项目可能没有。这使得在实践中很难使用 Cargo 的最小版本标志（这也是为什么它仍然是不稳定的）。如果您依赖于 foo，并且 foo 依赖于 bar，而 bar 的规范为 bar = "1"，而实际上它需要 bar = "1.4"，无论您如何列出 foo，Cargo 都会报告无法编译 foo，因为 -Z 标志告诉它始终优先选择最小版本。您可以通过在依赖项中直接列出 bar 并指定适当的版本要求来解决此问题，但这些解决方法可能很麻烦并且难以维护。您可能会列出大量仅通过传递依赖项引入的依赖项，并且随着时间的推移，您将不得不保持该列表的更新。
**注意** 目前的一个提案是提供一个标志，该标志在当前 crate 中偏向最小版本，但在依赖项中偏向最大版本，这似乎非常有前景。

##### 更新日志

- 对于除了最简单的 crate 之外，我强烈建议保持一个更新日志。没有什么比看到一个依赖项进行了主版本升级，然后不得不深入 Git 日志中找出变更内容以及如何更新代码更令人沮丧的了。我建议您不要只是将 Git 日志转储到一个名为 changelog 的文件中，而是保持一个手动的更新日志。这样更有可能有用。
- 一个简单但不错的更新日志格式是 Keep a Changelog 的格式，文档在 <https://keepachangelog.com/> 中有记录。

##### Unreleased Versions

即使依赖项的源是目录或Git存储库，Rust在考虑版本号时也会起作用。这意味着即使您尚未发布到crates.io，语义化版本控制也很重要；在发布之间，Cargo.toml中列出的版本号很重要。语义化版本控制标准没有规定如何处理这种情况，但我将提供一个工作流程，既能很好地工作，又不会太繁琐。

- 在发布了一个版本之后，立即在 Cargo.toml 中更新版本号为下一个带有类似 -alpha.1 后缀的补丁版本。如果刚发布了 2.0.3，那么新版本将是 2.0.4-alpha.1。如果刚发布了一个 alpha 版本，则增加 alpha 编号即可。
- 在发布之间对代码进行更改时，请注意增量或破坏性的更改。如果发生了破坏性更改，并且与上次发布时的版本号没有变化，那么请增加版本号。例如，如果上次发布的版本是2.0.3，当前版本是2.0.4-alpha.2，并且您进行了增量更改，请将具有更改的版本号设置为2.1.0-alpha.1。如果进行了破坏性更改，则将其设置为3.0.0-alpha.1。如果相应的版本增加已经完成，请只增加alpha编号。
- 当您发布一个版本时，删除后缀（除非您想进行预发布），然后发布，并从头开始。
- 这个过程是有效的，因为它可以更好地支持两种常见的工作流程。首先，假设开发人员依赖于您 crate 的主版本2，但他们需要一个目前只在Git中可用的功能。然后，您提交了一个破坏性的更改。如果您不同时增加主版本号，他们的代码将以意想不到的方式突然失败，可能是无法编译，或者是由于奇怪的运行时问题。如果您按照这里提出的步骤进行操作，他们将收到Cargo的通知，说明发生了破坏性的更改，他们将不得不解决这个问题或者固定一个特定的提交。
- 接下来，想象一下，一个开发人员需要一个他们刚刚贡献给您 crate 的功能，但这个功能尚未包含在您 crate 的任何发布版本中。他们在很长一段时间内都在使用您 crate 的 Git 依赖项，因此他们项目中的其他开发人员已经有了您 crate 仓库的旧版本。如果您在 Git 中不增加主版本号，这个开发人员就没有办法告知他们的项目现在依赖于刚刚合并的功能。如果他们推送了他们的更改，他们的同事开发人员将发现项目无法编译，因为 Cargo 将重用旧的检出。然而，如果开发人员可以增加 Git 依赖项的次要版本号，那么 Cargo 将意识到旧的检出已经过时。
- 这个工作流程并不完美。它没有提供一个很好的方式来传达在发布之间的多个次要或主要更改，并且您仍然需要做一些工作来跟踪版本。然而，它确实解决了 Rust 开发人员在使用 Git 依赖项时遇到的两个最常见问题，即使在发布之间进行多个此类更改，这个工作流程仍然可以捕捉到许多问题。
- 如果您对版本号的小幅或连续变化不太担心，您可以通过始终增加版本号的适当部分来改进此建议的工作流程。但是，请注意，根据您进行此类更改的频率，这可能会使您的版本号变得非常大！

#### 摘要

在本章中，我们已经介绍了一些配置、组织和发布 crate 的机制，既有利于自己，也有利于他人。我们还讨论了在使用 Cargo 中处理依赖项和特性时的一些常见问题，希望这些问题不会再困扰到你。在下一章中，我们将转向测试，并深入探讨如何超越我们所熟悉和喜爱的 Rust 的简单 #[test] 函数。

### 6.测试

在本章中，我们将介绍各种扩展Rust测试能力的方法，以及您可能希望添加到测试组合中的其他类型的测试。Rust提供了许多内置的测试工具，这些工具在《Rust编程语言》中得到了很好的介绍，主要由#[test]属性和tests/目录表示。当您开始一个项目时，这些工具可以很好地满足您的需求，并且适用于各种应用和规模。然而，随着代码库的发展和测试需求的增加，您可能需要超越仅仅在个别函数上添加#[test]标记的方式。

- 本章分为两个主要部分。第一部分介绍了Rust的测试机制，如标准测试框架和条件测试代码。第二部分介绍了评估Rust代码正确性的其他方法，如基准测试、代码检查和模糊测试。

#### Rust Testing Mechanisms

要理解Rust提供的各种测试机制，首先必须了解Rust如何构建和运行测试。当您运行cargo test --lib时，Cargo所做的唯一特殊之处是向rustc传递--test标志。该标志告诉rustc生成一个测试二进制文件，该文件运行所有单元测试，而不仅仅是编译crate的库或二进制文件。在幕后，--test有两个主要效果。首先，它启用了cfg(test)，以便您可以有条件地包含测试代码（稍后会详细介绍）。其次，它使编译器生成一个测试harness：一个精心生成的main函数，在运行时调用程序中的每个`#[test]函数`。

##### 测试工具

编译器通过一种混合使用过程宏（procedural macros）和一些神奇的方式生成测试harness的main函数。我们将在第7章中更详细地讨论过程宏。测试harness将每个使用#[test]注解的函数转换为一个测试描述符，这是过程宏的一部分。然后，它将每个描述符的路径暴露给生成的main函数，这是神奇的部分。描述符包括测试的名称、任何额外设置的选项（如#[should_panic]），等等。在核心部分，测试harness迭代遍历crate中的测试，运行它们，捕获结果，并打印结果。因此，它还包括解析命令行参数的逻辑（例如--test-threads=1），捕获测试输出，以并行方式运行列出的测试，并收集测试结果。

- 截至目前，Rust开发人员正在努力使测试harness生成的魔法部分成为公开可用的API，以便开发人员可以构建自己的测试harness。这项工作仍处于实验阶段，但该提案与现有模型相当接近。需要解决的一部分魔法是如何确保生成的main函数可以访问#[test]函数，即使它们位于私有子模块中。
- 集成测试（位于tests/目录中的测试）遵循与单元测试相同的流程，唯一的区别是它们被编译为独立的 crate，意味着它们只能访问主 crate 的公共接口，并且在没有 #[cfg(test)] 的情况下对主 crate 进行编译。对于 tests/ 目录中的每个文件，都会生成一个测试 harness。为了允许您在测试中共享子模块，不会为 tests/ 子目录中的文件生成测试 harness。
**注意** 如果您明确希望在子目录中的文件中使用测试harness，可以通过将文件命名为main.rs来选择启用它。
- Rust不要求您使用默认的测试harness。您可以选择退出并实现自己的main方法来代表测试运行器，通过在Cargo.toml中设置harness = false来为给定的集成测试，如示例6-1所示。您定义的main方法将被调用来运行测试。

```rust

[[test]]
name = "custom"
path = "tests/custom.rs"
harness = false

```

代码清单 6-1：选择退出标准测试harness

- 没有测试harness，#[test]周围的所有魔法都不会发生。
相反，您需要编写自己的main函数来运行要执行的测试代码。实质上，您正在编写一个普通的Rust二进制文件，只是碰巧由cargo test运行。该二进制文件负责处理默认harness通常执行的所有事情（如果您想支持它们），例如命令行标志。harness属性是针对每个集成测试单独设置的，因此您可以有一个使用标准harness的测试文件和一个不使用harness的测试文件。

**默认测试harness的参数**
默认的测试harness支持一些命令行参数，用于配置测试的运行方式。这些参数不直接传递给cargo test，而是传递给Cargo为您编译和运行的测试二进制文件。要访问这组参数，您可以在cargo test后面加上--，然后是测试二进制文件的参数。例如，要查看测试二进制文件的帮助文本，您可以运行cargo test -- --help。

- 通过这些命令行参数，可以使用一些方便的配置选项。--nocapture标志禁用了通常在运行Rust测试时发生的输出捕获。如果您想实时观察测试的输出而不是在测试失败后一次性查看，这非常有用。您可以使用--test-threads选项来限制并发运行的测试数量，这对于有 hang 或 segfault 的测试很有帮助，您可以通过按顺序运行测试来找出是哪个测试出了问题。还有一个--skip选项，用于跳过与特定模式匹配的测试，--ignored用于运行通常被忽略的测试（例如那些需要外部程序运行的测试），--list用于列出所有可用的测试。

- 请记住，这些参数都是由默认的测试harness实现的，因此如果您禁用它（使用harness = false），您将需要在您的main函数中自己实现所需的参数！
- 没有测试harness的集成测试主要用于基准测试，我们稍后会看到，但它们也在您想要运行不适合标准“一个函数，一个测试”模型的测试时非常有用。例如，您经常会看到无harness的测试与模糊测试器、模型检查器以及需要自定义全局设置的测试一起使用（例如在WebAssembly下或在使用自定义目标时）。

#### `#[cfg(test)]`

当 Rust 构建测试代码时，它会设置编译器配置标志 test，您可以使用条件编译来编写代码，除非特别进行测试，否则该代码将被编译掉。表面上看，这似乎有些奇怪：难道您不想测试与生产环境中完全相同的代码吗？确实如此，但是在测试时仅有的代码可以让您编写更好、更全面的测试，有几种方式。

**MOCKING**
在编写测试时，您通常希望对正在测试的代码以及代码可能交互的任何其他类型具有严格的控制。例如，如果您正在测试一个网络客户端，您可能不希望在真实网络上运行单元测试，而是希望直接控制“网络”发出的字节和时间。或者，如果您正在测试一个数据结构，您希望您的测试使用的类型允许您控制每次调用时每个方法返回的内容。您还可能希望收集指标，例如给定方法被调用的频率或是否发出了给定的字节序列。

- 这些“假”类型和实现被称为模拟对象（mocks），它们是任何大型单元测试套件的关键特性。虽然您通常可以手动完成获取此类控制所需的工作，但最好有一个库来为您处理大部分琐碎的细节。这就是自动模拟的作用。模拟库将提供生成具有特定属性或签名的类型（包括函数）的功能，以及在测试执行期间控制和检查这些生成的项的机制。
- 在Rust中，模拟通常通过泛型来实现 - 只要您的程序、数据结构、框架或工具对您可能想要模拟的任何内容都是泛型的（或者接受一个特质对象），您就可以使用模拟库生成符合要求的类型来实例化这些泛型参数。然后，通过使用生成的模拟类型实例化您的泛型构造，编写单元测试，然后开始测试！
- 在泛型不方便或不合适的情况下，例如如果您想避免将类型的特定方面泛型化给用户，您可以将您想要模拟的状态和行为封装在一个专用的结构体中。然后，您可以生成该结构体及其方法的模拟版本，并使用条件编译根据cfg(test)或类似cfg(feature = "test_mock_foo")的测试专用功能来使用真实或模拟实现。
- 目前，在Rust社区中还没有出现一个单一的模拟库，甚至也没有出现一个单一的模拟方法，被认为是唯一正确的答案。我所知道的最全面和详尽的模拟库是mockall crate，但它仍在积极开发中，还有许多其他竞争者。

##### Test-Only APIs

首先，拥有仅供测试使用的代码可以让您向（单元）测试公开额外的方法、字段和类型，以便测试不仅可以检查公共API的行为是否正确，还可以检查内部状态是否正确。例如，考虑 hashbrown 中实现标准库 HashMap 的 HashMap 类型。HashMap 类型实际上只是围绕 RawTable 类型的包装器，而 RawTable 类型实现了大部分哈希表逻辑。假设在对空映射进行 HashMap::insert 操作后，您想要检查映射中的一个桶是否非空，如代码清单 6-2 所示。

```rust
#[test]
fn insert_just_one() {
  let mut m = HashMap::new();
  m.insert(42, ());
  let full = m.table.buckets.iter().filter(Bucket::is_full).count();
  assert_eq!(full, 1);
}
```

代码清单 6-2：访问不可访问的内部状态的测试，因此无法编译

- 这段代码无法编译，因为尽管测试代码可以访问HashMap的私有table字段，但它无法访问RawTable的私有buckets字段，因为RawTable位于不同的模块中。我们可以通过将buckets字段的可见性设置为pub(crate)来修复这个问题，但我们实际上不希望HashMap能够直接访问buckets，因为它可能会意外地破坏RawTable的内部状态。即使将buckets作为只读字段可用也可能会有问题，因为HashMap中的新代码可能会开始依赖于RawTable的内部状态，从而使未来的修改变得更加困难。
- 解决方案是使用#[cfg(test)]。我们可以向RawTable添加一个方法，只允许在测试时访问buckets，如代码清单6-3所示，从而避免为其余代码添加不必要的风险。然后，可以更新代码清单6-2中的代码，调用buckets()而不是访问私有的buckets字段。

```rust
impl RawTable {
    #[cfg(test)]
    pub(crate) fn buckets(&self) -> &[Bucket] {
      &self.buckets
    }
}
```

代码清单 6-3：使用 #[cfg(test)] 使内部状态在测试环境中可访问

##### 测试断言的记账

拥有仅在测试期间存在的代码的第二个好处是，您可以增加程序以执行额外的运行时记账，然后可以由测试进行检查。例如，想象一下，您正在编写自己版本的标准库中的BufWriter类型。在测试时，您希望确保BufWriter不会不必要地发出系统调用。最明显的方法是让BufWriter跟踪它在底层Write上调用write的次数。然而，在生产环境中，这些信息并不重要，并且跟踪它会引入（微小的）性能和内存开销。使用 # [cfg(test)]，您可以只在测试时进行记账，如代码清单6-4所示。

```rust

struct BufWriter<T> {
  # [cfg(test)]
  write_through: usize,
// other fields...
}

impl<T: Write> Write for BufWriter<T> {
  fn write(&mut self, buf: &[u8]) -> Result<usize> {
  // ...
  if self.full() {

    # [cfg(test)]
    self.write_through += 1;
    let n = self.inner.write(&self.buffer[..])?;
  // ...
  }
}
```

代码清单 6-4：使用 #[cfg(test)] 限制记账到测试上下文中
请记住，test 只对正在编译为测试的 crate 设置。对于单元测试，这是您要测试的 crate，正如您所期望的那样。然而，对于集成测试，它是作为测试编译的集成测试二进制文件 - 您要测试的 crate 只是作为库进行编译，因此不会设置 test。

#### Doctests

Rust代码片段在文档注释中会自动作为测试用例运行。这些通常被称为doctests。因为doctests出现在您的crate的公共文档中，并且用户很可能模仿其中的内容，所以它们被作为集成测试运行。这意味着doctests无法访问私有字段和方法，并且测试不会设置在主crate的代码上。每个doctest都被编译为独立的crate，并在隔离环境中运行，就像用户将doctest复制粘贴到自己的程序中一样。

- 在幕后，编译器对doctest进行了一些预处理，使其更简洁。最重要的是，它自动在您的代码周围添加了一个fn main。这使得doctest只关注用户可能关心的重要部分，比如实际使用您库中的类型和方法的部分，而不包含不必要的样板代码。
- 您可以通过在doctest中定义自己的fn main来选择退出此自动包装。例如，如果您想使用#[tokio::main] async fn main编写异步主函数，或者如果您想向doctest添加其他模块，您可能希望这样做。
- 在您的doctest中使用?运算符时，通常不需要使用自定义的main函数，因为rustdoc会根据一些启发式规则将返回类型设置为Result<(), impl Debug>，如果您的代码看起来使用了?（例如，如果以Ok(())结尾）。如果类型推断对函数的错误类型感到困惑，您可以通过将doctest的最后一行更改为显式类型来消除歧义，例如：Ok::<(), T>(())。
- Doctests具有一些额外的功能，对于编写更复杂接口的文档非常有用。首先是隐藏单行的能力。如果您在doctest的一行前面加上#，那么该行在编译和运行doctest时会被包含，但不会在生成的文档中的代码片段中显示。这使您可以轻松隐藏对当前示例不重要的细节，例如为虚拟类型实现特性或生成值。如果您希望呈现一系列示例而不每次都显示相同的前导代码，这也很有用。清单6-5展示了一个带有隐藏行的doctest示例。

```rust

/// Completely frobnifies a number through I/O.
///
/// In this first example we hide the value generation.
/// ```
/// # let unfrobnified_number = 0;
/// # let already_frobnified = 1;
/// assert!(frobnify(unfrobnified_number).is_ok());
/// assert!(frobnify(already_frobnified).is_err());
/// ```
///
/// Here's an example that uses ? on multiple types
/// and thus needs to declare the concrete error type,
/// but we don't want to distract the user with that.
/// We also hide the use that brings the function into scope.
/// ```
/// # use mylib::frobnify;
/// frobnify("0".parse()?)?;
/// # Ok::<(), anyhow::Error>(())
/// ```
///
/// You could even replace an entire block of code completely,
/// though use this _very_ sparingly:
/// ```
/// # /*
/// let i = ...;
/// # */
/// # let i = 42;
/// frobnify(i)?;
/// ```
fn frobnify(i: usize) -> std::io::Result<()> {

```

Listing 6-5: Hiding lines in a doctest with #

**注意** 使用此功能时要小心；如果用户复制粘贴示例代码，然后由于您隐藏的必要步骤而无法正常工作，可能会令用户感到沮丧。

- 与 #[test] 函数类似，doctest 也支持修改运行方式的属性。这些属性紧跟在用于表示代码块的三个反引号之后，多个属性可以用逗号分隔。
- 与测试函数类似，您可以使用 should_panic 属性来指示特定的 doctest 在运行时应该发生 panic，或者使用 ignore 来仅在使用 --ignored 标志运行 cargo test 时检查代码段。您还可以使用 no_run 属性来指示给定的 doctest 应该编译但不应运行。
- 属性 compile_fail 告诉 rustdoc 文档示例中的代码不应该编译通过。这向用户表明某个特定的用法是不可能的，并作为一个有用的测试，提醒您在库的相关方面发生变化时更新文档。您还可以使用此属性来检查某些类型的静态属性是否成立。清单 6-6 展示了如何使用 compile_fail 来检查给定类型是否不实现 Send，这在不安全代码中可能需要维护安全性保证。

```rust


compile_fail
# struct MyNonSendType(std::rc::Rc<()>);
fn is_send<T: Send>() {}
is_send::<MyNonSendType>();

```

清单 6-6：使用 compile_fail 测试代码无法编译

`compile_fail` 是一个相对简单的工具，它不会提供代码无法编译的原因。例如，如果代码由于缺少分号而无法编译，那么 `compile_fail` 测试将会被认为是成功的。因此，通常情况下，您需要在确保测试确实无法编译并出现预期错误后才添加该属性。
如果您需要更细粒度的编译错误测试，例如在开发宏时，可以查看 trybuild crate。

#### Additional Testing Tools

测试远不止于运行测试函数并验证其产生预期结果。对测试技术、方法论和工具的全面调查超出了本书的范围，但在扩展您的测试技能时，有一些关键的 Rust 特定知识需要了解。

##### Linting

您可能不认为代码检查工具的检查是测试，但在Rust中，它们通常可以被视为测试。Rust代码检查工具Clippy将其许多lints分类为正确性lints。这些lints可以捕获编译通过但几乎肯定是错误的代码模式。一些示例包括a = b; b = a，未能交换a和b；std::mem::forget(t)，其中t是一个引用；以及for x in y.next()，它只会迭代y的第一个元素。如果您还没有将Clippy作为CI流程的一部分运行，那么您可能应该考虑这样做。

- Clippy提供了许多其他的lint，虽然通常很有帮助，但可能比您期望的更具主观性。例如，默认情况下启用的type_complexity lint会在您的程序中使用特别复杂的类型（例如Rc<Vec<Vec<Box<(u32, u32, u32, u32)>>>>）时发出警告。虽然这个警告鼓励您编写更易读的代码，但您可能会觉得它过于苛刻，不太实用。如果您的代码的某个部分错误地触发了特定的lint，或者您只想允许特定的实例，您可以使用#[allow(clippy::name_of_lint)]在该代码片段中禁用该lint。
- Rust编译器还提供了一套自己的lints，以警告的形式存在，尽管这些lints通常更多地用于指导编写符合惯用写法的代码，而不是检查正确性。相反，编译器中的正确性lints被视为错误（可以查看rustc -W help获取列表）。

**注意** 并非所有的编译器警告都是默认启用的。那些默认禁用的警告通常仍在不断完善，或者更多关注的是代码风格而非内容。一个很好的例子是“Rust 2018 edition”的惯用写法警告，您可以使用 #![warn(rust_2018_idioms)] 启用它。当启用此警告时，编译器会告诉您是否未能利用 Rust 2018 edition 带来的变化。当您开始一个新项目时，您可能还想养成启用一些其他警告的习惯，比如 missing_docs 和 missing_debug_implementations，它们会在您的 crate 中有任何公共项未被文档化或任何公共类型未添加 Debug 实现时发出警告。

#### Test Generation

编写一个良好的测试套件是一项很大的工作。即使在你完成了这项工作之后，你编写的测试只能测试你在编写它们时考虑到的特定行为集。幸运的是，你可以利用许多测试生成技术来开发更好、更全面的测试。这些技术为你生成输入，用于检查你的应用程序的正确性。有许多这样的工具存在，每个工具都有其自身的优点和缺点，所以在这里我只会介绍这些工具使用的主要策略：模糊测试和属性测试。

##### Fuzzing

整本书都可以写关于模糊测试的内容，但从高层次来看，它的思想很简单：生成随机输入到你的程序中，然后观察是否崩溃。如果程序崩溃了，那就是一个 bug。例如，如果你正在编写一个 URL 解析库，你可以通过系统地生成随机字符串并将它们传递给解析函数来进行模糊测试，直到它发生 panic。如果简单地按顺序生成输入，比如从 a 开始，然后是 b，然后是 c，等等，那么要生成一个像 http://[:]. 这样棘手的 URL 将需要很长时间。实际上，现代的模糊测试工具使用代码覆盖度指标来探索代码中的不同路径，这使得它们能够更快地达到更高的覆盖度，而不是完全随机选择输入。

- Fuzzers非常擅长发现您的代码无法正确处理的奇怪边界情况。它们需要很少的设置：您只需将模糊器指向一个接受“可模糊化”输入的函数，然后它就会开始工作。例如，清单6-7展示了如何对URL解析器进行模糊测试的示例。

```rust

libfuzzer_sys::fuzz_target!(|data: &[u8]| {
  if let Ok(s) = std::str::from_utf8(data) {
  let_ = url::Url::parse(s);
  }
});
```

清单 6-7：使用 libfuzzer 对 URL 解析器进行模糊测试

- 模糊器将为闭包生成半随机输入，并将任何形成有效的 UTF-8 字符串的输入传递给解析器。请注意，此处的代码不检查解析是否成功或失败，而是寻找解析器由于违反内部不变量而引发 panic 或其他崩溃的情况。
- 模糊测试工具会持续运行，直到您终止它，因此大多数模糊测试工具都配备了在探索一定数量的测试用例后停止的内置机制。如果您的输入不是一个简单的可模糊化类型，比如哈希表，您通常可以使用类似 arbitrary 的 crate 将模糊测试生成的字节字符串转换为更复杂的 Rust 类型。这感觉像是魔法，但在底层实际上是以非常直接的方式实现的。该 crate 定义了一个 Arbitrary trait，其中包含一个方法 arbitrary，该方法从随机字节源构造实现类型。原始类型如 u32 或 bool 从输入中读取所需数量的字节以构造自身的有效实例，而像 HashMap 或 BTreeSet 这样的更复杂类型则从输入中产生一个数字来指示它们的长度，然后在其内部类型上调用 Arbitrary 该数字次数。甚至还有一个属性 #[derive(Arbitrary)]，它通过在每个包含的类型上调用 arbitrary 来实现 Arbitrary！要进一步探索模糊测试，我建议从 cargo-fuzz 开始。

##### Property-Based Testing

有时候你不仅想检查程序是否崩溃，还想确保它按照预期工作。你的 add 函数没有发生 panic 是很好，但如果它告诉你 add(1, 4) 的结果是 68，那可能还是错的。这就是属性测试的作用；你描述了代码应该遵守的一些属性，然后属性测试框架生成输入并检查这些属性是否确实成立。

- 使用属性测试的常见方法是首先编写一个简单但不太高效的代码版本，你对其正确性有信心。
然后，对于给定的输入，将该输入分别提供给你要测试的代码和简化但不太高效的版本。
如果两个实现的结果或输出相同，那么你的代码是正确的，这就是你要验证的正确性属性。
但如果结果不同，那么很可能发现了一个 bug。
你还可以使用属性测试来检查与正确性直接相关的属性以外的属性，比如某个实现的操作是否比另一个实现的操作时间更短。
共同的原则是，你希望真实版本和测试版本之间的任何结果差异都能提供有用的信息和可操作性，以便每次失败都能让你进行改进。
简化但不太高效的实现可以是标准库中的一个（比如std::collections::VecDeque），你想要替换或增强它，也可以是一个你试图优化的算法的简化版本（比如朴素与优化的矩阵乘法）。

- 如果这种生成输入直到满足某个条件的方法听起来很像模糊测试，那是因为它确实是——比我聪明的人们认为模糊测试就是“只要不崩溃就行”的属性测试。
- 属性测试的一个缺点是它更加依赖于提供的输入描述。而模糊测试会尝试所有可能的输入，属性测试往往会根据开发者的注释进行引导，比如“0到64之间的数字”或“包含三个逗号的字符串”。这使得属性测试能够更快地找到模糊测试可能需要很长时间才能随机遇到的情况，但它需要手动工作，并可能忽略重要但特定的错误输入。然而，随着模糊测试和属性测试越来越接近，模糊测试工具也开始获得这种基于约束的搜索能力。
- 如果你对属性测试生成感兴趣，我推荐从 proptest crate 开始。
  
**测试操作序列**
由于模糊测试工具和属性测试工具允许您生成任意的 Rust 类型，因此您不仅限于测试 crate 中的单个函数调用。例如，假设您想要测试某个类型 Foo 在执行特定操作序列时是否正确行为。您可以定义一个列出操作的枚举 Operation，并使您的测试函数接受一个 Vec<Operation>。然后，您可以实例化一个 Foo，并依次对该 Foo 执行每个操作。大多数测试工具都支持最小化输入，因此，如果找到违反属性的输入，它们甚至会搜索最小的操作序列！

#### Test Augmentation

假设您已经设置了一个完美的测试套件，并且您的代码通过了所有的测试。这是非常美妙的。但是，有一天，其中一个通常可靠的测试莫名其妙地失败或者由于段错误而崩溃了。这种非确定性的测试失败有两个常见原因：竞态条件，即如果两个操作以特定顺序在不同的线程上发生，您的测试可能会失败；以及不安全代码中的未定义行为，例如如果某些不安全代码从未初始化的内存中读取特定值。[]: # END: ed8c6549bwf9

- 使用普通测试捕获这些类型的错误可能很困难 - 通常情况下，您无法对线程调度、内存布局和内容或其他类似随机的系统因素具有足够的低级控制，以编写可靠的测试。您可以在循环中多次运行每个测试，但即使如此，如果错误情况非常罕见或不太可能发生，也可能无法捕获错误。幸运的是，有一些工具可以帮助增强您的测试，使捕获这些类型的错误变得更容易。
- 其中一个工具是令人惊叹的 Miri，它是 Rust 的中级中间表示（MIR）的解释器。MIR 是 Rust 的内部简化表示，它帮助编译器找到优化并检查属性，而无需考虑 Rust 本身的所有语法糖。通过 Miri 运行测试非常简单，只需运行 cargo miri test。Miri 解释执行您的代码，而不是像普通二进制文件那样编译和运行它，这使得测试运行速度较慢。但作为回报，Miri 可以在代码的每一行执行时跟踪整个程序状态。这使得 Miri 能够检测并报告程序是否出现某些类型的未定义行为，例如未初始化的内存读取、在值被丢弃后继续使用或越界指针访问。Miri 不会让这些操作产生奇怪的程序行为，可能只有在某些情况下才会导致可观察的测试失败（如崩溃），而是在发生时立即检测并告知您。
- 例如，考虑清单 6-8 中非常不安全的代码，它创建了两个对值的独占引用。

```rust

let mut x = 42;
let x: *mut i32 = &mut x;
let (x1, x2) = unsafe { (&mut*x, &mut *x) };
println!("{} {}", x1, x2);
```

清单 6-8：Miri 检测到不正确的极不安全代码

- 在撰写本文时，如果您通过Miri运行此代码，您将收到一个指出问题所在的错误：

```rust
error: Undefined Behavior: trying to reborrow for Unique at alloc1383, but
parent tag <2772> does not have an appropriate item in the borrow stack
--> src/main.rs:4:6
|
4 | let (x1, x2) = unsafe { (&mut*x, &mut *x) };
| ^^ trying to reborrow for Unique at alloc1383, but parent tag <2772>
does not have an appropriate item in the borrow stack

```

**注意** Miri仍在开发中，其错误消息并不总是最容易理解的。这是一个正在积极解决的问题，所以当您阅读本文时，错误输出可能已经得到了很大的改进！

- 另一个值得一看的工具是 Loom，这是一个聪明的库，它尝试确保您的测试在每个相关的并发操作交错中运行。在高层次上，Loom跟踪所有跨线程同步点，并一遍又一遍地运行您的测试，每次调整线程从这些同步点继续的顺序。因此，如果线程A和线程B都获取相同的Mutex，Loom将确保测试先以A先获取，然后以B先获取的方式运行。Loom还跟踪原子访问、内存顺序和对UnsafeCell的访问（我们将在第9章中讨论），并检查线程是否不适当地访问它们。如果测试失败，Loom可以给出确切的运行情况，包括哪些线程以什么顺序执行，以便您确定崩溃发生的原因。

##### Performance Testing

编写性能测试很困难，因为往往很难准确地模拟反映您的 crate 在实际使用中的工作负载。但是拥有这样的测试非常重要；如果您的代码突然运行速度变慢了100倍，那真的应该被视为一个 bug，但如果没有性能测试，您可能无法发现这个回归。如果您的代码运行速度提高了100倍，这也可能表明有些地方出了问题。这两种情况都是将自动化性能测试作为 CI 的一部分的好理由——如果性能发生了显著的变化，无论是向好的方向还是向坏的方向，您都应该知道。

- 与功能测试不同，性能测试没有一个共同的、明确定义的输出。功能测试要么成功，要么失败，而性能测试可能会给出吞吐量、延迟配置文件、处理的样本数量或其他与所涉及应用程序相关的指标。此外，性能测试可能需要在循环中运行一个函数数十万次，或者可能需要在分布式多核盒子的网络上运行数小时。因此，很难以一般意义上讨论如何编写性能测试。相反，在本节中，我们将看一下在使用Rust编写性能测试时可能遇到的一些问题以及如何减轻这些问题。经常被忽视的三个常见陷阱是性能差异、编译器优化和I/O开销。让我们依次探讨每个问题。

##### Performance Variance

性能可能因各种原因而有所变化，许多因素会影响特定机器指令序列的运行速度。有些是显而易见的，比如CPU和内存时钟速度，或者机器的负载情况，但许多因素则更加微妙。例如，您的内核版本可能会影响分页性能，您的用户名长度可能会改变内存布局，房间的温度可能会导致CPU降频。最终，如果您运行两次基准测试，很不可能得到相同的结果。实际上，即使使用相同的硬件，您可能观察到显著的差异。或者，从另一个角度来看，您的代码可能变得更慢或更快，但由于基准测试环境的差异，这种影响可能是不可见的。

- 消除性能结果中的所有差异是不可能的，除非您能够在一个高度多样化的机器群上重复运行基准测试。即使如此，我们仍然需要尽力处理测量差异，以从嘈杂的测量结果中提取出信号。在实践中，我们在对抗差异方面的最佳助手是多次运行每个基准测试，然后查看测量结果的分布，而不仅仅是单个结果。Rust 提供了一些工具来帮助我们。例如，与其问“这个函数平均运行多长时间？”，像 hdrhistogram 这样的 crate 可以让我们查看诸如“我们观察到的样本中有多少运行时间落在了 95% 的范围内？”这样的统计数据。为了更加严谨，我们可以使用统计学中的零假设检验等技术，以建立一些置信度，确保测量到的差异确实对应于真正的变化，而不仅仅是噪音。
  
##### 编译器优化

现在的编译器非常聪明。它们消除死代码，编译时计算复杂表达式，展开循环，并进行其他黑魔法，以从我们的代码中挤取出每一点性能。通常情况下，这很好，但当我们试图测量特定代码段的速度时，编译器的聪明才智可能会给我们带来无效的结果。例如，看一下清单 6-9 中用于对 Vec::push 进行基准测试的代码。

```rust

let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
vs.push(i);
}
println!("took {:?}", start.elapsed());
```

清单 6-9：一个可疑地快速的性能基准测试

- 如果你查看使用像 godbolt.org 或 cargo-asm 这样的工具编译的 release 模式下的汇编输出，你会立即注意到有些不对劲：对 Vec::with_capacity 和 Vec::push 的调用，以及整个 for 循环，都不见了。它们被完全优化掉了。编译器意识到代码中实际上没有需要执行的向量操作，并将其消除为死代码。当然，编译器完全有权这样做，但对于基准测试来说，这并不特别有帮助。
- 为了避免这些优化对基准测试的影响，标准库提供了 std::hint::black_box。这个函数在撰写本文时仍在讨论和混淆中，并且仍在等待稳定，但它非常有用，值得在这里讨论。在其核心，它只是一个恒等函数（接受 x 并返回 x），告诉编译器假设函数的参数以任意（合法）的方式使用。它不会阻止编译器对输入参数应用优化，也不会阻止编译器优化返回值的使用方式。相反，它鼓励编译器实际计算函数的参数（在假设参数将被使用的情况下），并将结果存储在对 CPU 可访问的某个地方，以便可以使用 black_box 调用计算出的值。编译器可以自由地在编译时计算输入参数，但它仍应将结果注入到程序中。

- 对于我们许多基准测试需求来说，这个函数就足够了，尽管不得不承认并非全部。例如，我们可以对清单 6-9 进行注释，以使向量访问不再被优化掉，如清单 6-10 所示。

```rust

let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
    black_box(vs.as_ptr());
    vs.push(i);
    black_box(vs.as_ptr());
}
println!("took {:?}", start.elapsed());
```

清单 6-10：清单 6-9 的修正版本

- 我们告诉编译器在每次循环迭代之前和之后，假设 vs 在任意方式上被使用，包括 push 调用。这迫使编译器按顺序执行每个 push，而不会合并或优化连续的调用，因为它必须假设在每个调用之间可能发生“无法优化掉的任意操作”（这就是 black_box 的作用）。
- 注意我们使用了 vs.as_ptr() 而不是 &vs。这是因为编译器应该假设 black_box 可以对其参数执行任何合法操作的一个警告。通过共享引用对 Vec 进行变异是不合法的，所以如果我们使用 black_box(&vs)，编译器可能会注意到 vs 在循环迭代之间不会改变，并基于这个观察实施优化！

###### I/O Overhead Measurement

在编写基准测试时，很容易意外地测量错误的内容。例如，我们经常希望实时获取有关基准测试进度的信息。为了做到这一点，我们可能会编写类似于清单 6-11 中的代码，旨在测量 my_function 的运行速度：

```rust

let start = std::time::Instant::now();
for i in 0..1_000_000 {
  println!("iteration {}", i);
  my_function();
}
println!("took {:?}", start.elapsed());
```

清单 6-11：我们真正在这里进行基准测试的是什么？

- 这看起来似乎达到了目标，但实际上它并没有真正测量 my_function 的速度。相反，这个循环很可能告诉我们打印一百万个数字需要多长时间。循环体中的 println! 在幕后做了很多工作：它将二进制整数转换为十进制数字进行打印，锁定标准输出，使用至少一个系统调用写出一系列 UTF-8 代码点，然后释放标准输出锁。不仅如此，如果您的终端打印输入的速度较慢，系统调用可能会阻塞。这是很多周期！而调用 my_function 的时间可能相形见绌。
- 当基准测试使用随机数时，类似的情况也会发生。如果你在循环中运行my_function(rand::random())，你可能主要测量的是生成一百万个随机数所花费的时间。对于获取当前时间、读取配置文件或启动新线程等操作也是如此，相对而言，这些操作都需要很长时间，可能会掩盖你实际想要测量的时间。
- 幸运的是，一旦你意识到这个问题，解决起来通常很容易。确保你的基准测试循环的主体几乎只包含你想要测量的特定代码。所有其他代码应该在基准测试开始之前或在基准测试的测量部分之外运行。如果你使用 criterion，可以看一下它提供的不同计时循环，它们都是为了满足需要不同测量策略的基准测试情况！

#### 摘要

在本章中，我们详细探讨了Rust提供的内置测试功能。我们还介绍了一些在测试Rust代码时有用的测试设施和技术。这是本书中关于中级Rust使用的高级方面的最后一章。从下一章关于声明式和过程式宏开始，我们将更加专注于Rust代码。下一页见！

### 7.MACROS

宏本质上是一种让编译器为您编写代码的工具。您给编译器一个根据一些输入参数生成代码的公式，编译器会将每次调用宏的地方替换为运行公式的结果。您可以将宏视为自动代码替换，其中您可以定义替换的规则。

- Rust的宏有很多不同的形式和大小，可以轻松实现许多不同形式的代码生成。主要有两种类型的宏：声明式宏和过程宏，我们将在本章中探讨这两种宏。我们还将介绍一些宏在日常编码中的便利之处，以及在更高级使用中可能出现的一些陷阱。
- 来自基于C的语言的程序员可能习惯于C和C++的邪恶之地，您可以使用#define将每个true更改为false，或者删除所有else关键字的出现。如果您是这种情况，您需要将宏与做一些“不好”的感觉分离开来。在Rust中，宏远非C宏的荒野西部。它们遵循（大多数）明确定义的规则，并且相当难以滥用。

#### Declarative Macros

声明式宏是使用`macro_rules!`语法定义的宏，它允许您方便地定义类似函数的宏，而无需编写专用的 crate（与过程宏不同）。一旦您定义了声明式宏，就可以使用宏的名称后跟感叹号来调用它。我喜欢将这种宏看作是一种编译器辅助的搜索和替换：它可以完成许多常规的、结构良好的转换任务，以及消除重复的样板代码。在您到目前为止对 Rust 的经验中，您认识的大多数宏可能都是声明式宏。但请注意，并非所有的函数式宏都是声明式宏；`macro_rules!` 本身就是一个例子，`format_args!` 是另一个例子。感叹号后缀只是告诉编译器，在编译时，宏调用将被替换为不同的源代码。

**注意** 由于Rust的解析器专门识别和解析带有!的宏调用，因此您只能在解析器允许的位置使用它们。它们可以在大多数您期望的位置使用，比如在表达式位置或impl块中，但并非所有地方都可以。例如，在需要标识符或匹配分支的位置，您（在撰写本文时）无法调用类似函数的宏。

- 或许并不立即明显为什么称呼声明式宏为声明式。毕竟，在你的程序中，你不是“声明”了一切吗？在这个上下文中，声明式指的是你不说出宏的输入应该如何转换为输出，只是说当输入为B时，你希望输出看起来像A。你声明它应该是这样的，编译器会找出所有必须发生的解析重连，使你的声明成为现实。这使得声明式宏简洁而富有表现力，尽管它们也往往变得相当晦涩，因为你只有有限的语言来表达你的声明。

##### 何时使用它们

声明式宏在你发现自己一遍又一遍地编写相同代码时非常有用，而你希望不再这样做。它们最适合于相对机械的替换——如果你想进行复杂的代码转换或大量的代码生成，那么过程宏可能更适合。

- 我最常使用声明式宏的情况是在我发现自己编写重复且结构相似的代码时，比如在测试和特性实现中。对于测试，我经常希望以稍微不同的配置运行相同的测试多次。我可能会像清单 7-1 中所示那样编写代码。

```rust

fn test_inner<T>(init: T, frobnify: bool) { ... }

# [test]

fn test_1u8_frobnified() {
test_inner(1u8, true);
}
// ...

# [test]

fn test_1i128_not_frobnified() {
test_inner(1i128, false);
}
```

清单 7-1：重复的测试代码
虽然这样可以工作，但它太冗长、太重复，而且容易出错。通过使用宏，我们可以做得更好，如清单 7-2 所示。

```rust
macro_rules! test_battery {
($($t:ty as $name:ident),*)) => {
$(
mod $name {

# [test]

fn frobnified() { test_inner::<$t>(1, true) }

# [test]

fn unfrobnified() { test_inner::<$t>(1, false) }
}
)*
}
}
test_battery! {
u8 as u8_tests,
// ...
i128 as i128_tests
);

```

清单 7-2：让宏为您重复

- 这个宏将每个逗号分隔的指令扩展为自己的模块，每个模块包含两个测试，一个调用带有true参数的test_inner，另一个调用带有false参数的test_inner。虽然宏的定义并不简单，但它使添加更多的测试变得更容易。在test_battery!调用中，每个类型占据一行，宏将负责生成针对true和false参数的测试。我们还可以让它为init的不同值生成测试。现在，我们大大降低了忘记测试特定配置的可能性！
- 对于trait实现，情况类似。如果你定义了自己的trait，通常你会希望为标准库中的一些类型实现该trait，即使这些实现是微不足道的。假设你发明了Clone trait，并希望为标准库中的所有Copy类型实现它。而不是手动为每个类型编写实现，你可以使用像清单7-3中的宏一样的工具。

```rust
macro_rules! clone_from_copy {
($($t:ty),_) => {
$(impl Clone for $t {
fn clone(&self) -> Self { *self }
})*
}
}
clone_from_copy![bool, f32, f64, u8, i8, /* ... */];
```

清单 7-3：使用宏一次性为许多相似类型实现 trait

- 在这里，我们为每个提供的类型生成了一个 Clone 的实现，其主体只是使用 * 从 &self 复制出来。你可能会想为什么我们不为 T: Copy 的类型添加一个 Clone 的通用实现。我们可以这样做，但一个很大的原因是它会强制其他 crate 中的类型也使用相同的 Clone 实现来处理它们自己的类型，而这些类型恰好是 Copy 的。一个名为 specialization 的实验性编译器特性可能提供了一种解决方法，但在撰写本文时，该特性的稳定化还有一段时间。因此，目前我们最好是具体列举这些类型。这种模式也不仅限于简单的转发实现：例如，你可以很容易地修改清单 7-3 中的代码，为所有整数类型实现一个 AddOne trait！
**注意** 如果你曾经想知道是否应该使用泛型还是声明式宏，那么你应该使用泛型。泛型通常比宏更符合人体工程学，并且与语言中的其他结构更好地集成。考虑以下经验法则：如果你的代码基于类型而变化，使用泛型；否则，使用宏。

##### How They Work

每种编程语言都有一种语法，它规定了如何将构成源代码的个别字符转换为标记（tokens）。
标记是语言的最低级构建块，例如数字、标点符号、字符串和字符字面量以及标识符；在这个级别上，语言关键字和变量名之间没有区别。例如，在类似Rust的语法中，文本(value + 4)将由五个标记序列(, value, +, 4, )表示。将文本转换为标记的过程还为编译器的其余部分和解析文本的低级细节提供了一层抽象。例如，在标记表示中，没有空白字符的概念，"/_foo_*/"和"/_foo_/"具有不同的表示（前者不是标记，后者是一个具有内容"/_foo_/"的字符串字面量标记）。

- 一旦源代码被转换为一系列的标记，编译器会遍历这个序列并为标记赋予语法意义。例如，以()为界定的标记组成一个组，!标记表示宏调用，等等。这就是解析的过程，最终产生了描述源代码结构的抽象语法树（AST）。举个例子，考虑表达式let x = || 4，它由一系列的标记组成：let（关键字）、x（标识符）、=（标点符号）、两个|（标点符号）和4（字面量）。当编译器将其转换为语法树时，它将其表示为一个语句，其模式是标识符x，右侧的表达式是一个没有参数的闭包，其主体是一个整数字面量4。请注意，语法树表示比标记序列更丰富，因为它根据语言的语法为标记组合赋予了语法意义。
- Rust宏决定了给定一系列标记被转换成的语法树。当编译器在解析过程中遇到宏调用时，它必须评估宏以确定替换的标记，这些标记最终将成为宏调用的语法树。然而，在这一点上，编译器仍然在解析标记，可能还没有准备好评估宏，因为它只是解析了宏定义的标记。因此，编译器推迟解析宏调用括号内的任何内容，并记住输入的标记序列。当编译器准备好评估指定的宏时，它在标记序列上评估宏，解析生成的标记，并将结果的语法树替换到宏调用的位置。
- 从技术上讲，编译器确实对宏的输入进行了一些解析。具体来说，它解析出基本的内容，如字符串字面量和分隔的组，并生成一系列的标记树，而不仅仅是标记。例如，代码x - (a.b + 4)解析为三个标记树的序列。第一个标记树是一个单独的标记，即标识符x，第二个标记树是一个单独的标记，即标点符号-，第三个标记树是一个组（使用括号作为分隔符），它本身由五个标记树组成：a（一个标识符）、.（标点符号）、b（另一个标识符）、+（另一个标点符号）和4（一个字面量）。这意味着宏的输入不一定是有效的Rust代码，但它必须由Rust编译器可以解析的代码组成。例如，你不能在Rust中写for <- x，但在宏调用内部可以，只要宏生成有效的语法。另一方面，你不能将for {传递给宏，因为它没有闭合的大括号。
- 声明宏始终生成有效的 Rust 代码作为输出。你不能让宏生成函数调用的前半部分或者没有后续代码块的 if 语句。声明宏必须生成一个表达式（基本上任何可以赋值给变量的东西）、一个语句（比如 let x = 1;）、一个项（如 trait 定义或 impl 块）、一个类型或者一个匹配模式。这使得 Rust 宏不容易被滥用：你简单地不能编写一个生成无效 Rust 代码的声明宏，因为宏定义本身就无法编译通过！
- 从高层次来看，这就是声明式宏的全部内容 - 当编译器遇到宏调用时，它将传递给宏的标记解析为标记流，并用生成的抽象语法树替换宏调用。

##### How to Write Declarative Macros

详细解释声明式宏支持的所有语法超出了本书的范围。然而，我们将介绍一些基础知识，因为有一些值得注意的奇怪之处。

- 声明式宏由两个主要部分组成：匹配器和转录器。
一个给定的宏可以有多个匹配器，每个匹配器都有一个关联的转录器。
当编译器找到一个宏调用时，它会从第一个到最后一个遍历宏的匹配器，
当它找到一个与调用中的标记匹配的匹配器时，它会通过遍历相应转录器的标记来替换调用。
清单 7-4 展示了声明式宏规则的不同部分如何组合在一起。

```rust
macro_rules! /_macro name _/ {
(/_ 1st matcher _/) => { /_ 1st transcriber _/ };
(/_ 2nd matcher _/) => { /_ 2nd transcriber_/ };
}
```

清单 7-4：声明式宏定义组件

##### 匹配器

- 您可以将宏匹配器视为编译器尝试以预定义的方式扭曲和弯曲以匹配在调用点给定的输入标记树的标记树。例如，考虑具有匹配器$a:ident + $b:expr的宏。该匹配器将匹配任何标识符（:ident）后跟加号，后跟任何Rust表达式（:expr）。如果使用x + 3 _5调用宏，编译器会注意到匹配器在设置$a = x和$b = 3_5时匹配。即使在匹配器中没有出现_，编译器也意识到3_5是一个有效的表达式，因此可以与接受任何表达式的$b:expr匹配（:expr部分）。

- 匹配器可以变得非常复杂，但它们具有巨大的表达能力，就像正则表达式一样。以一个不太复杂的例子为例，这个匹配器接受一个以 key => value 格式给出的逗号分隔的键值对序列 ($())，其中至少有一个 (+) 键值对：

```rust
$($key:expr => $value:expr),+

```

而且，至关重要的是，使用这个匹配器调用宏的代码可以为键或值提供任意复杂的表达式 - 匹配器的魔力会确保键和值表达式被适当地分割。

- 宏规则支持各种片段类型；你已经看到了标识符的 :ident 和表达式的 :expr，还有类型的 :ty，甚至还有任何单个标记树的 :tt！你可以在 Rust 语言参考手册的第三章中找到片段类型的完整列表（https://doc.rust-lang.org/reference/macros-by-example.html）。这些片段类型，再加上重复匹配模式的机制（$()），使你能够匹配大多数简单的代码模式。然而，如果你发现使用匹配器难以表达你想要的模式，你可以尝试使用过程宏，其中你不需要遵循 macro_rules! 所要求的严格语法。我们将在本章后面更详细地介绍这些内容。

##### Transcribers

- 一旦编译器匹配了声明式宏的匹配器，它就会使用匹配器关联的转录器生成代码。宏匹配器定义的变量被称为元变量，编译器会将转录器中的每个元变量的出现替换为与匹配器中相应部分匹配的输入。如果匹配器中有重复（如前面示例中的$()，+），您可以在转录器中使用相同的语法，它将根据输入中的每个匹配重复一次，并且每个扩展都包含适当的替换值。例如，对于$key和$value匹配器，我们可以编写以下转录器，为每个匹配的$key/$value对生成一个插入调用到某个映射中：

```rust
$(map.insert($key, $value);)+
```

- 注意，在这里我们希望每次重复都有一个分号，而不仅仅是用于分隔重复，因此我们将分号放在重复的括号内。


**注意** 在转录器中的每个重复中，您必须使用一个元变量，以便编译器知道在匹配器中使用哪个重复（如果有多个）。

##### Hygiene

你可能听说过 Rust 的宏是清洁的，也许认为它们的清洁性使它们更安全、更易于使用，但可能并不完全理解这是什么意思。当我们说 Rust 的宏是清洁的时，我们的意思是声明式宏（通常）不能影响未显式传递给它的变量。一个简单的例子是，如果你声明了一个名为 foo 的变量，然后调用一个也定义了名为 foo 的变量的宏，那么默认情况下，宏的 foo 在调用点（宏被调用的地方）是不可见的。同样地，宏不能访问在调用点定义的变量（甚至是 self），除非它们被显式传递进来。
大多数情况下，你可以将宏标识符视为存在于与它们展开的代码不同的命名空间中。举个例子，看一下清单 7-5 中的代码，其中有一个宏试图（但失败了）在调用点遮蔽一个变量。

```rust

macro_rules! let_foo {
($x:expr) => {
let foo = $x;
}
}
let foo = 1;
// expands to let foo = 2;
let_foo!(2);
assert_eq!(foo, 1);

```
清单 7-5：宏存在于它们自己的小宇宙中。大多数情况下。

- 在编译器展开 let_foo!(2) 后，assert 看起来应该会失败。然而，原始代码中的 foo 和宏生成的 foo 存在于不同的宇宙中，除了它们恰好共享一个可读的名称之外，它们之间没有任何关系。实际上，编译器会抱怨宏中的 let foo 是一个未使用的变量。这种隔离对于使宏更容易调试非常有帮助 - 你不必担心因为选择了相同的变量名而意外遮蔽或覆盖宏调用者中的变量！
- 然而，这种清洁的分离并不适用于除变量标识符之外的内容。声明式宏与调用点共享类型、模块和函数的命名空间。这意味着你的宏可以定义新的函数，在调用点处调用它们，为在其他地方定义的类型添加新的实现（而不是传递进来的类型），引入一个新的模块，然后可以在宏被调用的地方访问它，等等。这是有意设计的 - 如果宏不能影响更广泛的代码，那么使用宏生成类型、特性实现和函数将变得更加繁琐，而这正是它们最有用的地方。
- 当编写一个希望从你的 crate 中导出的宏时，宏中类型的缺乏清洁性尤为重要。为了使宏真正可重用，你不能假设调用方的作用域中会有什么类型。也许调用你的宏的代码定义了一个 mod std {}，或者导入了自己的 Result 类型。为了保险起见，确保使用完全指定的类型，如 ::core::option::Option 或 ::alloc::boxed::Box。如果你特别需要引用定义宏的 crate 中的内容，可以使用特殊的元变量 $crate。
  
**注意** 如果可能的话，避免使用 ::std 路径，以便宏在 no_std crate 中继续工作。


- 如果你希望宏影响调用者作用域中的特定变量，你可以选择在宏和调用者之间共享标识符。关键是记住标识符的来源，因为它将与该命名空间绑定。如果在宏中放置 let foo = 1;，那么标识符 foo 的来源是宏，将永远不会在调用者的标识符命名空间中可用。另一方面，如果宏以 $foo:ident 作为参数，然后写入 let $foo = 1;，当调用者使用 !(foo) 调用宏时，标识符将源自调用者，并且将引用调用者作用域中的 foo。

- 标识符不必如此明确地传递；在宏外部源自代码中出现的任何标识符都将引用调用者作用域中的标识符。在清单 7-6 的示例中，变量标识符出现在 :expr 中，但仍然可以访问调用者作用域中的变量。

```rust
macro_rules! please_set {
($i:ident, $x:expr) => {
  $i = $x;
  }

}
let mut x = 1;
please_set!(x, x + 1);
assert_eq!(x, 2);
```

清单 7-6：让宏在调用点处访问标识符

- 我们可以在宏中使用 = $i + 1，但是我们不能使用 = x + 1，因为宏的定义范围中没有名为 x 的变量。
- 关于声明式宏和作用域的最后一点说明：与 Rust 中的几乎所有其他内容不同，声明式宏只有在声明后才存在于源代码中。如果您尝试在文件中后面定义的位置使用宏，这是行不通的！这适用于整个项目；如果您在一个模块中声明了一个宏，并希望在另一个模块中使用它，那么声明宏的模块必须出现在 crate 中的较早位置，而不是较晚位置。如果 foo 和 bar 是 crate 根目录下的模块，并且 foo 声明了一个 bar 想要使用的宏，那么 mod foo 必须在 lib.rs 中出现在 mod bar 之前！
**注意** 这种奇怪的宏作用域规则有一个例外（正式称为文本作用域），即如果你使用#[macro_export]标记宏。这个注解将宏提升到crate的根部，并将其标记为pub，以便它可以在你的crate中的任何地方或者被你的crate的依赖项使用。

#### Procedural Macros

- 你可以将过程宏看作是解析器和代码生成的组合，其中你在中间编写粘合代码。在高层次上，使用过程宏，编译器收集到宏的输入令牌序列，并运行你的程序来确定要用什么令牌替换它们。
- 过程宏之所以被称为过程宏，是因为你定义了如何根据一些输入令牌生成代码，而不仅仅是编写生成的代码。编译器在这方面几乎没有太多智能的参与 - 就它所知，过程宏更像是一个源代码预处理器，可以任意替换代码。你的输入仍然需要能够被解析为一系列 Rust 令牌的流，但仅此而已！

##### Types of Procedural Macros

过程宏有三种不同的类型，每种类型都专门用于特定的常见用例：
• 函数宏，例如由 macro_rules! 生成的宏
• 属性宏，例如 #[test]
• 派生宏，例如 #[derive(Serialize)]
这三种类型使用相同的底层机制：编译器会向宏提供一系列的标记，并期望你返回一系列与输入树（可能）相关的标记。然而，它们在宏的调用方式和输出处理方式上有所不同。我们将简要介绍每一种类型。

###### Function-Like Macros

函数宏是过程宏中最简单的形式。与声明式宏类似，它只是将调用点处的宏代码替换为过程宏返回的代码。然而，与声明式宏不同，函数宏没有任何限制：这些宏（像所有的过程宏一样）不需要是清洁的，并且不会保护您免受与调用点周围代码中的标识符交互的影响。相反，您的宏应该明确指出哪些标识符应与周围代码重叠（使用Span::call_site），哪些应该被视为宏的私有标识符（使用Span::mixed_site，我们稍后会讨论）。

###### Attribute Macros

属性宏还会整体替换属性所赋予的项，但它接受两个输入：出现在属性中的标记树（不包括属性的名称）和整个项的标记树，包括该项可能具有的其他属性。属性宏允许您轻松编写一个过程宏，用于转换项，例如通过向函数定义添加前言或尾声（类似于 #[test] 的作用）或通过修改结构体的字段。

###### Derive Macros

The derive macro is slightly different from the other two in that it adds
to, rather than replaces, the target of the macro. Even though this limitation
may seem severe, derive macros were one of the original motivating
factors behind the creation of procedural macros. Specifically, the serde
crate needed derive macros to be able to implement its now-well-known # [derive(Serialize, Deserialize)] magic

- Derive macros are arguably the simplest of the procedural macros,
since they have such a rigid form: you can append items only after the
annotated item; you can’t replace the annotated item, and you cannot have
the derivation take arguments. Derive macros do allow you to define helper
attributes—attributes that can be placed inside the annotated type to give
clues to the derive macro (like #[serde(skip)])—but these function mostly
like markers and are not independent macros.

###### The Cost of Procedural Macros
Before we talk about when each of the different procedural macro types is
appropriate, it’s worth discussing why you may want to think twice before
you reach for a procedural macro—namely, increased compile time.

- Procedural macros can significantly increase compile times for two
main reasons. The first is that they tend to bring with them some pretty
heavy dependencies. For example, the syn crate, which provides a parser
for Rust token streams that makes the experience of writing procedural
macros much easier, can take tens of seconds to compile with all features
enabled. You can (and should) mitigate this by disabling features you do
not need and compiling your procedural macros in debug mode rather
than release mode. Code often compiles several times faster in debug
mode, and for most procedural macros, you won’t even notice the difference
in execution time.
- The second reason why procedural macros increase compile time is
that they make it easy for you to generate a lot of code without realizing it.
While the macro saves you from having to actually type the generated code,
it does not save the compiler from having to parse, compile, and optimize
it. As you use more procedural macros, that generated boilerplate adds up,
and it can bloat your compile times.
- That said, the actual execution time of procedural macros is rarely a
factor in overall compile time. While the compiler has to wait for the procedural
macro to do its thing before it can continue, in practice, most procedural
macros don’t do any heavy computation. That said, if your procedural
macro is particularly involved, you may end up with your compiles spending
a significant chunk of execution time on your procedural macro code,
which is worth keeping an eye out for!

###### So You Think You Want a Macro

Let’s now look at some good uses for each type of procedural macro. We’ll
start with the easy one: derive macros.

###### When to Use Derive Macros

Derive macros are used for one thing, and one thing only: to automate the
implementation of a trait where automation is possible. Not all traits have
obvious automated implementations, but many do. In practice, you should
consider adding a derive macro for a trait only if the trait is implemented
often and if its implementation for any given type is fairly obvious. The first
of these conditions may seem like common sense; if your trait is going to be
implemented only once or twice, it’s probably not worth writing and maintaining
a convoluted derive macro for it.
The second condition may seem stranger, however: what does it mean
for the implementation to be “obvious”? Consider a trait like Debug. If you
were told what Debug does and were shown a type, you would probably
expect an implementation of Debug to output the name of each field alongside
the debug representation of its value. And that’s what derive(Debug)
does. What about Clone? You’d probably expect it to just clone every field—
and again, that’s what derive(Clone) does. With derive(serde::Serialize), we
expect it to serialize every field and its value, and it does just that. In general,
you want the derivation of a trait to match the developer’s intuition for
what it probably does. If there is no obvious derivation for a trait, or worse
yet, if your derivation does not match the obvious implementation, then
you’re probably better off not giving it a derive macro.

112 Chapter 7
When to Use Function-Like Macros
Function-like macros are harder to give a general rule of thumb for. You
might say you should use function-like macros when you want a functionlike
macro but can’t express it with macro_rules!, but that’s a fairly subjective
guideline. You can do a lot with declarative macros if you really put your
mind to it, after all!
There are two particularly good reasons to reach for a function-like
macro:
• If you already have a declarative macro, and its definition is becoming
so hairy that the macro is hard to maintain.
• If you have a pure function that you need to be able to execute at compile
time but cannot express it with const fn. An example of this is the phf
crate, which generates a hash map or set using a perfect hash function
when given a set of keys provided at compile time. Another is hex-
literal,
which takes a string of hexadecimal characters and replaces it with the
corresponding bytes. In general, anything that does not merely transform
the input at compile time but actually computes over it is likely to
be a good candidate.
I do not recommend reaching for a function-like macro just so that you
can break hygiene within your macro. Hygiene for function-like macros is a
feature that avoids many debugging headaches, and you should think very
carefully before you intentionally break it.
When to Use Attribute Macros
That leaves us with attribute macros. Though these are arguably the most
general of procedural macros, they are also the hardest to know when to
use. Over the years and time and time again, I have seen four ways in which
attribute macros add tremendous value.
Test generation
It is very common to want to run the same test under multiple different
configurations, or many similar tests with the same bootstrapping code.
While a declarative macro may let you express this, your code is often
easier to read and maintain if you have an attribute like #[foo_test] that
introduces a setup prelude and postscript in each annotated test, or a
repeatable attribute like #[test_case(1)] #[test_case(2)] to mark that a
given test should be repeated multiple times, once with each input.
Framework annotations
Libraries like rocket use attribute macros to augment functions and
types with additional information that the framework then uses without
the user having to do a lot of manual configuration. It’s so much more
convenient to be able to write #[get("/<name>")] fn hello(name: String)
than to have to set up a configuration struct with function pointers and

Macros 113
the like. Essentially, the attributes make up a miniature domain-specific
language (DSL) that hides a lot of boilerplate that’d otherwise be necessary.
Similarly, the asynchronous I/O framework tokio lets you use

# [tokio::main] async fn main() to automatically set up a runtime and run

your asynchronous code, thereby saving you from writing the same runtime
setup in every asynchronous application’s main function.
Transparent middleware
Some libraries want to inject themselves into your application in unobtrusive
ways to provide added value that does not change the application’s
functionality. For example, tracing and logging libraries like tracing and
metric collection libraries like metered allow you to transparently instrument
a function by adding an attribute to it, and then every call to that
function will run some additional code dictated by the library.
Type transformers
Sometimes you want to go beyond merely deriving traits for a type and
actually change the type’s definition in some fundamental way. In these
cases, attribute macros are the way to go. The pin_project crate is a great
example of this: its primary purpose is not to implement a particular
trait but rather to ensure that all pinned access to fields of a given type
happens according to the strict rules that are set forth by Rust’s Pin type
and the Unpin trait (we’ll talk more about those types in Chapter 8).
It does this by generating additional helper types, adding methods to
the annotated type, and introducing static safety checks to ensure that
users don’t accidentally shoot themselves in the foot. While pin_project
could have been implemented with a procedural derive macro, that
derived trait implementation would likely not have been obvious, which
violates one of our rules for when to use procedural macros.
How Do They Work?
At the heart of all procedural macros is the TokenStream type, which can be
iterated over to get the individual TokenTree items that make up that token
stream. A TokenTree is either a single token—like an identifier, punctuation,
or a literal—or another TokenStream enclosed in a delimiter like () or {}. By
walking a TokenStream, you can parse out whatever syntax you wish as long as
the individual tokens are valid Rust tokens. If you want to parse your input
specifically as Rust code, you will likely want to use the syn crate, which
implements a complete Rust parser and can turn a TokenStream into an easyto-
traverse Rust AST.
With most procedural macros, you want to not only parse a TokenStream
but also produce Rust code to be injected into the program that invokes the
procedural macro. There are two main ways to do so. The first is to manually
construct a TokenStream and extend it one TokenTree at a time. The second is to
use TokenStream’s implementation of FromStr, which lets you parse a string that

114 Chapter 7
contains Rust code into a TokenStream with "".parse::<TokenStream>(). You can
also mix and match these; if you want to prepend some code to your macro’s
input, just construct a TokenStream for the prologue, and then use the Extend
trait to append the original input.
**NOTE** TokenStream also implements Display, which pretty-prints the tokens in the stream.
This comes in super handy for debugging!
Tokens are very slightly more magical than I’ve described so far in that
every token, and indeed every TokenTree, also has a span. Spans are how the
compiler ties generated code back to the source code that generated it.
Every token’s span marks where that token originated. For example, consider
a (declarative) macro like the one in Listing 7-7, which generates a
trivial Debug implementation for the provided type.
macro_rules! name_as_debug {
($t:ty) => {
impl ::core::fmt::Debug for $t {
fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result
{ ::core::write!(f, ::core::stringify!($t)) }
} }; }
Listing 7-7: A very simple macro for implementing Debug
Now let’s imagine that someone invokes this macro with name_as_debug!
(u31). Technically, the compiler error occurs inside the macro, specifically
where we write for $t (the other use of $t can handle an invalid type). But
we’d like the compiler to point the user at the u31 in their code—and indeed,
that’s what spans let us do.
The span of the $t in the generated code is the code mapped to $t in
the macro invocation. That information is then carried through the compiler
and associated with the eventual compiler error. When that compiler
error is eventually printed, the compiler will print the error from inside the
macro saying that the type u31 does not exist but will highlight the u31 argument
in the macro invocation, since that’s the error’s associated span!
Spans are quite flexible, and they enable you to write procedural
macros that can produce sophisticated error messages if you use the
compile_
error! macro. As its name implies, compile_error! causes the compiler
to emit an error wherever it is placed with the provided string as the
message. This may not seem very useful, until you pair it with a span. By
setting the span of the TokenTree you generate for the compile_error! invocation
to be equal to the span of some subset of the input, you are effectively
telling the compiler to emit this compiler error and point the user to this
part of the source. Together, these two mechanisms let a macro produce
errors that seem to stem from the relevant part of the code, even though
the actual compiler error is somewhere in the generated code that the
user never even sees!

Macros 115
**NOTE** If you’ve ever been curious how syn’s error handling works, its Error type implements
an Error::to_compile_error method, which turns it into a TokenStream that
holds only a compile_error! directive. What’s particularly neat with syn’s Error type
is that it internally holds a collection of errors, each of which produces a distinct
compile_
error! directive with its own span so that you can easily produce multiple
independent errors from your procedural macro.
The power of spans doesn’t end there; spans are also how Rust’s macro
hygiene is implemented. When you construct an Ident token, you also give
the span for that identifier, and that span dictates the scope of that identifier.
If you set the identifier’s span to be Span::call_site(), the identifier
is resolved where the macro was called from and will thus not be isolated
from the surrounding scope. If, on the other hand, you set it to Span::mixed
_site() then (variable) identifiers are resolved at the macro definition site,
and so will be completely hygienic with respect to similarly named variables
at the call site. Span::mixed_site is so called because it matches the rules
around identifier hygiene for macro_rules!, which, as we discussed earlier,
“mixes” identifier resolution between using the macro definition site for
variables and using the call site for types, modules, and everything else.
Summary
In this chapter we covered both declarative and procedural macros, and
looked at when you might find each of them useful in your own code. We
also took a deeper dive into the mechanisms that underpin each type of
macro and some of the features and gotchas to be aware of when you write
your own macros. In the next chapter, we’ll start our journey into asynchronous
programming and the Future trait. I promise—it’s just on
the next page.

### 8.ASYNCHRONOUS PROGRAMMING

Asynchronous programming is, as the
name implies, programming that is not
synchronous.
At a high level, an asynchronous
operation is one that executes in the
background—the program won’t wait for the asynchronous
operation to complete but will instead
continue
to the next line of code immediately.
If you’re not already familiar with asynchronous programming, that definition
may feel insufficient as it doesn’t actually explain what asynchronous
programming is. To really understand the asynchronous programming
model and how it works in Rust, we have to first dig into what the alternative
is. That is, we need to understand the synchronous programming
model before we can understand the asynchronous one. This is important
in both clarifying the concepts and demonstrating the trade-offs of using
asynchronous programming: an asynchronous solution is not always the
right one! We’ll start this chapter by taking a quick journey through what

118 Chapter 8
motivates asynchronous programming as a concept in the first place; then
we’ll dig into how asynchrony in Rust actually works under the hood.
What’s the Deal with Asynchrony?
Before we get to the details of the synchronous and asynchronous programming
models, we first need to take a quick look at what your computer is
actually doing when it runs your programs.
Computers are fast. Really fast. So fast, in fact, that they spend most of
their time waiting for things to happen. Unless you’re decompressing files,
encoding audio, or crunching numbers, chances are that your CPU mostly
sits idle, waiting for operations to complete. It’s waiting for a network packet
to arrive, for the mouse to move, for the disk to finish writing some bytes, or
maybe even just for a read from main memory to complete. From the CPU’s
perspective, eons go by between most such events. When one does occur, the
CPU runs a few more instructions, then goes back to waiting again. Take a
look at your CPU utilization—it’s probably somewhere in the low single digits,
and that’s likely where it hovers the majority of the time.
Synchronous Interfaces
Synchronous interfaces allow your program (or rather, a single thread in
your program) to execute only a single operation at a time; each operation
has to wait for the previous synchronous operation to finish before it gets
to run. Most interfaces you see in the wild are synchronous: you call them,
they go do some stuff, and eventually they return when the operation has
completed and your program can continue from there. The reason for this,
as we’ll see later in this chapter, is that making an operation asynchronous
takes a fair bit of extra machinery. Unless you need the benefits of asynchrony,
sticking to the synchronous model requires much less pomp and
circumstance.
Synchronous interfaces hide all this waiting; the application calls a
function that says “write these bytes to this file,” and some time later, that
function completes and the next line of code executes. Behind the scenes,
what really happens is that the operating system queues up a write operation
to the disk and then puts the application to sleep until the disk reports
that it has finished the write. The application experiences this as the function
taking a long time to execute, but in reality it isn’t really executing at
all, just waiting.
An interface that performs operations sequentially in this way is also
often referred to as blocking, since the operation in the interface that has
to wait for some external event to happen in order for it to make progress
blocks further execution until that event happens. Whether you refer to an
interface as synchronous or blocking, the basic idea is the same: the application
does not move on until the current operation finishes. While the
operation is waiting, so is the application.
Synchronous interfaces are usually considered to be easy to work with
and simple to reason about, since your code executes just one line at a time.

Asynchronous Programming 119
But they also allow the application to do only one thing at a time. That
means if you want your program to wait for either user input or a network
packet, you’re out of luck unless your operating system provides an operation
specifically for that. Similarly, even if your application could do some
other useful work while the disk is writing a file, it doesn’t have that option
as the file write operation blocks the execution!
Multithreading
By far the most common solution to allowing concurrent execution is to use
multithreading. In a multithreaded program, each thread is responsible for
executing a particular independent sequence of blocking operations, and
the operating system multiplexes among the threads so that if any thread can
make progress, progress is made. If one thread blocks, some other thread
may still be runnable, and so the application can continue to do useful
work.
Usually, these threads communicate with each other using a synchronization
primitive like a lock or a channel so that the application can still coordinate
their efforts. For example, you might have one thread that waits for
user input, one thread that waits for network packets, and another thread
that waits for either of those threads to send a message on a channel shared
between all three threads.
Multithreading gives you concurrency—the ability to have multiple independent
operations that can be executed at any one time. It’s up to the system
running the application (in this case, the operating system) to choose
among the threads that aren’t blocked and decide which to execute next.
If one thread is blocked, it can choose to run another one that can make
progress instead.
Multithreading combined with blocking interfaces gets you quite far,
and large swaths of production-ready software are built in this way. But this
approach is not without its shortcomings. First, keeping track of all these
threads quickly gets cumbersome; if you have to spin up a thread for every
concurrent task, including simple ones like waiting for keyboard input, the
threads add up fast, and so does the additional complexity needed to keep
track of how all those threads interact, communicate, and coordinate.
Second, switching between threads gets costly the more of them there
are. Every time one thread stops running and another one starts back up
in its place, you need to do a round-trip to the operating system scheduler,
and that’s not free. On some platforms, spawning new threads is also a fairly
heavyweight process. Applications with high performance needs often mitigate
this cost by reusing threads and using operating system calls that allow
you to block on many related operations, but ultimately you are left with the
same problem: blocking interfaces require that you have as many threads as
the number of blocking calls you want to make.
Finally, threads introduce parallelism into your program. The distinction
between concurrency and parallelism is subtle, but important: concurrency
means that the execution of your tasks is interleaved, whereas
parallelism means that multiple tasks are executing at the same time. If you
have two tasks, their execution expressed in ASCII might look like _-_-_

120 Chapter 8
(concurrency) versus ===== (parallelism). Multithreading does not necessarily
imply parallelism—even though you have many threads, you might have
only a single core, so only one thread is executing at a given time—but the
two usually go hand in hand. You can make two threads mutually exclusive
in their execution by using a Mutex or other synchronization primitive, but
that introduces additional complexity—threads want to run in parallel.
And while parallelism is often a good thing—who doesn’t want their program
to run faster on more cores—it also means that your program must
handle truly simultaneous access to shared data structures. This means
moving from Rc, Cell, and RefCell to the more powerful but also slower Arc
and Mutex. While you may want to use the latter types in your concurrent
program to enable parallelism, threading forces you to use them. We’ll look
at multithreading in much greater detail in Chapter 10.
Asynchronous Interfaces
Now that we’ve explored synchronous interfaces, we can look at the alternative:
asynchronous or nonblocking interfaces. An asynchronous interface
is one that may not yield a result straightaway, and may instead indicate
that the result will be available at some later time. This gives the caller the
opportunity to do something else in the meantime rather than having to go
to sleep until that particular operation completes. In Rust parlance, an asynchronous
interface is a method that returns a Poll, as defined in Listing 8-1.
enum Poll<T> {
Ready(T),
Pending
}
Listing 8-1: The core of asynchrony: the “here you are or come back later” type
Poll usually shows up in the return type of functions whose names start
with poll—these are methods that signal they can attempt an operation
without blocking. We’ll get into how exactly they do that later in this chapter,
but in general they attempt to perform as much as they can of the operation
before they would normally block, and then return. And crucially,
they remember where they left off so that they can resume execution later
when additional progress can again be made.
These nonblocking functions allow us to easily perform multiple tasks
concurrently. For example, if you want to read from either the network or
the user’s keyboard, whichever has an event available first, all you have to
do is poll both in a loop until one of them returns Poll::Ready. No need for
any additional threads or synchronization!
The word loop here should make you a little nervous. You don’t want your
program to burn through a loop three billion times a second when it may
be minutes until the next input occurs. In the world of blocking interfaces,
this wasn’t a problem since the operating system simply put the thread to
sleep and then took care of waking it up when a relevant event occurred, but
how do we avoid burning cycles while waiting in this brave new nonblocking
world? That’s what much of the remainder of this chapter will be about.

Asynchronous Programming 121
Standardized Polling
To get to a world where every library can be used in a nonblocking fashion,
we could have every library author cook up their own poll methods, all with
slightly different names, signatures, and return types—but that would quickly
get unwieldy. Instead, in Rust, polling is standardized through the Future
trait. A simplified version of Future is shown in Listing 8-2 (we’ll get back to
the real one later in this chapter).
trait Future {
type Output;
fn poll(&mut self) -> Poll<Self::Output>;
}
Listing 8-2: A simplified view of the Future trait
Types that implement the Future trait are known as futures and represent
values that may not be available yet. A future could represent the next
time a network packet comes in, the next time the mouse cursor moves,
or just the point at which some amount of time has elapsed. You can read
Future<Output = Foo> as “a type that will produce a Foo in the future.” Types
like this are often referred to in other languages as promises—they promise
that they will eventually yield the indicated type. When a future eventually
returns Poll::Ready(T), we say that the future resolves into a T.
With this trait in place, we can generalize the pattern of providing poll
methods. Instead of having methods like poll_recv and poll_keypress, we can
have methods like recv and keypress that both return impl Future with an
appropriate Output type. This doesn’t change the fact that you have to poll
them—we’ll deal with that later—but it does mean that at least there is a
standardized interface to these kinds of pending values, and we don’t need
to use the poll_ prefix everywhere.
**NOTE** In general, you should not poll a future again after it has returned Poll::Ready. If
you do, the future is well within its rights to panic. A future that is safe to poll after it
has returned Ready is sometimes referred to as a fused future.
Ergonomic Futures
Writing a type that implements Future in the way I’ve described so far is
quite a pain. To see why, first take a look at the fairly straightforward asynchronous
code block in Listing 8-3 that simply tries to forward messages
from the input channel rx to the output channel tx.
async fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {
while let Some(t) = rx.next().await {
tx.send(t).await;
}
}
Listing 8-3: Implementing a channel-forwarding future using async and await

122 Chapter 8
This code, written using async and await syntax, looks very similar to its
equivalent synchronous code and is easy to read. We simply send each message
we receive in a loop until there are no more messages, and each await
point corresponds to a place where a synchronous variant might block. Now
think about if you instead had to express this code by manually implementing
the Future trait. Since each call to poll starts at the top of the function,
you’d need to package the necessary state to continue from the last place
the code yielded. The result is fairly grotesque, as Listing 8-4 demonstrates.
enum Forward<T> { 1
WaitingForReceive(ReceiveFuture<T>, Option<Sender<T>>),
WaitingForSend(SendFuture<T>, Option<Receiver<T>>),
}
impl<T> Future for Forward<T> {
type Output = (); 2
fn poll(&mut self) -> Poll<Self::Output> {
match self { 3
Forward::WaitingForReceive(recv, tx) => {
if let Poll::Ready((rx, v)) = recv.poll() {
if let Some(v) = v {
let tx = tx.take().unwrap(); 4
*self = Forward::WaitingForSend(tx.send(v), Some(rx)); 5
// Try to make progress on sending.
return self.poll(); 6
} else {
// No more items.
Poll::Ready(())
}
} else {
Poll::Pending
}
}
Forward::WaitingForSend(send, rx) => {
if let Poll::Ready(tx) = send.poll() {
let rx = rx.take().unwrap();
*self = Forward::WaitingForReceive(rx.receive(), Some(tx));
// Try to make progress on receiving.
return self.poll();
} else {
Poll::Pending
}
}
}
}
}
Listing 8-4: Manually implementing a channel-forwarding future
You’ll rarely have to write code like this in Rust anymore, but it gives
important insight into how things work under the hood, so let’s walk through
it. First, we define our future type as an enum 1, which we’ll use to keep track
of what we’re currently waiting on. This is a consequence of the fact that

Asynchronous Programming 123
when we return Poll::Pending, the next call to poll will start at the top of the
function again. We need some way to know what we were in the middle of so
that we know which operation to continue on. Furthermore, we need to keep
track of different information depending on what we’re doing: if we’re waiting
for a receive to finish, we need to keep that ReceiveFuture (the definition
of which is not shown in this example) so that we can poll it the next time we
are polled ourselves, and the same goes for SendFuture. The Options here might
strike you as weird too; we’ll get back to those shortly.
When we implement Future for Forward, we declare its output type as
() 2 because this future doesn’t actually return anything. Instead, the
future resolves (with no result) when it has finished forwarding everything
from the input channel to the output channel. In a more complete example,
the Output of our forwarding type might be a Result so that it could communicate
errors from receive() and send() back up the stack to the function
that’s polling for the completion of the forwarding. But this code is complicated
enough already, so we’ll leave that for another day.
When Forward is polled, it needs to resume wherever it last left off,
which we find out by matching on the enum variant currently held in
self 3. For whichever branch we go into, the first step is to poll the future
that blocks progress for the current operation; if we’re trying to receive, we
poll the ReceiveFuture, and if we’re trying to send, we poll the SendFuture. If
that call to poll returns Poll::Pending, then we can make no progress, and
we return Poll::Pending ourselves. But if the current future resolves, we
have work to do!
When one of the inner futures resolves, we need to update what the
current operation is by switching which enum variant is stored in self.
In order to do so, we have to move out of self to call Receiver::receive or
Sender::send—but we can’t do that because all we have is &mut self. So, we
store the state we have to move in an Option, which we move out of with
Option::take 4. This is silly since we’re about to overwrite self anyway 5,
and hence the Options will always be Some, but sometimes tricks are needed
to make the borrow checker happy.
Finally, if we do make progress, we then poll self again 6 so that if
we can immediately make progress on the pending send or receive, we do
so. This is actually necessary for correctness when implementing the real
Future trait, which we’ll get back to later, but for now think of this as an
optimization.
We just hand-wrote a state machine: a type that has a number of possible
states and moves between them in response to particular events. This was a
fairly simple state machine, at that. Imagine having to write code like this for
more complicated use cases where you have additional intermediate steps!
Beyond writing the unwieldy state machine, we have to know the types
of the futures that Sender::send and Receiver::receive return so that we can
store them in our type. If those methods instead returned impl Future, we’d
have no way to write out the types for our variants. The send and receive
methods also have to take ownership of the sender and the receiver; if they
did not, the lifetimes of the futures they returned would be tied to the

124 Chapter 8
borrow of self, which would end when we return from poll. But that would
not work, since we’re trying to store those futures in self.
**NOTE** You may have noticed that Receiver looks a lot like an asynchronous version of
Iterator. Others have noticed the same thing, and the standard library is on its way
to adding a trait specifically for types that can meaningfully implement poll_next.
Down the line, these asynchronous iterators (often referred to as streams) may end up
with first-class language support, such as the ability to loop over them directly!
Ultimately, this code is hard to write, hard to read, and hard to change. If
we wanted to add error handling, for example, the code complexity would
increase significantly. Luckily, there’s a better way!
async/await
Rust 1.39 gave us the async keyword and the closely related await postfix
operator, which we used in the original example in Listing 8-3. Together,
they provide a much more convenient mechanism for writing asynchronous
state machines like the one in Listing 8-5. Specifically, they let you write the
code in such a way that it doesn’t even look like a state machine!
async fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {
while let Some(t) = rx.next().await {
tx.send(t).await;
}
}
Listing 8-5: Implementing a channel-forwarding future using async and await, repeated
from Listing 8-3
If you don’t have much experience with async and await, the difference
between Listing 8-4 and Listing 8-5 might give you an idea of why the Rust
community was so excited to see them land. But since this is an intermediate
book, let’s dive a little deeper to understand just how this short segment
of code can replace the much longer manual implementation. To do that,
we first need to talk about generators—the mechanism by which async and
await are implemented.
Generators
Briefly described, a generator is a chunk of code with some extra compilergenerated
bits that enables it to stop, or yield, its execution midway through
and then resume from where it last yielded later on. Take the forward function
in Listing 8-3, for example. Imagine that it gets to the call to send, but
the channel is currently full. The function can’t make any more progress,
but it also cannot block (this is nonblocking code, after all), so it needs to
return. Now suppose the channel eventually clears and we want to proceed
with the send. If we call forward again from the top, it’ll call next again and
the item we previously tried to send will be lost, so that’s no good. Instead,
we turn forward into a generator.

Asynchronous Programming 125
Whenever the forward generator cannot make progress anymore, it
needs to store its current state somewhere so that when its execution eventually
resumes, it resumes in the right place with the right state. It saves the
state through an associated data structure that’s generated by the compiler,
which contains all the state of the generator at a given point in time. A
method on that data structure (also generated) then allows the generator
to resume from its current state, stored in &mut self, and updates the state
again when the generator again cannot make progress.
This “return but allow me to resume later” operation is called yielding,
which effectively means it returns while keeping some extra state on the
side. When we later want to resume a call to forward, we invoke the known
entry point into the generator (the resume method, which is poll for async
generators), and the generator inspects the previously stored state in self
to decide what to do next. This is exactly the same thing we did manually in
Listing 8-4! In other words, the code in Listing 8-5 loosely desugars to the
hypothetical code shown in Listing 8-6.
generator fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {
loop {
let mut f = rx.next();
let r = if let Poll::Ready(r) = f.poll() { r } else { yield };
if let Some(t) = r {
let mut f = tx.send(t);
let _ = if let Poll::Ready(r) = f.poll() { r } else { yield };
} else { break Poll::Ready(()); }
}
}
Listing 8-6: Desugaring async/await into a generator
At the time of writing, generators are not actually usable in Rust—they
are only used internally by the compiler to implement async/await—but that
may change in the future. Generators come in handy in a number of cases,
such as to implement iterators without having to carry around a struct or to
implement an impl Iterator that figures out how to yield items one at a time.
If you look closely at Listings 8-5 and 8-6, they may seem a little magical
once you know that every await or yield is really a return from the function.
After all, there are several local variables in the function, and it’s not clear
how they’re restored when we resume later on. This is where the compilergenerated
part of generators comes into play. The compiler transparently
injects code to persist those variables into and read them from the generator’s
associated data structure, rather than the stack, at the time of execution.
So if you declare, write to, or read from some local variable a, you are
really operating on something akin to self.a. Problem solved! It’s all really
quite marvelous.
One subtle but important difference between the manual forward implementation
and the async/await version is that the latter can hold references
across yield points. This enables functions like Receiver::next and Sender::send
in Listing 8-5 to take &mut self rather than the self they took in Listing 8-4.
If we tried to use a &mut self receiver for these methods in the manual state

126 Chapter 8
machine implementation, the borrow checker would have no way to enforce
that the Receiver stored inside Forward cannot be referenced between when
Receiver::next is called and when the future it returns resolves, and so it would
reject the code. Only by moving the Receiver into the future can we convince
the compiler that the Receiver is not otherwise accessible. Meanwhile, with
async/await, the borrow checker can inspect the code before the compiler
turns it into a state machine and verify that rx is indeed not accessed again
until after the future is dropped, when the await on it returns.
THE SIZE OF GENERATORS
The data structure used to back a generator’s state must be able to hold the combined
state at any one yield point. If your async fn contains, say, a [u8; 8192],
those 8KiB must be stored in the generator itself. Even if your async fn contains
only smaller local variables, it must also contain any future that it awaits, since it
needs to be able to poll such a future later, when poll is invoked.
This nesting means that generators, and thus futures based on async
functions and blocks, can get quite large without any visible indicator of that
increased size in your code. This can in turn impact your program’s runtime
performance, since those giant generators may have to be copied across function
calls and in and out of data structures, which amounts to a fair amount of
memory copying. In fact, you can usually identify when the size of your generator-
based futures is affecting performance by looking for excessive amounts of
time spent in the memcpy function in your application’s performance profiles!
Finding these large futures isn’t always easy, however, and often requires
manually identifying long or complex chains of async functions. Clippy may
be able to help with this in the future, but at the time of writing, you’re on your
own. When you do find a particularly large future, you have two options: you
can try to reduce the amount of local state the async functions need, or you
can move the future to the heap (with Box::pin) so that moving the future just
requires moving the pointer to it. The latter is by far the easiest way to go, but
it also introduces an extra allocation and a pointer indirection. Your best bet is
usually to put the problematic future on the heap, measure your performance,
and then use your performance benchmarks to guide you from there.
Pin and Unpin
We’re not quite done. While generators are neat, a challenge arises from
the technique as I’ve described it so far. In particular, it’s not clear what
happens if the code in the generator (or, equivalently, the async block) takes
a reference to a local variable. In the code from Listing 8-5, the future that
rx.next() returns must necessarily hold a reference to rx if a next message
is not immediately available so that it knows where to try again when the
generator next resumes. When the generator yields, the future and the reference
the future contains get stashed away inside the generator. But what

Asynchronous Programming 127
now happens if the generator is moved? Specifically, look at the code in
Listing 8-7, which calls forward.
async fn try_forward<T>(rx: Receiver<T>, tx: Sender<T>) -> Option<impl Future>
{
let mut f = forward(rx, tx);
if f.poll().is_pending() { Some(f) } else { None }
}
Listing 8-7: Moving a future after polling it
The try_forward function polls forward only once, to forward as many
messages as possible without blocking. If the receiver may still produce more
messages (that is, if it returned Poll::Pending instead of Poll::Ready(None)),
those messages are deferred to be forwarded at some later time by returning
the forwarding future to the caller, which may choose to poll again at a time
when it sees fit.
Let’s work through what happens here with what we know about async
and await so far. When we poll the forward generator, it goes through the
while loop some unknown number of times and eventually returns either
Poll::Ready(()) if the receiver ended, or Poll::Pending otherwise. If it returns
Poll::Pending, the generator contains a future returned from either rx.next()
or tx.send(t). Those futures both contain a reference to one of the arguments
initially provided to forward (rx and tx, respectively), which must also
be stored in the generator. But when try_forward returns the entire generator,
the fields of the generator also move. Thus, rx and tx no longer reside
at the same locations in memory, and the references stored in the stashedaway
future are no longer pointing to the right data!
What we’ve run into here is a case of a self-referential data structure: one
that holds both data and references to that data. With generators, these selfreferential
structures are very easy to construct, and being unable to support
them would be a significant blow to ergonomics because it would mean you
wouldn’t be able to hold references across any yield point. The (ingenious)
solution for supporting self-referential data structures in Rust comes in the
form of the Pin type and the Unpin trait. Very briefly, Pin is a wrapper type that
prevents the wrapped type from being (safely) moved, and Unpin is a marker
trait that says the implementing type can be removed safely from a Pin.
Pin
There’s a lot of nuance to cover here, so let’s start with a concrete use of
the Pin wrapper. Listing 8-2 gave you a simplified version of the Future trait,
but we’re now ready to peel back one part of the simplification. Listing 8-8
shows the Future trait somewhat closer to its final form.
trait Future {
type Output;
fn poll(self: Pin<&mut Self>) -> Poll<Self::Output>;
}
Listing 8-8: A less simplified view of the Future trait with Pin

128 Chapter 8
In particular, this definition requires that you call poll on Pin<&mut Self>.
Once you have a value behind a Pin, that constitutes a contract that that value
will never move again. This means that you can construct self-references
internally to your heart’s delight, exactly as you want for generators.
**NOTE** While Future makes use of Pin, Pin is not tied to the Future trait—you can use Pin for
any self-referential data structure.
But how do you get a Pin to call poll? And how can Pin ensure that
the contained value won’t move? To see how this magic works, let’s look
at the definition of std::pin::Pin and some of its key methods, shown in
Listing 8-9.
struct Pin<P> { pointer: P }
impl<P> Pin<P> where P: Deref {
pub unsafe fn new_unchecked(pointer: P) -> Self;
}
impl<'a, T> Pin<&'a mut T> {
pub unsafe fn get_unchecked_mut(self) -> &'a mut T;
}
impl<P> Deref for Pin<P> where P: Deref {
type Target = P::Target;
fn deref(&self) -> &Self::Target;
}
Listing 8-9: std::pin::Pin and its key methods
There’s a lot to unpack here, and we’re going to have to go over the
definition in Listing 8-9 a few times before all the bits make sense, so please
bear with me.
First, you’ll notice that Pin holds a pointer type. That is, rather than hold
some T directly, it holds a type P that dereferences through Deref into T. This
means that rather than have a Pin<MyType>, you’ll have a Pin<Box<MyType>> or
Pin<Rc<MyType>> or Pin<&mut MyType>. The reason for this design is simple—
Pin’s primary goal is to make sure that once you place a T behind a Pin, that
T won’t move, as doing so might invalidate self-references stored in the T. If
the Pin just held a T directly, then simply moving the Pin would be enough to
invalidate that invariant! In the remainder of this section, I’ll refer to P as
the pointer type and T as the target type.
Next, notice that Pin’s constructor, new_unchecked, is unsafe. This is
because the compiler has no way to actually check that the pointer type
indeed promises that the pointed-to (target) type won’t move again. Consider,
for example, a variable foo on the stack. If Pin’s constructor were safe,
we could do Pin::new(&mut foo), call a method that requires Pin<&mut Self>
(and thus assumes that Self won’t move again), and then drop the Pin. At
this point, we could modify foo as much as we liked, since it is no longer
borrowed—including moving it! We could then pin it again and call the
same method, which would be none the wiser that any self-referential pointers
it may have constructed the first time around would now be invalid.

Asynchronous Programming 129
PIN CONSTRUCTOR SAFETY
The other reason the constructor for Pin is unsafe is that its safety depends on
the implementation of traits that are themselves safe. For example, the way
that Pin<P> implements get_unchecked_mut is to use the implementation of
DerefMut::deref_mut for P. While the call to get_unchecked_mut is unsafe, the
impl DerefMut for P is not. Yet it receives a &mut self, and can thus freely (and
without unsafe code) move the T. The same thing applies to Drop. The safety
requirement for Pin::new_unchecked is therefore not only that the pointer type
will not let the target type be moved again (like in the Pin<&mut T> example),
but also that its Deref, DerefMut, and Drop implementations do not move the
pointed-to value behind the &mut self they receive.
We then get to the get_unchecked_mut method, which gives you a mutable
reference to the T behind the Pin’s pointer type. This method is also unsafe,
because once we give out a &mut T, the caller has to promise it won’t use
that &mut T to move the T or otherwise invalidate its memory, lest any selfreferences
be invalidated. If this method weren’t unsafe, a caller could
call a method that takes Pin<&mut Self> and then call the safe variant of
get_unchecked_mut on two Pin<&mut _>s, then use mem::swap to swap the values
behind the Pin. If we were to then call a method that takes Pin<&mut Self>
again on either Pin, its assumption that the Self hasn’t moved would be violated,
and any internal references it stored would be invalid!
Perhaps surprisingly, Pin<P> always implements Deref<Target = T>, and
that is entirely safe. The reason for this is that a &T does not let you move T
without writing other unsafe code (UnsafeCell, for example, as we’ll discuss
in Chapter 9). This is a good example of why the scope of an unsafe block
extends beyond just the code it contains. If you wrote some code in one part
of the application that (unsafely) replaced a T behind an & using UnsafeCell,
then it could be that that &T initially came from a Pin<&mut T>, and that you
have now violated the invariant that the T behind the Pin may never move,
even though the place where you unsafely replaced the &T did not even mention
Pin!
**NOTE** If you’ve browsed through the Pin documentation while reading this chapter, you may
have noticed Pin::set, which takes a &mut self and a <P as Deref>::Target and
safely changes the value behind the Pin. This is possible because set does not return
the value that was previously pinned—it simply drops it in place and stores the new
value there instead. Therefore, it does not violate the pinning invariants: the old
value was never accessed outside of a Pin after it was placed there.
Unpin: The Key to Safe Pinning
At this point you might ask: given that getting a mutable reference is unsafe
anyway, why not have Pin hold a T directly? That is, rather than require an

130 Chapter 8
indirection through a pointer type, you could instead make the contract
for get_unchecked_mut that it is only safe to call if you haven’t moved the Pin.
The answer to that question lies in a neat safe use of Pin that the pointer
design enables. Recall that the whole reason we want Pin in the first place is
so we can have target types that may contain references to themselves (like
a generator) and give their methods a guarantee that the target type hasn’t
moved and thus that internal self-references remain valid. Pin lets us use the
type system to enforce that guarantee, which is great. But unfortunately,
with the design so far, Pin is very unwieldy to work with. This is because it
always requires unsafe code, even if you are working with a target type that
doesn’t contain any self-references, and so doesn’t care whether it’s been
moved or not.
This is where the marker trait Unpin comes into play. An implementation
of Unpin for a type simply asserts that the type is safe to move out of a Pin
when used as a target type. That is, the type promises that it will never use
any of Pin’s guarantees about the referent not moving again when used as a
target type, and thus those guarantees may be broken. Unpin is an auto-trait,
like Send and Sync, and so is auto-implemented by the compiler for any type
that contains only Unpin members. Only types that explicitly opt out of Unpin
(like generators) and types that contain those types are !Unpin.
For target types that are Unpin, we can provide a much simpler safe
interface to Pin, as shown in Listing 8-10.
impl<P> Pin<P> where P: Deref, P::Target: Unpin {
pub fn new(pointer: P) -> Self;
}
impl<P> DerefMut for Pin<P> where P: DerefMut, P::Target: Unpin {
fn deref_mut(&mut self) -> &mut Self::Target;
}
Listing 8-10: The safe API to Pin for Unpin target types
To make sense of the safe API in Listing 8-10, think about the safety
requirements of the unsafe methods from Listing 8-9: the function
Pin::new_unchecked is unsafe because the caller must promise that the referent
cannot be moved outside of the Pin, and that the implementations
of Deref, DerefMut, and Drop for the pointer type do not move the referent
through the reference they receive. Those requirements are there to
ensure that once we give out a Pin to a T, we never move that T again. But
if the T is Unpin, it has declared that it does not care if it is moved even if it
was previously pinned, so it’s fine if the caller does not satisfy any of those
requirements!
Similarly, get_unchecked_mut is unsafe because the caller must guarantee
that it doesn’t move the T out of the &mut T—but with T: Unpin, T has declared
that it’s fine being moved even after being pinned, so that safety requirement
is no longer important. This means that for Pin<P> where P::Target:
Unpin, we can simply provide safe variants of both those methods (DerefMut
being the safe version of get_unchecked_mut). In fact, we can even provide a
Pin::into_inner that simply gives back the owned P if the target type is Unpin,
since the Pin is essentially irrelevant!

Asynchronous Programming 131
Ways of Obtaining a Pin
With our new understanding of Pin and Unpin, we can now make progress
toward using the new Future definition from Listing 8-8 that requires
Pin<&mut Self>. The first step is to construct the required type. If the future
type is Unpin, that step is easy—we just use Pin::new(&mut future). If it is not
Unpin, we can pin the future in one of two main ways: by pinning to the
heap or pinning to the stack.
Let’s start with pinning to the heap. The primary contract of Pin is that
once something has been pinned, it cannot move. The pinning API takes
care of honoring that contract for all methods and traits on Pin, so the main
role of any function that constructs a Pin is to ensure that if the Pin itself
moves, the referent value does not move too. The easiest way to ensure that is
to place the referent on the heap, and then place just a pointer to the referent
in the Pin. You can then move the Pin to your heart’s delight, but the target
will remain where it was. This is the rationale behind the (safe) method
Box::pin, which takes a T and returns a Pin<Box<T>>. There’s no magic to it; it
simply asserts that Box follows the Pin constructor, Deref, and Drop contracts.
UNPIN BOX
While we’re on the topic of Box, take a look at the implementation of Unpin for
Box. The Box type unconditionally implements Unpin for any T, even if that T is
not Unpin. This might strike you as odd, given the earlier assertion that Unpin
is an auto-trait that is generally implemented for a type only if all of the type’s
members are also Unpin. Box is an exception to this for the same reason that it
can provide a safe Pin constructor: if you move a Box<T>, you do not move the
T. In other words, the unconditional implementation asserts that you can move a
Box<T> out of a Pin even if T cannot be moved out of a Pin. Note, however, that
this does not enable you to move a T that is !Unpin out of a Pin<Box<T>>.
The other option, pinning to the stack, is a little more involved, and at
the time of writing requires a smidgen of unsafe code. We have to ensure
that the pinned value cannot be accessed after the Pin with a &mut to it has
been dropped. We accomplish that by shadowing the value as shown in the
macro in Listing 8-11 or by using one of the crates that provide exactly this
macro. One day it may even make it into the standard library!
macro_rules! pin_mut {
($var:ident) => {
let mut $var = $var;
let mut $var = unsafe { Pin::new_unchecked(&mut $var) };
}
}
Listing 8-11: Macro for pinning to the stack

132 Chapter 8
By taking the name of the variable to pin to the stack, the macro
ensures that the caller has the value it wants to pin somewhere on the
stack already. The shadowing of $var ensures that the caller cannot drop
the Pin and continue to use the unpinned value (which would breach the
Pin contract
for any target type that’s !Unpin). By moving the value stored
in $var, the macro also ensures that the caller cannot drop the $var binding
the macro declarations without also dropping the original variable.
Specifically, without that line, the caller could write (note the extra scope):
let foo = /**/; { pin_mut!(foo); foo.poll() }; foo.mut_self_method();
Here, we give a pinned instance of foo to poll, but then we later use a
&mut to foo without a Pin, which violates the Pin contract. With the extra reassignment,
on the other hand, that code would also move foo into the new
scope, rendering it unusable after the scope ends.
Pinning on the stack therefore requires unsafe code, unlike Box::pin,
but avoids the extra allocation that Box introduces and also works in no_std
environments.
Back to the Future
We now have our pinned future, and we know what that means. But you
may have noticed that none of this important pinning stuff shows up in
most asynchronous code you write with async and await. And that’s because
the compiler hides it from you.
Think back to when we discussed Listing 8-5, when I told you that
<expr>.await desugars into something like:
loop { if let Poll::Ready(r) = expr.poll() { break r } else { yield } }
That was an ever-so-slight simplification because, as we’ve seen, you can
call Future::poll only if you have a Pin<&mut Self> for the future. The desugaring
is actually a bit more sophisticated, as shown in Listing 8-12.
1 match expr {
mut pinned => loop {
2 match unsafe { Pin::new_unchecked(&mut pinned) }.poll() {
Poll::Ready(r) => break r,
Poll::Pending => yield,
}
}
}
Listing 8-12: A more accurate desugaring of <expr>.await
The match 1 is a neat shorthand to not only ensure that the expansion
remains a valid expression, but also move the expression result into
a variable that we can then pin on the stack. Beyond that, the main new
addition is the call to Pin::new_unchecked 2. That call is safe because for the
containing async block to be polled, it must already be pinned due to the
signature of Future::poll. And the async block was polled for us to reach

Asynchronous Programming 133
the call to Pin::new_unchecked, so the generator state is pinned. Since pinned
is stored in the generator that corresponds to the async block (it must be so
that yield will resume correctly), we know that pinned will not move again.
Furthermore, pinned is not accessible except through a Pin once we’re in the
loop, so no code is able to move out of the value in pinned. Thus, we meet all
the safety requirements of Pin::new_unchecked, and the code is safe.
Going to Sleep
We went pretty deep into the weeds with Pin, but now that we’re out the
other side, there is another issue around futures that may have been making
your brain itch. If a call to Future::poll returns Poll::Pending, you need
something to call poll again at a later time to check whether you can make
progress yet. That something is usually called the executor. Your executor
could be a simple loop that polls all the futures you are waiting on until
they’ve all returned Poll::Ready, but that would burn a lot of CPU cycles you
could probably have used for other, more useful things, like running your
web browser. Instead, we want the executor to do whatever useful work it
can do, and then go to sleep. It should stay asleep until one of the futures
can make progress, and only then wake up to do another pass, before going
to sleep again.
Waking Up
The condition that determines when to check back with a given future varies
widely. It might be “when a network packet arrives on this port,” “when
the mouse cursor moves,” “when someone sends on this channel,” “when
the CPU receives a particular interrupt,” or even “after this much time has
passed.” On top of that, developers can write their own futures that wrap
multiple other futures, and thus, they may have several wake-up conditions.
Some futures may even introduce their own entirely custom wake events.
To accommodate these many use cases, Rust introduces the notion of
a Waker: a way to wake the executor to signal that progress can be made. The
Waker is what makes the whole machinery around futures work. The executor
constructs a Waker that integrates with the mechanism the executor uses to
go to sleep, and passes the Waker in to any Future it polls. How? With the additional
parameter to Future::poll that I’ve hidden from you so far. Sorry about
that. Listing 8-13 gives the final and true definition for Future—no more lies!
trait Future {
type Output;
fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output>;
}
Listing 8-13: The actual Future trait with Context
The &mut Context contains the Waker. The argument is a Context, not a
Waker directly, so that we can augment the asynchronous ecosystem with
additional context for futures should that be deemed necessary.

134 Chapter 8
The primary method on Waker is wake (and the by-reference variant wake
_by_ref), which should be called when the future can again make progress.
The wake method takes no arguments, and its effects are entirely defined
by the executor that constructed the Waker. You see, Waker is secretly generic
over the executor. Or, more precisely, whatever constructed the Waker gets to
dictate what happens when Waker::wake is called, when a Waker is cloned, and
when a Waker is dropped. This all happens through a manually implemented
vtable, which functions similarly to the dynamic dispatch we discussed way
back in Chapter 2.
It’s a somewhat involved process to construct a Waker, and the mechanics
of it aren’t all that important for using one, but you can see the building
blocks in the RawWakerVTable type in the standard library. It has a constructor
that takes the function pointers for wake and wake_by_ref as well as Clone and
Drop. The RawWakerVTable, which is usually shared among all of an executor’s
wakers, is bundled up with a raw pointer intended to hold data specific to
each Waker instance (like which future it’s for) and is turned into a RawWaker.
That is in turn passed to Waker::from_raw to produce a safe Waker that can be
passed to Future::poll.
Fulfilling the Poll Contract
So far we’ve skirted around what a future actually does with a Waker. The
idea is fairly simple: if Future::poll returns Poll::Pending, it is the future’s
responsibility to ensure that something calls wake on the provided Waker
when the future is next able to make progress. Most futures uphold this
property by returning Poll::Pending only if some other future also returned
Poll::Pending; in this way, it trivially fulfills the contract of poll since the
inner future must follow that same contract. But there can’t be turtles all
the way down. At some point, you reach a future that does not poll other
futures but instead does something like write to a network socket or attempt
to receive on a channel. These are commonly referred to as leaf futures since
they have no children. A leaf future has no inner future but instead directly
represents some resource that may not yet be ready to return a result.
**NOTE** The poll contract is the reason why the recursive poll call 6 back in Listing 8-4 is
necessary for correctness.
Leaf futures typically come in one of two shapes: those that wait for
events that originate within the same process (like a channel receiver), and
those that wait for events external to the process (like a TCP packet read).
Those that wait for internal events all tend to follow the same pattern: store
the Waker where the code that will wake you up can find it, and have that
code call wake on the Waker when it generates the relevant event. For example,
consider a leaf future that has to wait for a message on an in-memory channel.
It stores its Waker inside the part of the channel that is shared between
the sender and the receiver and then returns Poll::Pending. When a sender
later comes along and injects a message into the channel, it notices the Waker
left there by the waiting receiver and calls wake on the Waker before returning
from send. Now the receiver is awoken, and the poll contract is upheld.

Asynchronous Programming 135
Leaf futures that deal with external events are more involved, as the
code that generates the event they’re waiting for knows nothing of futures
or wakers. Most often the generating code is the operating system kernel,
which knows when a disk is ready or a timer expires, but it could also be
a C library that invokes a callback into Rust when an operation completes
or some other such external entity. A leaf future wrapping an external
resource like this could spin up a thread that executes a blocking system
call (or waits for the C callback) and then use the internal waking mechanism,
but that would be wasteful; you would spin up a thread every time
an operation had to wait and be left with lots of single-use threads sitting
around waiting for things.
Instead, executors tend to provide implementations of leaf futures
that communicate behind the scenes with the executor to arrange for
the appropriate interaction with the operating system. How exactly this
is orchestrated depends on the executor and the operating system, but
roughly speaking the executor keeps track of all the event sources that it
should listen for the next time it goes to sleep. When a leaf future realizes
it must wait for an external event, it updates that executor’s state (which it
knows about since it’s provided by the executor crate) to include that external
event source alongside its Waker. When the executor can no longer make
progress, it gathers all of the event sources the various pending leaf futures
are waiting for and does a big blocking call to the operating system, telling
it to return when any of the resources the leaf futures are waiting on have
a new event. On Linux, this is usually achieved with the epoll system call;
Windows, the BSDs, macOS, and pretty much every other operating system
provide similar mechanisms. When that call returns, the executor calls wake
on all the wakers associated with event sources that the operating system
reported events for, and thus the poll contract is fulfilled.
**NOTE** A reactor is the part of an executor that leaf futures register event sources with and
that the executor waits on when it has no more useful work to do. It is possible to
separate the executor and the reactor, though bundling them together often improves
performance as the two can be co-optimized more readily.
A knock-on effect of the tight integration between leaf futures and
the executor is that leaf futures from one executor crate often cannot be
used with a different executor. Or at least, they cannot be used unless the
leaf future’s executor is also running. When the leaf future goes to store
its Waker and register the event source it’s waiting for, the executor it was
built against needs to have that state set up and needs to be running so
that the event source will actually be monitored and wake eventually called.
There are ways around this, such as having leaf futures spawn an executor
if one is not already running, but this is not always advisable as it means
that an application can transparently end up with multiple executors running
at the same time, which can reduce performance and mean you must
inspect the state of multiple executors when debugging.
Library crates that wish to support multiple executors have to be generic
over their leaf resources. For example, instead of using a particular executor’s

136 Chapter 8
TcpStream or File future type, a library can store a generic T: AsyncRead +
AsyncWrite. However, the ecosystem has yet to settle on exactly what these traits
should look like and which traits are needed, so for the moment it’s fairly difficult
to make code truly generic over the executor. For example, while AsyncRead
and AsyncWrite are somewhat common across the ecosystem (or can be easily
adapted if necessary), no traits currently exist for running a future in the
background (spawning, which we’ll discuss later) or for representing a timer.
Waking Is a Misnomer
You may already have realized that Waker::wake doesn’t necessarily seem to
wake anything. For example, for external events (as described in the previous
section), the executor is already awake, and it might seem silly for it to
then call wake on a Waker that belongs to that executor anyway! The reality is
that Waker::wake is a bit of a misnomer—in reality, it signals that a particular
future is runnable. That is, it tells the executor that it should make sure to
poll this particular future when it gets around to it rather than go to sleep
again, since this future can make progress. This might wake the executor if
it is currently sleeping so it will go poll that future, but that’s more of a side
effect than its primary purpose.
It is important for the executor to know which futures are runnable
for two reasons. First, it needs to know when it can stop polling a future
and go to sleep; it’s not sufficient to just poll each future until it returns
Poll::Pending, since polling a later future might make it possible to progress
an earlier future. Consider the case where two futures bounce messages
back and forth on channels to one another. When you poll one, the other
becomes ready, and vice versa. In this case, the executor should never go to
sleep, as there is always more work to do.
Second, knowing which futures are runnable lets the executor avoid
polling futures unnecessarily. If an executor manages thousands of pending
futures, it shouldn’t poll all of them just because an event made one of them
runnable. If it did, executing asynchronous code would get very slow indeed.
Tasks and Subexecutors
The futures in an asynchronous program form a tree: a future may contain
any number of other futures, which in turn may contain other futures, all the
way down to the leaf futures that interact with wakers. The root of each tree is
the future you give to whatever the executor’s main “run” function is. These
root futures are called tasks, and they are the only point of contact between
the executor and the futures tree. The executor calls poll on the task, and
from that point forward the code of each contained future must figure out
which inner future(s) to poll in response, all the way down to the relevant leaf.
Executors generally construct a separate Waker for each task they poll so
that when wake is later called, they know which task was just made runnable
and can mark it as such. That is what the raw pointer in RawWaker is for—to differentiate
between tasks while sharing the code for the various Waker methods.
When the executor eventually polls a task, that task starts running from
the top of its implementation of Future::poll and must decide from there how

Asynchronous Programming 137
to get to the future deeper down that can now make progress. Since each
future knows only about its own fields, and nothing about the whole tree, this
all happens through calls to poll that each traverse one edge in the tree.
The choice of which inner future to poll is often obvious, but not always.
In the case of async/await, the future to poll is the one we’re blocked waiting
for. But in a future that waits for the first of several futures to make progress
(often called a select), or for all of a set of futures (often called a join),
there are many options. A future that has to make such a choice is basically
a subexecutor. It could poll all of its inner futures, but doing so could be
quite wasteful. Instead, these subexecutors often wrap the Waker they receive
in poll’s Context with their own Waker type before they invoke poll on any
inner future. In the wrapping code, they mark the future they just polled as
runnable in their own state before they call wake on the original Waker. That
way, when the executor eventually polls the subexecutor future again, the
subexecutor can consult its own internal state to figure out which of its inner
futures caused the current call to poll, and then only poll those.
BLOCKING IN ASYNC CODE
You must be careful about calling synchronous code from asynchronous code,
since any time an executor thread spends executing the current task is time it’s
not spending running other tasks. If a task occupies the current thread for a
prolonged period of time without yielding back to the executor, which might
happen when executing a blocking system call (like std::sync::sleep), running
a subexecutor that doesn’t yield occasionally, or running in a tight loop with no
awaits, then other tasks the current executor thread is responsible for won’t get
to run during that time. Usually, this manifests as long delays between when
certain tasks can make progress (such as when a client connects) and when
they actually get to execute.
Some multithreaded executors implement work-stealing techniques, where
idle executor threads steal tasks from busy executor threads, but this is more of
a mitigation than a solution. Ultimately, you could end up in a situation where
all the executor threads are blocked, and thus no tasks get run until one of the
blocking operations completes.
In general, you should be very careful with executing compute-intensive
operations or calling functions that could block in an asynchronous context.
Such operations should either be converted to asynchronous operations where
possible or executed on dedicated threads that then communicate using a
primitive that does support asynchrony, like a channel. Some executors also
provide mechanisms for indicating that a particular segment of asynchronous
code might block or for yielding voluntarily in the context of loops that might
otherwise not yield, which can compose part of the solution. A good rule of
thumb is that no future should be able to run for more than 1 ms without returning
Poll::Pending.

138 Chapter 8
Tying It All Together with spawn
When working with asynchronous executors, you may come across an
operation that spawns a future. We’re now in a position to explore what
that means! Let’s do so by way of example. First, consider the simple server
implementation in Listing 8-14.
async fn handle_client(socket: TcpStream) -> Result<()> {
// Interact with the client over the given socket.
}
async fn server(socket: TcpListener) -> Result<()> {
while let Some(stream) = socket.accept().await? {
handle_client(stream).await?;
}
}
Listing 8-14: Handling connections sequentially
The top-level server function is essentially one big future that listens
for new connections and does something when a new connection arrives.
You hand that future to the executor and say “run this,” and since you don’t
want your program to then exit immediately, you’ll probably have the executor
block on that future. That is, the call to the executor to run the server
future will not return until the server future resolves, which may be never
(another client could always arrive later).
Now, every time a new client connection comes in, the code in
Listing 8-14 makes a new future (by calling handle_client) to handle that
connection. Since the handling is itself a future, we await it and then move
on to the next client connection.
The downside of this approach is that we only ever handle one connection
at a time—there is no concurrency. Once the server accepts a connection,
the handle_client function is called, and since we await it, we don’t go
around the loop again until handle_client’s return future resolves (presumably
when that client has left).
We could improve on this by keeping a set of all the client futures and
having the loop in which the server accepts new connections also check all
the client futures to see if any can make progress. Listing 8-15 shows what
that might look like.
async fn server(socket: TcpListener) -> Result<()> {
let mut clients = Vec::new();
loop {
poll_client_futures(&mut clients)?;
if let Some(stream) = socket.try_accept()? {
clients.push(handle_client(stream));
}
}
}
Listing 8-15: Handling connections with a manual executor

Asynchronous Programming 139
This at least handles many connections concurrently, but it’s quite
convoluted. It’s also not very efficient because the code now busy-loops,
switching between handling the connections we already have and accepting
new ones. And it has to check each connection each time, since it won’t
know which ones can make progress (if any). It also can’t await at any point,
since that would prevent the other futures from making progress. You could
implement your own wakers to ensure that the code polls only the futures
that can make progress, but ultimately this is going down the path of developing
your own mini-executor.
Another downside of sticking with just the one task for the server that
internally contains the futures for all of the client connections is that the
server ends up being single-threaded. There is just the one task and to poll
it the code must hold an exclusive reference to the task’s future (poll takes
Pin<&mut Self>), which only one thread can hold at a time.
The solution is to make each client future its own task and leave it to
the executor to multiplex among all the tasks. Which, you guessed it, you
do by spawning the future. The executor will continue to block on the
server future, but if it cannot make progress on that future, it will use its
execution machinery to make progress on the other tasks in the meantime
behind the scenes. And best of all, if the executor is multithreaded and
your client futures are Send, it can run them in parallel since it can hold
&muts to the separate tasks concurrently. Listing 8-16 gives an example of
what this might look like.
async fn server(socket: TcpListener) -> Result<()> {
while let Some(stream) = socket.accept().await? {
// Spawn a new task with the Future that represents this client.
// The current task will continue to just poll for more connections
// and will run concurrently (and possibly in parallel) with handle_client.
spawn(handle_client(stream));
}
}
Listing 8-16: Spawning futures to create more tasks that can be polled concurrently
When you spawn a future and thus make it a task, it’s sort of like spawning
a thread. The future continues running in the background and is multiplexed
concurrently with any other tasks given to the executor. However,
unlike a spawned thread, spawned tasks still depend on being polled by
the executor. If the executor stops running, either because you drop it or
because your code no longer runs the executor’s code, those spawned tasks
will stop making progress. In the server example, imagine what will happen
if the main server future resolves for some reason. Since the executor
has returned control back to your code, it cannot continue doing, well,
anything. Multi-threaded executors often spawn background threads that
continue to poll tasks even if the executor yields control back to the user’s
code, but not all executors do this, so check your executor before you rely
on that behavior!

140 Chapter 8
Summary
In this chapter, we’ve taken a look behind the scenes of the asynchronous
constructs available in Rust. We’ve seen how the compiler implements generators
and self-referential types, and why that work was necessary to support
what we now know as async/await. We’ve also explored how futures are
executed, and how wakers allow executors to multiplex among tasks when
only some of them can make progress at any given moment. In the next
chapter, we’ll tackle what is perhaps the deepest and most discussed area
of Rust: unsafe code. Take a deep breath, and then turn the page.

### 9.UNSAFE CODE

The mere mention of unsafe code often
elicits strong responses from many in the
Rust community, and from many of those
watching Rust from the sidelines. While some
maintain it’s “no big deal,” others decry it as “the reason
all of Rust’s promises are a lie.” In this chapter,
I hope to pull back the curtain a bit to explain what unsafe is, what it isn’t,
and how you should go about using it safely. At the time of writing, and
likely also when you read this, Rust’s precise requirements for unsafe code
are still being determined, and even if they were all nailed down, the complete
description would be beyond the scope of this book. Instead, I’ll do
my best to arm you with the building blocks, intuition, and tooling you’ll
need to navigate your way through most unsafe code.
Your main takeaway from this chapter should be this: unsafe code is the
mechanism Rust gives developers for taking advantage of invariants that,
for whatever reason, the compiler cannot check. We’ll look at the ways in
which unsafe does that, what those invariants may be, and what we can do
with it as a result.
142 Chapter 9
INVARIANTS
Throughout this chapter, I’ll be talking a lot about invariants. Invariant is just a
fancy way of saying “something that must be true for your program to be correct.”
For example, in Rust, one invariant is that references, using & and &mut,
do not dangle—they always point to valid values. You can also have application-
or library-specific invariants, like “the head pointer is always ahead of the
tail pointer” or “the capacity is always a power of two.” Ultimately, invariants
represent all the assumptions required for your code to be correct. However,
you may not always be aware of all the invariants that your code uses, and
that’s where bugs creep in.
Crucially, unsafe code is not a way to skirt the various rules of Rust, like
borrow checking, but rather a way to enforce those rules using reasoning
that is beyond the compiler. When you write unsafe code, the onus is on
you to ensure that the resulting code is safe. In a way, unsafe is misleading
as a keyword when it is used to allow unsafe operations through unsafe {};
it’s not that the contained code is unsafe, it’s that the code is allowed to perform
otherwise unsafe operations because in this particular context, those
operations are safe.
The rest of this chapter is split into four sections. We’ll start with a brief
examination of how the keyword itself is used, then explore what unsafe
allows you to do. Next, we’ll look at the rules you must follow in order to
write safe unsafe code. Finally, I’ll give you some advice about how to actually
go about writing unsafe code safely.
The unsafe Keyword
Before we discuss the powers that unsafe grants you, we need to talk about
its two different meanings. The unsafe keyword serves a dual purpose in
Rust: it marks a particular function as unsafe to call and it enables you to
invoke unsafe functionality in a particular code block. For example, the
method in Listing 9-1 is marked as unsafe, even though it contains no
unsafe code. Here, the unsafe keyword serves as a warning to the caller that
there are additional guarantees that someone who writes code that invokes
decr must manually check.
impl<T> SomeType<T> {
pub unsafe fn decr(&self) {
self.some_usize -= 1;
}
}
Listing 9-1: An unsafe method that contains only safe code
Unsafe Code 143
Listing 9-2 illustrates the second usage. Here, the method itself is not
marked as unsafe, even though it contains unsafe code.
impl<T> SomeType<T> {
pub fn as_ref(&self) -> &T {
unsafe { &*self.ptr }
}
}
Listing 9-2: A safe method that contains unsafe code
These two listings differ in their use of unsafe because they embody
different contracts. decr requires the caller to be careful when they call the
method, whereas as_ref assumes that the caller was careful when invoking
other unsafe methods (like decr). To see why, imagine that SomeType is really
a reference-counted type like Rc. Even though decr only decrements a number,
that decrement may in turn trigger undefined behavior through the
safe method as_ref. If you call decr and then drop the second-to-last Rc of a
given T, the reference count drops to zero and the T will be dropped—but
the program might still call as_ref on the last Rc, and end up with a dangling
reference.
**NOTE** Undefined behavior describes the consequences of a program that violates invariants
of the language at runtime. In general, if a program triggers undefined behavior,
the outcome is entirely unpredictable. We’ll cover undefined behavior in greater
detail later in this chapter.
Conversely, as long as there is no way to corrupt the Rc reference count
using safe code, it is always safe to dereference the pointer inside the Rc
the way the code for as_ref does—the fact that &self exists is proof that the
pointer must still be valid. We can use this to give the caller a safe API to
an otherwise unsafe operation, which is a core piece of how to use unsafe
responsibly.
For historical reasons, every unsafe fn contains an implicit unsafe block
in Rust today. That is, if you declare an unsafe fn, you can always invoke any
unsafe methods or primitive operations inside that fn. However, that decision
is now considered a mistake, and it’s currently being reverted through
the already accepted and implemented RFC 2585. This RFC warns about
having an unsafe fn that performs unsafe operations without an explicit
unsafe block inside it. The lint will also likely become a hard error in future
editions of Rust. The idea is to reduce the “footgun radius”—if every unsafe
fn is one giant unsafe block, then you might accidentally perform unsafe
operations without realizing it! For example, in decr in Listing 9-1, under
the current rules you could also have added*std::ptr::null() without any
unsafe annotation.
The distinction between unsafe as a marker and unsafe blocks as a
mechanism to enable unsafe operations is important, because you must
think about them differently. An unsafe fn indicates to the caller that
they have to be careful when calling the fn in question and that they must
ensure that the function’s documented safety invariants hold.
144 Chapter 9
Meanwhile, an unsafe block implies that whoever wrote that block carefully
checked that the safety invariants for any unsafe operations performed
inside it hold. If you want an approximate real-world analogy, unsafe fn is an
unsigned contract that asks the author of calling code to “solemnly swear X,
Y, and Z.” Meanwhile, unsafe {} is the calling code’s author signing off on all
the unsafe contracts contained within the block. Keep that in mind as we
go through the rest of this chapter.
Great Power
So, once you sign the unsafe contract with unsafe {}, what are you allowed
to do? Honestly, not that much. Or rather, it doesn’t enable that many new
features. Inside an unsafe block, you are allowed to dereference raw pointers
and call unsafe fns.
That’s it. Technically, there are a few other things you can do, like
accessing mutable and external static variables and accessing fields of
unions, but those don’t change the discussion much. And honestly, that’s
enough. Together, these powers allow you to wreak all sorts of havoc, like
turning types into one another with mem::transmute, dereferencing raw pointers
that point to who knows where, casting &'a to &'static, or making types
shareable across thread boundaries even though they’re not thread-safe.
In this section, we won’t worry too much about what can go wrong with
these powers. We’ll leave that for the boring, responsible, grown-up section
that comes after. Instead, we’ll look at these neat shiny new toys and what
we can do with them.
Juggling Raw Pointers
One of the most fundamental reasons to use unsafe is to deal with Rust’s raw
pointer types: *const T and*mut T. You should think of these as more or less
analogous to &T and &mut T, except that they don’t have lifetimes and are not
subject to the same validity rules as their & counterparts, which we’ll discuss
later in the chapter. These types are interchangeably referred to as pointers
and raw pointers, mostly because many developers instinctively refer to
references as pointers, and calling them raw pointers makes the distinction
clearer.
Since fewer rules apply to *than &, you can cast a reference to a pointer
even outside an unsafe block. Only if you want to go the other way, from*
to &, do you need unsafe. You’ll generally turn a pointer back into a reference
to do useful things with the pointed-to data, such as reading or modifying
its value. For that reason, a common operation to use on pointers is
unsafe { &_ptr } (or &mut_). The *there may look strange as the code is just
constructing a reference, not dereferencing the pointer, but it makes sense
if you look at the types; if you have a*mut T and want a &mut T, then &mut ptr
would just give you a &mut _mut T. You need the_ to indicate that you want
the mutable reference to what ptr is a pointer to.
Unsafe Code 145
POINTER TYPES
You may be wondering what the difference is between *mut T and*const T
and std::ptr::NonNull<T>. Well, the exact specification is still being worked
out, but the primary practical difference between *mut T and*const T/
NonNull<T> is that *mut T is invariant in T (see “Lifetime Variance” in Chapter 1),
whereas the other two are covariant. As the names imply,*const T and
NonNull<T> differ primarily in that NonNull<T> is not allowed to be a null pointer,
whereas *const T is.
My best advice in choosing among these types is to use your intuition
about whether you would have written &mut or & if you were able to name the
relevant lifetime. If you would have written &, and you know that the pointer is
never null, use NonNull<T>. It benefits from a cool optimization called the niche
optimization: basically, since the compiler knows that the type can never be
null, it can use that information to represent types like Option<NonNull<T>> without
any extra overhead, since the None case can be represented by setting the
NonNull to be a null pointer! The null pointer value is a niche in the NonNull<T>
type. If the pointer might be null, use*const T. And if you would have written
&mut T, use *mut T.
Unrepresentable Lifetimes
As raw pointers do not have lifetimes, they can be used in circumstances
where the liveness of the value being pointed to cannot be expressed statically
within Rust’s lifetime system, such as a self-pointer in a self-referential
struct like the generators we discussed in Chapter 8. A pointer that points
into self is valid for as long as self is around (and doesn’t move, which is
what Pin is for), but that isn’t a lifetime you can generally name. And while
the entire self-referential type may be 'static, the self-pointer isn’t—if it
were static, then even if you gave away that pointer to someone else, they
could continue to use it forever, even after self was gone! Take the type in
Listing 9-3 as an example; here we attempt to store the raw bytes that make
up a value alongside its stored representation.
struct Person<'a> {
name: &'a str,
age: usize,
}
struct Parsed {
bytes: [u8; 1024],
parsed: Person<'???>,
}
Listing 9-3: Trying, and failing, to name the lifetime of a self-referential reference
146 Chapter 9
The reference inside Person wants to refer to data stored in bytes in
Parsed, but there is no lifetime we can assign to that reference from Parsed.
It’s not 'static or something like 'self (which doesn’t exist), because if
Parsed is moved, the reference is no longer valid.
Since pointers do not have lifetimes, they circumvent this problem
because you don’t have to be able to name the lifetime. Instead, you just have
to make sure that when you do use the pointer, it’s still valid, which is what you
sign off on when you write unsafe { &*ptr }. In the example in Listing 9-3,
Person would instead store a *const str and then unsafely turn that into a &str
at the appropriate times when it can guarantee that the pointer is still valid.
A similar issue arises with a type like Arc, which has a pointer to a value
that’s shared for some duration, but that duration is known only at runtime
when the last Arc is dropped. The pointer is kind-of, sort-of 'static, but not
really—like in the self-referential case, the pointer is no longer valid when
the last Arc reference goes away, so the lifetime is more like 'self. In Arc’s
cousin, Weak, the lifetime is also “when the last Arc goes away,” but since a
Weak isn’t an Arc, the lifetime isn’t even tied to self. So, Arc and Weak both
use raw pointers internally.
Pointer Arithmetic
With raw pointers, you can do arbitrary pointer arithmetic, just like you
can in C, by using .offset(), .add(), and .sub() to move the pointer to any
byte that lives within the same allocation. This is most often used in highly
space-optimized data structures, like hash tables, where storing an extra
pointer for each element would add too much overhead and using slices
isn’t possible. Those are fairly niche use cases, and we won’t be talking
more about them in this book, but I encourage you to read the code for
hashbrown::RawTable (<https://github.com/rust-lang/hashbrown/>) if you want to
learn more!
The pointer arithmetic methods are unsafe to call even if you don’t
want to turn the pointer into a reference afterwards. There are a couple
of reasons for this, but the main one is that it is illegal to make a pointer
point beyond the end of the allocation that it originally pointed to. Doing
so triggers undefined behavior, and the compiler is allowed to decide to
eat your code and replace it with arbitrary nonsense that only a compiler
could understand. If you do use these methods, read the documentation
carefully!
To Pointer and Back Again
Often when you need to use pointers, it’s because you have some normal
Rust type, like a reference, a slice, or a string, and you have to move to the
world of pointers for a bit and then go back to the original normal type.
Some of the key standard library types therefore provide you with a way to
turn them into their raw constituent parts, such as a pointer and a length
for a slice, and a way to turn them back into the whole using those same
parts. For example, you can get a slice’s data pointer with as_ptr and its
length with []::len. You can then reconstruct the slice by providing those
Unsafe Code 147
same values to std::slice::from_raw_parts. Vec, Arc, and String have similar
methods that return a raw pointer to the underlying allocation, and Box has
Box::into_raw and Box::from_raw, which do the same thing.
Playing Fast and Loose with Types
Sometimes, you have a type T and want to treat it as some other type U.
Whether that’s because you need to do lightning-fast zero-copy parsing
or because you need to fiddle with some lifetimes, Rust provides you with
some (very unsafe) tools to do so.
The first and by far most widely used of these is pointer casting: you can
cast a*const T to any other *const U (and the same for mut), and you don’t
even need unsafe to do it. The unsafety comes into play only when you later
try to use the cast pointer as a reference, as you have to assert that the raw
pointer can in fact be used as a reference to the type it’s pointing to.
This kind of pointer type casting comes in particularly handy when working
with foreign function interfaces (FFI)—you can cast any Rust pointer to a
*const std::ffi::c_void or *mut std::ffi::c_void, and then pass that to a C function
that expects a void pointer. Similarly, if you get a void pointer from C that
you previously passed in, you can trivially cast it back into its original type.
Pointer casts are also useful when you want to interpret a sequence
of bytes as plain old data—types like integers, Booleans, characters, and
arrays, or #[repr(C)] structs of these—or write such types directly out as a
byte stream without serialization. There are a lot of safety invariants to keep
in mind if you want to try to do that, but we’ll leave that for later.
Calling Unsafe Functions
Arguably unsafe’s most commonly used feature is that it enables you to
call unsafe functions. Deeper down the stack, most of those functions are
unsafe because they operate on raw pointers at some fundamental level, but
higher up the stack you tend to interact with unsafety primarily through
function calls.
There’s really no limit to what calling an unsafe function might enable,
as it is entirely up to the libraries you interact with. But in general, unsafe
functions can be divided into three camps: those that interact with non-
Rust interfaces, those that skip safety checks, and those that have custom
invariants.
Foreign Function Interfaces
Rust lets you declare functions and static variables that are defined in a
language other than Rust using extern blocks (which we’ll discuss at length
in Chapter 11). When you declare such a block, you’re telling Rust that the
items appearing within it will be implemented by some external source when
the final program binary is linked, such as a C library you are integrating
with. Since externs exist outside of Rust’s control, they are inherently unsafe
to access. If you call a C function from Rust, all bets are off—it might overwrite
your entire memory contents and clobber all your neatly arranged
148 Chapter 9
references into random pointers into the kernel somewhere. Similarly, an
extern static variable could be modified by external code at any time, and
could be filled with all sorts of bad bytes that don’t reflect its declared type
at all. In an unsafe block, though, you can access externs to your heart’s
delight, as long as you’re willing to vouch for the other side of the extern
behaving according to Rust’s rules.
I’ll Pass on Safety Checks
Some unsafe operations can be made entirely safe by introducing additional
runtime checks. For example, accessing an item in a slice is unsafe
since you might try to access an item beyond the length of the slice. But,
given how common the operation is, it’d be unfortunate if indexing into a
slice was unsafe. Instead, the safe implementation includes bounds checks
that (depending on the method you use) either panic or return an Option if
the index you provide is out of bounds. That way, there is no way to cause
undefined behavior even if you pass in an index beyond the slice’s length.
Another example is in hash tables, which hash the key you provide rather
than letting you provide the hash yourself; this ensures that you’ll never try
to access a key using the wrong hash.
However, in the endless pursuit of ultimate performance, some developers
may find these safety checks add just a little too much overhead in their
tightest loops. To cater to situations where peak performance is paramount
and the caller knows that the indexes are in bounds, many data structures
provide alternate versions of particular methods without these safety
checks. Such methods usually include the word unchecked in the name to
indicate that they blindly trust the provided arguments to be safe and that
they do not do any of those pesky, slow safety checks. Some examples are
NonNull::new_unchecked, slice::get_unchecked, NonZero::new_unchecked, Arc::get
_mut_unchecked, and str::from_utf8_unchecked.
In practice, the safety and performance trade-off for unchecked methods
is rarely worth it. As always with performance optimization, measure
first, then optimize.
Custom Invariants
Most uses of unsafe rely on custom invariants to some degree. That is, they
rely on invariants beyond those provided by Rust itself, which are specific to
the particular application or library. Since so many functions fall into this
category, it’s hard to give a good general summary of this class of unsafe
functions. Instead, I’ll give some examples of unsafe functions with custom
invariants that you may come across in practice and want to use:
MaybeUninit::assume_init
The MaybeUninit type is one of the few ways in which you can store
values that are not valid for their type in Rust. You can think of a
MaybeUninit<T> as a T that may not be legal to use as a T at the moment.
For example, a MaybeUninit<NonNull> is allowed to hold a null pointer,
a MaybeUninit<Box> is allowed to hold a dangling heap pointer, and a
Unsafe Code 149
MaybeUninit<bool> is allowed to hold the bit pattern for the number 3
(normally it must be 0 or 1). This comes in handy if you are constructing
a value bit by bit or are dealing with zeroed or uninitialized memory
that will eventually be made valid (such as by being filled through
a call to std::io::Read::read). The assume_init function asserts that the
MaybeUninit now holds a valid value for the type T and can therefore be
used as a T.
ManuallyDrop::drop
The ManuallyDrop type is a wrapper type around a type T that does not
drop that T when the ManuallyDrop is dropped. Or, phrased differently, it
decouples the dropping of the outer type (ManuallyDrop) from the dropping
of the inner type (T). It implements safe access to the T through
DerefMut<Target = T> but also provides a drop method (separately from
the drop method of the Drop trait) to drop the wrapped T without dropping
the ManuallyDrop. That is, the drop function takes &mut self despite
dropping the T, and so leaves the ManuallyDrop behind. This comes in
handy if you have to explicitly drop a value that you cannot move, such
as in implementations of the Drop trait. Once that value is dropped, it
is no longer safe to try to access the T, which is why the call to drop is
unsafe—it asserts that the T will never be accessed again.
std::ptr::drop_in_place
drop_in_place lets you call a value’s destructor directly through a pointer
to that value. This is unsafe because the pointee will be left behind
after the call, so if some code then tries to dereference the pointer, it’ll
be in for a bad time! This method is particularly useful when you may
want to reuse memory, such as in an arena allocator, and need to drop
an old value in place without reclaiming the surrounding memory.
Waker::from_raw
In Chapter 8 we talked about the Waker type and how it is made up of a
data pointer and a RawWaker that holds a manually implemented vtable.
Once a Waker has been constructed, the raw function pointers in the
vtable, such as wake and drop, can be called from safe code (through
Waker::wake and drop(waker), respectively). Waker::from_raw is where the
asynchronous executor asserts that all the pointers in its vtable are
in fact valid function pointers that follow the contract set forth in the
documentation of RawWakerVTable.
std::hint::unreachable_unchecked
The hint module holds functions that give hints to the compiler about
the surrounding code but do not actually produce any machine code.
The unreachable_unchecked function in particular tells the compiler that it
is impossible for the program to reach a section of the code at runtime.
This in turn allows the compiler to make optimizations based on that
150 Chapter 9
knowledge, such as eliminating conditional branches to that location.
Unlike the unreachable! macro, which panics if the code does reach the
line in question, the effects of an erroneous unreachable_unchecked are
hard to predict. The compiler optimizations may cause peculiar and
hard-to-debug behavior, not to mention that your program will continue
running when something it believed to be true was not!
std::ptr::{read,write}_{unaligned,volatile}
The ptr module holds a number of functions that let you work with odd
pointers—those that do not meet the assumptions that Rust generally
makes about pointers. The first of these functions are read_unaligned and
write_unaligned, which let you access pointers that point to a T even if
that T is not stored according to T’s alignment (see the section on alignment
in Chapter 2). This might happen if the T is contained directly in
a byte array or is otherwise packed in with other values without proper
padding. The second notable pair of functions is read_volatile and
write_volatile, which let you operate on pointers that don’t point to
normal memory. Concretely, these functions will always access the given
pointer (they won’t be cached in a register, for example, even if you
read the same pointer twice in a row), and the compiler won’t reorder
the volatile accesses relative to other volatile accesses. Volatile operations
come in handy when working with pointers that aren’t backed
by normal DRAM memory—we’ll discuss this further in Chapter 11.
Ultimately, these methods are unsafe because they dereference the
given pointer (and to an owned T, at that), so you as the caller need to
sign off on all the contracts associated with doing so.
std::thread::Builder::spawn_unchecked
The normal thread::spawn that we know and love requires that the
provided closure is 'static. That bound stems from the fact that the
spawned thread might run for an indeterminate amount of time; if
we were allowed to use a reference to, say, the caller’s stack, the caller
might return well before the spawned thread exits, rendering the reference
invalid. Sometimes, however, you know that some non-'static
value in the caller will outlive the spawned thread. This might happen
if you join the thread before dropping the value in question, or if the
value is dropped only strictly after you know the spawned thread will
no longer use it. That’s where spawn_unchecked comes in—it does not
have the 'static bound and thus lets you implement those use cases as
long as you’re willing to sign the contract saying that no unsafe accesses
will happen as a result. Be careful of panics, though; if the caller panics,
it might drop values earlier than you planned and cause undefined
behavior in the spawned thread!
Note that all of these methods (and indeed all unsafe methods in the
standard library) provide explicit documentation for their safety invariants,
as should be the case for any unsafe method.
Unsafe Code 151
Implementing Unsafe Traits
Unsafe traits aren’t unsafe to use, but unsafe to implement. This is because
unsafe code is allowed to rely on the correctness (defined by the trait’s documentation)
of the implementation of unsafe traits. For example, to implement
the unsafe trait Send, you need to write unsafe impl Send for .... Like
unsafe functions, unsafe traits generally have custom invariants that are (or
at least should be) specified in the documentation for the trait. Thus, it’s
difficult to cover unsafe traits as a group, so here too I’ll give some common
examples from the standard library that are worth going over.
Send and Sync
The Send and Sync traits denote that a type is safe to send or share across
thread boundaries, respectively. We’ll talk more about these traits in
Chapter 10, but for now what you need to know is that they are auto-traits,
and so they’ll usually be implemented for most types for you by the compiler.
But, as tends to be the case with auto-traits, Send and Sync will not be
implemented if any members of the type in question are not themselves Send
or Sync.
In the context of unsafe code, this problem occurs primarily due to
raw pointers, which are neither Send nor Sync. At first glance, this might
seem reasonable: the compiler has no way to know who else may have a raw
pointer to the same value or how they may be using it at the moment, so
how can the type be safe to send across threads? Now that we’re seasoned
unsafe developers though, that argument seems weak—after all, dereferencing
a raw pointer is already unsafe, so why should handling the invariants of
Send and Sync be any different?
Strictly speaking, raw pointers could be both Send and Sync. The problem
is that if they were, the types that contain raw pointers would automatically
be Send and Sync themselves, even though their author might not
realize that was the case. The developer might then unsafely dereference
the raw pointers without ever thinking about what would happen if those
types were sent or shared across thread boundaries, and thus inadvertently
introduce undefined behavior. Instead, the raw pointer types block these
automatic implementations as an additional safeguard to unsafe code to
make authors explicitly sign the contract that they have also followed the
Send and Sync invariants.
**NOTE** A common mistake with unsafe implementations of Send and Sync is to forget to add
bounds to generic parameters: unsafe impl<T: Send> Send for MyUnsafeType<T> {}.
GlobalAlloc
The GlobalAlloc trait is how you implement a custom memory allocator
in Rust. We won’t talk too much about that topic in this book, but the
trait itself is interesting. Listing 9-4 gives the required methods for the
GlobalAlloc trait.
152 Chapter 9
pub unsafe trait GlobalAlloc {
pub unsafe fn alloc(&self, layout: Layout) ->*mut u8;
pub unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);
}
Listing 9-4: The GlobalAlloc trait with its required methods
At its core, the trait has one method for allocating a new chunk of
memory, alloc, and one for deallocating a chunk of memory, dealloc. The
Layout argument describes the type’s size and alignment, as we discussed in
Chapter 2. Each of those methods is unsafe and carries a number of safety
invariants that its callers must uphold.
GlobalAlloc itself is also unsafe because it places restrictions on the
implementer of the trait, not the caller of its methods. Only the unsafety
of the trait ensures that implementers agree to uphold the invariants that
Rust itself assumes of its memory allocator, such as in the standard library’s
implementation of Box. If the trait was not unsafe, an implementer could
safely implement GlobalAlloc in a way that produced unaligned pointers or
incorrectly sized allocations, which would trigger unsafety in otherwise safe
code that assumes that allocations are sane. This would break the rule that
safe code should not be able to trigger memory unsafety in other safe code,
and thus cause all sorts of mayhem.
Surprisingly Not Unpin
The Unpin trait is not unsafe, which comes as a surprise to many Rust developers.
It may even come as a surprise to you after reading Chapter 8. After
all, the trait is supposed to ensure that self-referential types aren’t invalidated
if they’re moved after they have established internal pointers (that is,
after they’ve been placed in a Pin). It seems strange, then, that Unpin can be
used to safely remove a type from a Pin.
There are two main reasons why Unpin isn’t an unsafe trait. First, it’s
unnecessary. Implementing Unpin for a type that you control does not grant
you the ability to safely pin or unpin a !Unpin type; that still requires unsafety
in the form of a call to Pin::new_unchecked or Pin::get_unchecked_mut. Second,
there is already a safe way for you to unpin any type you control: the Drop trait!
When you implement Drop for a type, you’re passed &mut self, even if your
type was previously stored in a Pin and is !Unpin, all without any unsafety. That
potential for unsafety is covered by the invariants of Pin::new_unchecked, which
must be upheld to create a Pin of such an !Unpin type in the first place.
When to Make a Trait Unsafe
Few traits in the wild are unsafe, but those that are all follow the same pattern.
A trait should be unsafe if safe code that assumes that trait is implemented
correctly can exhibit memory unsafety if the trait is not implemented
correctly.
The Send trait is a good example to keep in mind here—safe code can
easily spawn a thread and pass a value to that spawned thread, but if Rc were
Unsafe Code 153
Send, that sequence of operations could trivially lead to memory unsafety.
Consider what would happen if you cloned an Rc<Box> and sent it to another
thread: the two threads could easily both try to deallocate the Box since they
do not correctly synchronize access to the Rc’s reference count.
The Unpin trait is a good counterexample. While it is possible to write
unsafe code that triggers memory unsafety if Unpin is implemented incorrectly,
no entirely safe code can trigger memory unsafety due to an implementation
of Unpin. It’s not always easy to determine that a trait can be safe
(indeed, the Unpin trait was unsafe throughout most of the RFC process),
but you can always err on the side of making the trait unsafe, and then
make it safe later on if you realize that is the case! Just keep in mind that
that is a backward incompatible change.
Also keep in mind that just because it feels like an incorrect (or even
malicious) implementation of a trait would cause a lot of havoc, that’s not
necessarily a good reason to make it unsafe. The unsafe marker should first
and foremost be used to highlight cases of memory unsafety, not just something
that can trigger errors in business logic. For example, the Eq, Ord,
Deref, and Hash traits are all safe, even though there is likely much code out
in the world that would go haywire if faced with a malicious implementation
of, say, Hash that returned a different random hash each time it was
called. This extends to unsafe code too—there is almost certainly unsafe
code out there that would be memory-unsafe in the presence of such an
implementation of Hash—but that does not mean Hash should be unsafe.
The same is true for an implementation of Deref that dereferenced to a different
(but valid) target each time. Such unsafe code would be relying on a
contract of Hash or Deref that does not actually hold; Hash never claimed that
it was deterministic, and neither did Deref. Or rather, the authors of those
implementations never used the unsafe keyword to make that claim!
**NOTE** An important implication of traits like Eq, Hash, and Deref being safe is that unsafe
code can rely only on the safety of safe code, not its correctness. This applies not only
to traits, but to all unsafe/safe code interactions.
Great Responsibility
So far, we’ve looked mainly at the various things that you are allowed to do
with unsafe code. But unsafe code is allowed to do those things only if it
does so safely. Even though unsafe code can, say, dereference a raw pointer,
it must do so only if it knows that pointer is valid as a reference to its pointee
at that moment in time, subject to all of Rust’s normal requirements of
references. In other words, unsafe code is given access to tools that could be
used to do unsafe things, but it must do only safe things using those tools.
That, then, raises the question of what safe even means in the first
place. When is it safe to dereference a pointer? When is it safe to transmute
between two different types? In this section, we’ll explore some of the key
Unsafe Code 155
dramatically change the semantics of the code, or even miscompile surrounding
code. What that does to your program is entirely dependent on
what the code in question does. The unpredictable impact of undefined
behavior is the reason why all undefined behavior should be considered a
serious bug, no matter how it currently manifests.
WHY UNDEFINED BEHAVIOR?
An argument that often comes up in conversations about undefined behavior
is that the compiler should emit an error if code exhibits undefined behavior
instead of doing something weird and unpredictable. That way, it would be
near-impossible to write bad unsafe code!
Unfortunately, that would be impossible because undefined behavior is
rarely explicit or obvious. Instead, what usually happens is that the compiler
simply applies optimizations under the assumption that the code follows the
specification. Should that turn out to not be the case—which is rarely clear until
runtime—it’s difficult to predict what the effect might be. Maybe the optimization
is still valid, and nothing bad happens; but maybe it’s not, and the semantics
of the code end up slightly different from that of the unoptimized version.
If we were to tell compiler developers that they aren’t allowed to assume
anything about the underlying code, what we’d really be telling them is that
they cannot perform a wide range of the optimizations that they implement with
great success today. Nearly all sophisticated optimizations make assumptions
about what the code in question can and cannot do according to the language
specification.
If you want a good illustration of how specifications and compiler optimizations
interact in strange ways where it’s hard to assign blame, I recommend
reading through Ralf Jung’s blog post “We Need Better Language Specs”
(<https://www.ralfj.de/blog/2020/12/14/provenance.html>).
Validity
Perhaps the most important concept to understand before writing unsafe
code is validity, which dictates the rules for what values inhabit a given
type—or, less formally, the rules for a type’s values. The concept is simpler
than it sounds, so let’s dive into some concrete examples.
Reference Types
Rust is very strict about what values its reference types can hold. Specifically,
references must never dangle, must always be aligned, and must always
point to a valid value for their target type. In addition, a shared and an
exclusive reference to a given memory location can never exist at the same
time, and neither can multiple exclusive references to a location. These
156 Chapter 9
rules apply regardless of whether your code uses the references or not—you
are not allowed to create a null reference even if you then immediately discard
it!
Shared references have the additional constraint that the pointee is
not allowed to change during the reference’s lifetime. That is, any value
the pointee contains must remain exactly the same over its lifetime. This
applies transitively, so if you have an & to a type that contains a*mut T, you
are not allowed to ever mutate the T through that *mut even though you
could write code to do so using unsafe. The only exception to this rule is a
value wrapped by the UnsafeCell type. All other types that provide interior
mutability, like Cell, RefCell, and Mutex, internally use an UnsafeCell.
An interesting result of Rust’s strict rules for references is that for many
years, it was impossible to safely take a reference to a field of a packed or
partially uninitialized struct that used repr(Rust). Since repr(Rust) leaves
a type’s layout undefined, the only way to get the address of a field was by
writing &some_struct.field as*const_. However, if some_struct is packed,
then some_struct.field may not be aligned, and thus creating an & to it is
illegal! Further, if some_struct isn’t fully initialized, then the some_struct reference
itself cannot exist! In Rust 1.51.0, the ptr::addr_of! macro was stabilized,
which added a mechanism for directly obtaining a reference to a field
without first creating a reference, fixing this particular problem. Internally,
it is implemented using something called raw references (not to be confused
with raw pointers), which directly create pointers to their operands rather
than going via a reference. Raw references were introduced in RFC 2582
but haven’t been stabilized themselves yet at the time of writing.
Primitive Types
Some of Rust’s primitive types have restrictions on what values they can
hold. For example, a bool is defined as being 1 byte large but is only allowed
to hold the value 0x00 or the value 0x01, and a char is not allowed to hold
a surrogate or a value above char::MAX. Most of Rust’s primitive types, and
indeed most of Rust’s types overall, also cannot be constructed from uninitialized
memory. These restrictions may seem arbitrary, but again often stem
from the need to enable optimizations that wouldn’t be possible otherwise.
A good illustration of this is the niche optimization, which we discussed
briefly when talking about pointer types earlier in this chapter. To recap,
the niche optimization tucks away the enum discriminant value in the
wrapped type in certain cases. For example, since a reference cannot ever
be all zeros, an Option<&T> can use all zeros to represent None, and thus avoid
spending an extra byte (plus padding) to store the discriminator byte. The
compiler can optimize Booleans in the same way and potentially take it
even further. Consider the type Option<Option<bool>>>. Since the compiler
knows that the bool is either 0x00 or 0x01, it’s free to use 0x02 to represent
Some(None) and 0x03 to represent None. Very nice and tidy! But if someone
were to come along and treat the byte 0x03 as a bool, and then place that
value in an Option<Option<bool>> optimized in this way, bad things would
happen.
Unsafe Code 157
It bears repeating that it’s not important whether the Rust compiler currently
implements this optimization or not. The point is that it is allowed to,
and therefore any unsafe code you write must conform to that contract or
risk hitting a bug later on should the behavior change.
Owned Pointer Types
Types that point to memory they own, like Box and Vec, are generally subject
to the same optimizations as if they held an exclusive reference to the
pointed-to memory unless they’re explicitly accessed through a shared
reference. Specifically, the compiler assumes that the pointed-to memory
is not shared or aliased elsewhere, and makes optimizations based on
that assumption. For example, if you extracted the pointer from a Box and
then constructed two Boxes from that same pointer and wrapped them in
ManuallyDrop to prevent a double-free, you’d likely be entering undefined
behavior territory. That’s the case even if you only ever access the inner
type through shared references. (I say “likely” because this isn’t fully settled
in the language reference yet, but a rough consensus has arisen.)
Storing Invalid Values
Sometimes you need to store a value that isn’t currently valid for its type.
The most common example of this is if you want to allocate a chunk of
memory for some type T and then read in the bytes from, say, the network.
Until all the bytes have been read in, the memory isn’t going to be a valid T.
Even if you just tried to read the bytes into a slice of u8, you would have to
zero those u8s first, because constructing a u8 from uninitialized memory is
also undefined behavior.
The MaybeUninit<T> type is Rust’s mechanism for working with values
that aren’t valid. A MaybeUninit<T> stores exactly a T (it is #[repr(transparent)]),
but the compiler knows to make no assumptions about the validity of that T.
It won’t assume that references are non-null, that a Box<T> isn’t dangling, or
that a bool is either 0 or 1. This means it’s safe to hold a T backed by uninitialized
memory inside a MaybeUninit (as the name implies). MaybeUninit is
also a very useful tool in other unsafe code where you have to temporarily
store a value that may be invalid. Maybe you have to store an aliased Box<T>
or stash a char surrogate for a second—MaybeUninit is your friend.
You will generally do only three things with a MaybeUninit: create it using
the MaybeUninit::uninit method, write to its contents using MaybeUninit::as
_mut_ptr, or take the inner T once it is valid again with MaybeUninit::assume_init.
As its name implies, uninit creates a new MaybeUninit<T> of the same size as
a T that initially holds uninitialized memory. The as_mut_ptr method gives
you a raw pointer to the inner T that you can then write to; nothing stops
you from reading from it, but reading from any of the uninitialized bits is
undefined behavior. And finally, the unsafe assume_init method consumes
the MaybeUninit<T> and returns its contents as a T following the assertion that
the backing memory now makes up a valid T.
Listing 9-5 shows an example of how we might use MaybeUninit to safely
initialize a byte array without explicitly zeroing it.
158 Chapter 9
fn fill(gen: impl FnMut() -> Option<u8>) {
let mut buf = [MaybeUninit::<u8>::uninit(); 4096];
let mut last = 0;
for (i, g) in std::iter::from_fn(gen).take(4096).enumerate() {
buf[i] = MaybeUninit::new(g);
last = i + 1;
}
// Safety: all the u8s up to last are initialized.
let init: &[u8] = unsafe {
MaybeUninit::slice_assume_init_ref(&buf[..last])
};
// ... do something with init ...
}
Listing 9-5: Using MaybeUninit to safely initialize an array
While we could have declared buf as [0; 4096] instead, that would
require the function to first write out all those zeros to the stack before
executing, even if it’s going to overwrite them all again shortly thereafter.
Normally that wouldn’t have a noticeable impact on performance, but if
this was in a sufficiently hot loop, it might! Here, we instead allow the array
to keep whatever values happened to be on the stack when the function was
called, and then overwrite only what we end up needing.
**NOTE** Be careful with dropping partially initialized memory. If a panic causes an unexpected
early drop before the MaybeUninit<T> has been fully initialized, you’ll have to
take care to drop only the parts of T that are now valid, if any. You can just drop the
MaybeUninit and have the backing memory forgotten, but if it holds, say, a Box, you
might end up with a memory leak!
Panics
An important and often overlooked aspect of ensuring that code using
unsafe operations is safe is that the code must also be prepared to handle
panics. In particular, as we discussed briefly in Chapter 5, Rust’s default
panic handler on most platforms will not crash your program on a panic
but will instead unwind the current thread. An unwinding panic effectively
drops everything in the current scope, returns from the current function,
drops everything in the scope that enclosed the function, and so on,
all the way down the stack until it hits the first stack frame for the current
thread. If you don’t take unwinding into account in your unsafe code, you
may be in for trouble. For example, consider the code in Listing 9-6, which
tries to efficiently push many values into a Vec at once.
impl<T: Default> Vec<T> {
pub fn fill_default(&mut self) {
let fill = self.capacity() - self.len();
if fill == 0 { return; }
let start = self.len();
unsafe {
self.set_len(start + n);
Unsafe Code 159
for i in 0..fill {
*self.get_unchecked_mut(start + i) = T::default();
}
}
}
}
Listing 9-6: A seemingly safe method for filling a vector with Default values
Consider what happens to this code if a call to T::default panics. First,
fill_default will drop all its local values (which are just integers) and then
return. The caller will then do the same. At some point up the stack, we
get to the owner of the Vec. When the owner drops the vector, we have a
problem: the length of the vector now indicates that we own more Ts than
we actually produced due to the call to set_len. For example, if the very
first call to T::default panicked when we aimed to fill eight elements, that
means Vec::drop will call drop on eight Ts that actually contain uninitialized
memory!
The fix in this case is simple: the code must update the length after writing
all the elements. We wouldn’t have realized there was a problem if we
didn’t carefully consider the effect of unwinding panics on the correctness
of our unsafe code.
When you’re combing through your code for these kinds of problems,
you’ll want to look out for any statements that may panic, and consider
whether your code is safe if they do. Alternatively, check whether you can
convince yourself that the code in question will never panic. Pay particular
attention to anything that calls user-provided code—in those cases, you have
no control over the panics and should assume that the user code will panic.
A similar situation arises when you use the ? operator to return early
from a function. If you do this, make sure that your code is still safe if it
does not execute the remainder of the code in the function. It’s rarer for ?
to catch you off guard since you opted into it explicitly, but it’s worth keeping
an eye out for.
Casting
As we discussed in Chapter 2, two different types that are both #[repr(Rust)]
may be represented differently in memory even if they have fields of the
same type and in the same order. This in turn means that it’s not always
obvious whether it is safe to cast between two different types. In fact, Rust
doesn’t even guarantee that two instances of a single type with generic
arguments that are themselves laid out the same way are represented the
same way. For example, in Listing 9-7, A and B are not guaranteed to have
the same in-memory representation.
struct Foo<T> {
one: bool,
two: PhantomData<T>,
}
struct Bar;
struct Baz;
160 Chapter 9
type A = Foo<Bar>;
type B = Foo<Baz>;
Listing 9-7: Type layout is not predictable.
The lack of guarantees for repr(Rust) is important to keep in mind
when you do type casting in unsafe code—just because two types feel like
they should be interchangeable, that is not necessarily the case. Casting
between two types that have different representations is a quick path to
undefined behavior. At the time of writing, the Rust community is actively
working out the exact rules for how types are represented, but for now, very
few guarantees are given, so that’s what we have to work with.
Even if identical types were guaranteed to have the same in-memory
representation, you’d still run into the same problem when types are
nested. For example, while UnsafeCell<T>, MaybeUninit<T>, and T all really
just hold a T, and you can cast between them to your heart’s delight, that
goes out the window once you have, for example, an Option<MaybeUninit<T>>.
Though Option<T> may be able to take advantage of the niche optimization
(using some invalid value of T to represent None for the Option), MaybeUninit<T>
can hold any bit pattern, so that optimization does not apply, and an extra
byte must be kept for the Option discriminator.
It’s not just optimizations that can cause layouts to diverge once wrapper
types come into play. As an example, take the code in Listing 9-8; here,
the layout of Wrapper<PhantomData<u8>> and Wrapper<PhantomData<i8>> is completely
different even though the provided types are both empty!
struct Wrapper<T: SneakyTrait> {
item: T::Sneaky,
iter: PhantomData<T>,
}
trait SneakyTrait {
type Sneaky;
}
impl SneakyTrait for PhantomData<u8> {
type Sneaky = ();
}
impl SneakyTrait for PhantomData<i8> {
type Sneaky = [u8; 1024];
}
Listing 9-8: Wrapper types make casting hard to get right.
All of this isn’t to say that you can never cast types in Rust. Things get a
lot easier, for example, when you control all of the types involved and their
trait implementations, or if types are #[repr(C)]. You just need to be aware
that Rust gives very few guarantees about in-memory representations, and
write your code accordingly!
The Drop Check
The Rust borrow checker is, in essence, a sophisticated tool for ensuring
the soundness of code at compile time, which is in turn what gives Rust a
Unsafe Code 161
way to express code being “safe.” How exactly the borrow checker does its
job is beyond the scope of this book, but one check, the drop check, is worth
going through in some detail since it has some direct implications for
unsafe code. To understand drop checking, let’s put ourselves in the Rust
compiler’s shoes for a second and look at two code snippets. First, take a
look at the little three-liner in Listing 9-9 that takes a mutable reference to
a variable and then mutates that same variable right after.
let mut x = true;
let foo = Foo(&mut x);
x = false;
Listing 9-9: The implementation of Foo dictates whether this code should compile
Without knowing the definition of Foo, can you say whether this code
should compile or not? When we set x = false, there is still a foo hanging
around that will be dropped at the end of the scope. We know that foo contains
a mutable borrow of x, which would indicate that the mutable borrow
that’s necessary to modify x is illegal. But what’s the harm in allowing it? It
turns out that allowing the mutation of x is problematic only if Foo implements
Drop—if Foo doesn’t implement Drop, then we know that Foo won’t
touch the reference to x after its last use. Since that last use is before we
need the exclusive reference for the assignment, we can allow the code! On
the other hand, if Foo does implement Drop, we can’t allow this code, since
the Drop implementation may use the reference to x.
Now that you’re warmed up, take a look at Listing 9-10. In this not-sostraightforward
code snippet, the mutable reference is buried even deeper.
fn barify<’a>(_: &’a mut i32) -> Bar<Foo<’a>> { .. }
let mut x = true;
let foo = barify(&mut x);
x = false;
Listing 9-10: The implementations of both Foo and Bar dictate whether this code should
compile
Again, without knowing the definitions of Foo and Bar, can you say
whether this code should compile or not? Let’s consider what happens if
Foo implements Drop but Bar does not, since that’s the most interesting case.
Usually, when a Bar goes out of scope, or otherwise gets dropped, it’ll still
have to drop Foo, which in turn means that the code should be rejected for
the same reason as before: Foo::drop might access the reference to x. However,
Bar may not contain a Foo directly at all, but instead just a PhantomData<Foo<'a>>
or a &'static Foo<'a>, in which case the code is actually okay—even though
the Bar is dropped, Foo::drop is never invoked, and the reference to x is never
accessed. This is the kind of code we want the compiler to accept because a
human will be able to identify that it’s okay, even if the compiler finds it difficult
to detect that this is the case.
The logic we’ve just walked through is the drop check. Normally it
doesn’t affect unsafe code too much as its default behavior matches user
expectations, with one major exception: dangling generic parameters.
162 Chapter 9
Imagine that you’re implementing your own Box<T> type, and someone
places a &mut x into it as we did in Listing 9-9. Your Box type needs to implement
Drop to free memory, but it doesn’t access T beyond dropping it. Since
dropping a &mut does nothing, it should be entirely fine for code to access
&mut x again after the last time the Box is accessed but before it’s dropped!
To support types like this, Rust has an unstable feature called dropck_eyepatch
(because it makes the drop check partially blind). The feature is likely
to remain unstable forever and is intended to serve only as a temporary
escape hatch until a proper mechanism is devised. The dropck_eyepatch feature
adds a #[may_dangle] attribute, which you can add as a prefix for generic
lifetimes and types in a type’s Drop implementation to tell the drop check
machinery that you won’t use the annotated lifetime or type beyond dropping
it. You use it by writing:
unsafe impl<#[may_dangle] T> Drop for ..
This escape hatch allows a type to declare that a given generic parameter
isn’t used in Drop, which enables use cases like Box<&mut T>. However,
it also introduces a new problem if your Box<T> holds a raw heap pointer,
*mut T, and allows T to dangle using #[may_dangle]. Specifically, the*mut T
makes Rust’s drop check think that your Box<T> doesn’t own a T, and thus
that it doesn’t call T::drop either. Combined with the may_dangle assertion
that we don’t access T when the Box<T> is dropped, the drop check now concludes
that it’s fine to have a Box<T> where the T doesn’t live until the Box
is dropped (like our shortened &mut x in Listing 9-10). But that’s not true,
since we do call T::drop, which may itself access, say, a reference to said x.
Luckily, the fix is simple: we add a PhantomData<T> to tell the drop check
that even though the Box<T> doesn’t hold any T, and won’t access T on drop,
it does still own a T and will drop one when the Box is dropped. Listing 9-11
shows what our hypothetical Box type would look like.
struct Box<T> {
t: NonNull<T>, // NonNull not _mut for covariance (Chapter 1)
_owned: PhantomData<T>, // For drop check to realize we drop a T
}
unsafe impl<#[may_dangle] T> for Box<T> { /_ ... _/ }
Listing 9-11: A definition for Box that is maximally flexible in terms of the drop check
This interaction is subtle and easy to miss, but it arises only when you
use the unstable #[may_dangle] attribute. Hopefully this subsection will serve
as a warning so that when you see unsafe impl Drop in the wild in the future,
you’ll know to look for a PhantomData<T> as well!
**NOTE** Another consideration for unsafe code concerning Drop is to make sure that you have
a Type<T> that lets T continue to live after self is dropped. For example, if you’re
implementing delayed garbage collection, you need to also add T: 'static. Otherwise,
if T = WriteOnDrop<&mut U>, the later access or drop of T could trigger undefined
behavior!
Unsafe Code 163
Coping with Fear
With this chapter mostly behind you, you may now be more afraid of unsafe
code than you were before you started. While that is understandable, it’s
important to stress that it’s not only possible to write safe unsafe code, but
most of the time it’s not even that difficult. The key is to make sure that you
handle unsafe code with care; that’s half the struggle. And be really sure
that there isn’t a safe implementation you can use instead before resorting
to unsafe.
In the remainder of this chapter, we’ll look at some techniques and
tools that can help you be more confident in the correctness of your unsafe
code when there’s no way around it.
Manage Unsafe Boundaries
It’s tempting to reason about unsafety locally; that is, to consider whether
the code in the unsafe block you just wrote is safe without thinking too
much about its interaction with the rest of the codebase. Unfortunately,
that kind of local reasoning often comes back to bite you. A good example
of this is the Unpin trait—you may write some code for your type that uses
Pin::new_unchecked to produce a pinned reference to a field of the type, and
that code may be entirely safe when you write it. But then at some later
point in time, you (or someone else) might add a safe implementation of
Unpin for said type, and suddenly the unsafe code is no longer safe, even
though it’s nowhere near the new impl!
Safety is a property that can be checked only at the privacy boundary of
all code that relates to the unsafe block. Privacy boundary here isn’t so much
a formal term as an attempt at describing “any part of your code that can
fiddle with the unsafe bits.” For example, if you declare a public type Foo in
a module bar that is marked pub or pub(crate), then any other code in the
same crate can implement methods on and traits for Foo. So, if the safety
of your unsafe code depends on Foo not implementing particular traits or
methods with particular signatures, you need to remember to recheck the
safety of that unsafe block any time you add an impl for Foo. If, on the other
hand, Foo is not visible to the entire crate, then a much smaller set of scopes
is able to add problematic implementations, and thus, the risk of accidentally
adding an implementation that breaks the safety invariants goes down
accordingly. If Foo is private, then only the current module and any submodules
can add such implementations.
The same rule applies to access to fields: if the safety of an unsafe block
depends on certain invariants over a type’s fields, then any code that can
touch those fields (including safe code) falls within the privacy boundary
of the unsafe block. Here, too, minimizing the privacy boundary is the
best approach—code that cannot get to the fields cannot mess up your
invariants!
Because unsafe code often requires this wide-reaching reasoning,
it’s best practice to encapsulate the unsafety in your code as best you can.
Provide the unsafety in the form of a single module, and strive to give that
164 Chapter 9
module an interface that is entirely safe. That way you only need to audit the
internals of that module for your invariants. Or better yet, stick the unsafe
bits in their own crate so that you can’t leave any holes open by accident!
It’s not always possible to fully encapsulate complex unsafe interactions
to a single, safe interface, however. When that’s the case, try to narrow
down the parts of the public interface that have to be unsafe so that you
have only a very small number of them, give them names that clearly communicate
that care is needed, and then document them rigorously.
It is sometimes tempting to remove the unsafe marker on internal APIs
so that you don’t have to stick unsafe {} throughout your code. After all,
inside your code you know never to invoke frobnify if you’ve previously
called bazzify, right? Removing the unsafe annotation can lead to cleaner
code but is usually a bad decision in the long run. A year from now, when
your codebase has grown, you’ve paged out some of the safety invariants,
and you “ just want to hack together this one feature real quick,” chances
are that you’ll inadvertently violate one of those invariants. And since
you don’t have to type unsafe, you won’t even think to check. Plus, even
if you never make mistakes, what about other contributors to your code?
Ultimately, cleaner code is not a good enough argument to remove the
intentionally noisy unsafe marker.
Read and Write Documentation
It goes without saying that if you write an unsafe function, you must document
the conditions under which that function is safe to call. Here, both
clarity and completeness are important. Don’t leave any invariants out, even
if you’ve already written them somewhere else. If you have a type or module
that requires certain global invariants—invariants that must always hold for
all uses of the type—then remind the reader that they must also uphold the
global invariants in every unsafe function’s documentation too. Developers
often read documentation in an ad hoc, on-demand manner, so you can
assume they have probably not read your carefully written module-level
documentation and need to be given a nudge to do so.
What may be less obvious is that you should also document all unsafe
implementations and blocks—think of this as providing proof that you
do indeed uphold the contract the operation in question requires. For
example, slice::get_unchecked requires that the provided index is within the
bounds of the slice; when you call that method, put a comment just above
it explaining how you know that the index is in fact guaranteed to be in
bounds. In some cases, the invariants that the unsafe block requires are
extensive, and your comments may get long. That’s a good thing. I have
caught mistakes many times by trying to write the safety comment for an
unsafe block and realizing halfway through that I actually don’t uphold
a key invariant. You’ll also thank yourself a year down the road when you
have to modify this code and ensure it’s still safe. And so will the contributor
to your project who just stumbled across this unsafe call and wants to
understand what’s going on.
Unsafe Code 165
Before you get too deep into writing unsafe code, I also highly recommend
that you go read the Rustonomicon (<https://doc.rust-lang.org/nomicon/>)
cover to cover. There are so many details that are easy to miss, and will
come back to bite you if you’re not aware of them. We’ve covered many
of them in this chapter, but it never hurts to be more aware. You should
also make liberal use of the Rust reference whenever you’re in doubt. It’s
added to regularly, and chances are that if you’re even slightly unsure about
whether some assumption you have is right, the reference will call it out. If
it doesn’t, consider opening an issue so that it’ll be added!
Check Your Work
Okay, so you’ve written some unsafe code, you’ve double- and triple-checked
all the invariants, and you think it’s ready to go. Before you put it into production,
there are some automated tools that you should run your test suite
through (you have a test suite, right?).
The first of these is Miri, the mid-level intermediate representation
interpreter. Miri doesn’t compile your code into machine code but instead
interprets the Rust code directly. This provides Miri with far more visibility
into what your program is doing, which in turn allows it to check that your
program doesn’t do anything obviously bad, like read from uninitialized
memory. Miri can catch a lot of very subtle and Rust-specific bugs and is a
lifesaver for anyone writing unsafe code.
Unfortunately, because Miri has to interpret the code to execute it, code
run under Miri often runs orders of magnitude slower than its compiled
counterpart. For that reason, Miri should really be used only to execute your
test suite. It can also check only the code that actually runs, and thus won’t
catch issues in code paths that your test suite doesn’t reach. You should think
of Miri as an extension of your test suite, not a replacement for it.
There are also tools known as sanitizers, which instrument machine code
to detect erroneous behavior at runtime. The overhead and fidelity of these
tools vary greatly, but one widely loved tool is Google’s AddressSanitizer.
It detects a large number of memory errors, such as use-after-free, buffer
overflows, and memory leaks, all of which are common symptoms of incorrect
unsafe code. Unlike Miri, these tools operate on machine code and thus
tend to be fairly fast—usually within the same order of magnitude. But like
Miri, they are constrained to analyzing the code that actually runs, so here
too a solid test suite is vital.
The key to using these tools effectively is to automate them through
your continuous integration pipeline so they’re run for every change, and
to ensure that you add regression tests over time as you discover errors.
The tools get better at catching problems as the quality of your test suite
improves, so by incorporating new tests as you fix known bugs, you’re earning
double points back, so to speak!
Finally, don’t forget to sprinkle assertions generously through unsafe
code. A panic is always better than triggering undefined behavior! Check
all of your assumptions with assertions if you can—even things like the size
166 Chapter 9
of a usize if you rely on that for safety. If you’re concerned about runtime
cost, make use of the debug_assert_ macros and the if cfg!(debug_assertions)
|| cfg!(test) construct to execute them only in debug and test contexts.
A HOUSE OF CARDS?
Unsafe code can violate all of Rust’s safety guarantees, and this is often touted
as a reason why Rust’s whole safety argument is a charade. The concern is
that it takes only one bit of incorrect unsafe code for the whole house to come
crashing down and all safety to be lost. Proponents of this argument then
sometimes argue that at the very least only unsafe code should be able to call
unsafe code, so that the unsafety is visible all the way to the highest level of the
application.
The argument is understandable—it is true that the safety of Rust code
relies on the safety of all the transitive unsafe code it ends up invoking. And
indeed, if some of that unsafe code is incorrect, it may have implications for
the safety of the program overall. However, what this argument misses is that
all successful safe languages provide a facility for language extensions that are
not expressible in the (safe) surface language, usually in the form of code written
in C or assembly. Just as Rust relies on the correctness of its unsafe code,
the safety of those languages relies on the correctness of those extensions.
Rust is different in that it doesn’t have a separate extension language, but
instead allows extensions to be written in what amounts to a dialect of Rust
(unsafe Rust). This allows much closer integration between the safe and unsafe
code, which in turn reduces the likelihood of errors due to impedance mismatches
at the interface between the two, or due to developers being familiar
with one but not the other. The closer integration also makes it easier to write
tools that analyze the correctness of the unsafe code’s interaction with the safe
code, as exemplified by tools like Miri. And since unsafe Rust continues to be
subject to the borrow checker for any operation that isn’t explicitly unsafe, there
remain many safety checks in place that aren’t present when developers must
drop down to a language like C.
Summary
In this chapter, we’ve walked through the powers that come with the unsafe
keyword and the responsibilities we accept by leveraging those powers. We
also talked about the consequences of writing unsafe unsafe code, and how
you really should be thinking about unsafe as a way to swear to the compiler
that you’ve manually checked that the indicated code is still safe. In the
next chapter, we’ll jump into concurrency in Rust and see how you can get
all those cores on your shiny new computer to pull in the same direction!

### 10 CONCURRENCY (ANdD PARALLELISM)

With this chapter I hope to provide you
with all the information and tools you’ll
need to take effective advantage of concurrency
in your Rust programs, to implement
support for concurrent use in your libraries, and to
use Rust’s concurrency primitives correctly. I won’t
directly teach you how to implement a concurrent
data structure or write a high-performance concurrent
application. Instead, my goal is to give you sufficient
understanding of the underlying mechanisms
that you’re equipped to wield them yourself for whatever
you may need them for.
Concurrency comes in three flavors: single-thread concurrency (like
with async/await, as we discussed in Chapter 8), single-core multithreaded
concurrency, and multicore concurrency, which yields true parallelism.
168 Chapter 10
Each flavor allows the execution of concurrent tasks in your program to be
interleaved in different ways. There are even more subflavors if you take the
details of operating system scheduling and preemption into account, but we
won’t get too deep into that.
At the type level, Rust represents only one aspect of concurrency: multithreading.
Either a type is safe for use by more than one thread, or it is not.
Even if your program has multiple threads (and so is concurrent) but only
one core (and so is not parallel), Rust must assume that if there are multiple
threads, there may be parallelism. Most of the types and techniques we’ll be
talking about apply equally whether two threads actually execute in parallel
or not, so to keep the language simple, I’ll be using the word concurrency
in the informal sense of “things running more or less at the same time”
throughout this chapter. When the distinction is important, I’ll call that out.
What’s particularly neat about Rust’s approach to type-based safe
multithreading
is that it is not a feature of the compiler, but rather a library
feature that developers can extend to develop sophisticated concurrency
contracts. Since thread safety is expressed in the type system through Send
and Sync implementations and bounds, which propagate all the way out
to application code, the thread safety of the entire program is checked
through type checking alone.
The Rust Programming Language already covers most of the basics when it
comes to concurrency, including the Send and Sync traits, Arc and Mutex, and
channels. I therefore won’t reiterate much of that here, except where it’s
worth repeating something specifically in the context of some other topic.
Instead, we’ll look at what makes concurrency difficult and some common
concurrency patterns intended to deal with those difficulties. We’ll also
explore how concurrency and asynchrony interact (and how they don’t)
before diving into how to use atomic operations to implement lower-level
concurrent operations. Finally, I’ll close out the chapter with some advice
for how to retain your sanity when working with concurrent code.
The Trouble with Concurrency
Before we dive into good patterns for concurrent programming and the
details of Rust’s concurrency mechanisms, it’s worth taking some time to
understand why concurrency is challenging in the first place. That is, why
do we need special patterns and mechanisms for concurrent code?
Correctness
The primary difficulty in concurrency is coordinating access—in particular,
write access—to a resource that is shared among multiple threads. If lots of
threads want to share a resource solely for the purposes of reading it, then
that’s usually easy: you stick it in an Arc or place it in something you can
get a &'static to, and you’re all done. But once any thread wants to write,
all sorts of problems arise, usually in the form of data races. Briefly, a data
race occurs when one thread updates shared state while a second thread is
also accessing that state, either to read it or to update it. Without additional
Concurrency (and Parallelism) 169
safeguards in place, the second thread may read partially overwritten state,
clobber parts of what the first thread wrote, or fail to see the first thread’s
write at all! In general, all data races are considered undefined behavior.
Data races are a part of a broader class of problems that primarily,
though not exclusively, occur in a concurrent setting: race conditions. A race
condition occurs whenever multiple outcomes are possible from a sequence
of instructions, depending on the relative timing of other events in the
system. These events can be threads executing a particular piece of code,
a timer going off, a network packet coming in, or any other time-variable
occurrence. Race conditions, unlike data races, are not inherently bad,
and are not considered undefined behavior. However, they are a breeding
ground for bugs when particularly peculiar races occur, as you’ll see
throughout this chapter.
Performance
Often, developers introduce concurrency into their programs in the
hope of increasing performance. Or, to be more precise, they hope that
concurrency will enable them to perform more operations per second in
aggregate by taking advantage of more hardware resources. This can be
done on a single core by having one thread run while another is waiting,
or across multiple cores by having threads do work simultaneously, one on
each core, that would otherwise happen serially on one core. Most developers
are referring to the latter kind of performance gain when they talk
about concurrency, which is often framed in terms of scalability. Scalability
in this context means “the performance of this program scales with the
number of cores,” implying that if you give your program more cores, its
performance improves.
While achieving such a speedup is possible, it’s harder than it seems.
The ultimate goal in scalability is linear scalability, where doubling the number
of cores doubles the amount of work your program completes per unit
of time. Linear scalability is also often called perfect scalability. However, in
reality, few concurrent programs achieve such speedups. Sublinear scaling
is more common, where the throughput increases nearly linearly as you go
from one core to two, but adding more cores yields diminishing returns.
Some programs even experience negative scaling, where giving the program
access to more cores reduces throughput, usually because the many threads
are all contending for some shared resource.
It might help to think of a group of people trying to pop all the bubbles
on a piece of bubble wrap—adding more people helps initially, but at some
point you get diminishing returns as the crowding makes any one person’s
job harder. If the humans involved are particularly ineffective, your group
may end up standing around discussing who should pop next and pop no
bubbles at all! This kind of interference among tasks that are supposed to
execute in parallel is called contention and is the archnemesis of scaling well.
Contention can arise in a number of ways, but the primary offenders are
mutual exclusion, shared resource exhaustion, and false sharing.

170 Chapter 10
Mutual Exclusion
When only a single concurrent task is allowed to execute a particular piece
of code at any one time, we say that execution of that segment of code is
mutually exclusive—if one thread executes it, no other thread can do so
at the same time. The archetypal example of this is a mutual exclusion
lock, or mutex, which explicitly enforces that only one thread gets to enter
a particular critical section of your program code at any one time. Mutual
exclusion can also happen implicitly, however. For example, if you spin up
a thread to manage a shared resource and send jobs to it over an mpsc channel,
that thread effectively implements mutual exclusion, since only one
such job gets to execute at a time.
Mutual exclusion can also occur when invoking operating system or
library calls that internally enforce single-threaded access to a critical section.
For example, for many years, the standard memory allocator required
mutual exclusion for some allocations, which made memory allocation an
operation that incurred significant contention in otherwise highly parallel
programs. Similarly, many operating system operations that may seem like
they should be independent, such as creating two files with different names
in the same directory, may end up having to happen sequentially inside the
kernel.
**NOTE** Scalable concurrent allocations is the raison d’être for the jemalloc memory allocator!
Mutual exclusion is the most obvious barrier to parallel speedup since,
by definition, it forces serial execution of some portion of your program.
Even if you make the remainder of your program scale with the number of
cores perfectly, the total speedup you can achieve is limited by the length of
the mutually exclusive, serial section. Be mindful of your mutually exclusive
sections, and seek to restrict them to only where strictly necessary.
**NOTE** For the theoretically minded, the limits on the achievable speedup as a result of mutually
exclusive sections of code can be computed using Amdahl’s law.
Shared Resource Exhaustion
Unfortunately, even if you achieve perfect concurrency within your tasks,
the environment those tasks need to interact with may itself not be perfectly
scalable. The kernel can handle only so many sends on a given TCP socket
per second, the memory bus can do only so many reads at once, and your
GPU has a limited capacity for concurrency. There’s no cure for this. The
environment is usually where perfect scalability falls apart in practice, and
fixes for such cases tend to require substantial re-engineering (or even new
hardware!), so we won’t talk much more about this topic in this chapter. Just
remember that scalability is rarely something you can “achieve,” and more
something you just strive for.

Concurrency (and Parallelism) 171
False Sharing
False sharing occurs when two operations that shouldn’t contend with one
another contend anyway, preventing efficient simultaneous execution. This
usually happens because the two operations happen to intersect on some
shared resource even though they use unrelated parts of that resource.
The simplest example of this is lock oversharing, where a lock guards
some composite state, and two operations that are otherwise independent
both need to take the lock to update their particular parts of the state.
This in turn means the operations must execute serially instead of in parallel.
In some cases it’s possible to split the single lock into two, one for each
of the disjoint parts, which enables the operations to proceed in parallel.
However, it’s not always straightforward to split a lock like this—the state
may share a single lock because some third operation needs to lock over
all the parts of the state. Usually you can still split the lock, but you have to
be careful about the order in which different threads take the split locks to
avoid deadlocks that can occur when two operations attempt to take them
in different orders (look up the “dining philosophers problem,” if you’re
curious). Alternatively, for some problems, you may be able to avoid the
critical section entirely by using a lock-free version of the underlying algorithm,
though those are also tricky to get right. Ultimately, false sharing is a
hard problem to solve, and there isn’t a single catchall solution—but identifying
the problem is a good start.
A more subtle example of false sharing occurs on the CPU level, as we
discussed briefly in Chapter 2. The CPU internally operates on memory in
terms of cache lines—longer sequences of consecutive bytes in memory—
rather than individual bytes, to amortize the cost of memory accesses. For
example, on most Intel processors, the cache line size is 64 bytes. This
means that every memory operation really ends up reading or writing some
multiple of 64 bytes. The false sharing comes into play when two cores
want to update the value of two different bytes that happen to fall on the
same cache line; those updates must execute sequentially even though the
updates are logically disjoint.
This might seem too low-level to matter, but in practice this kind of
false sharing can decimate the parallel speedup of an application. Imagine
that you allocate an array of integer values to indicate how many operations
each thread has completed, but the integers all fall within the same cache
line—now, all your otherwise parallel threads will contend on that one
cache line for every operation they perform. If the operations are relatively
quick, most of your execution time may end up being spent contending on
those counters!
The trick to avoiding false cache line sharing is to pad your values so
that they are the size of a cache line. That way, two adjacent values always
fall on different cache lines. But of course, this also inflates the size of your
data structures, so use this approach only when benchmarks indicate a
problem.
172 Chapter 10
THE COST OF SCALABILITY
A somewhat orthogonal aspect of concurrency that you should be mindful of is
the cost of introducing concurrency in the first place. Compilers are really good
at optimizing single-threaded code—they’ve been doing it for a long time, after
all—and single-threaded code tends to get away with fewer expensive safeguards
(like locks, channels, or atomic instructions) than concurrent code can.
In aggregate, the various costs of concurrency can make a parallel program
slower than its single-threaded counterpart, given any number of cores! This is
why it’s important to measure both before and after you optimize and parallelize:
the results may surprise you.
If you’re curious about this topic, I highly recommend you read Frank
McSherry’s 2015 paper “Scalability! But at what COST?” (<https://www>
.frankmcsherry.org/assets/COST.pdf), which uncovers some particularly
egregious examples of “costly scaling.”
Concurrency Models
Rust has three patterns for adding concurrency to your programs that
you’ll come across fairly often: shared memory concurrency, worker pools,
and actors. Going through every way you could add concurrency in detail
would take a book of its own, so here I’ll focus on just these three patterns.
Shared Memory
Shared memory concurrency is, conceptually, very straightforward: the
threads cooperate by operating on regions of memory shared between
them. This might take the form of state guarded by a mutex or stored in
a hash map with support for concurrent access from many threads. The
many threads may be doing the same task on disjoint pieces of data, such
as if many threads perform some function over disjoint subranges of a Vec,
or they may be performing different tasks that require some shared state,
such as in a database where one thread handles user queries to a table
while another optimizes the data structures used to store that table in the
background.
When you use shared memory concurrency, your choice of data structures
is significant, especially if the threads involved need to cooperate
very closely. A regular mutex might prevent scaling beyond a very small
number of cores, a reader/writer lock might allow many more concurrent
reads at the cost of slower writes, and a sharded reader/writer lock might
allow perfectly scalable reads at the cost of making writes highly disruptive.
Similarly, some concurrent hash maps aim for good all-round performance
while others specifically target, say, concurrent reads where writes are rare.
In general, in shared memory concurrency, you want to use data structures
Concurrency (and Parallelism) 173
that are specifically designed for something as close to your target use case
as possible, so that you can take advantage of optimizations that trade off
performance aspects your application does not care about for those it does.
Shared memory concurrency is a good fit for use cases where threads
need to jointly update some shared state in a way that does not commute.
That is, if one thread has to update the state s with some function f, and
another has to update the state with some function g, and f(g(s)) != g(f(s)),
then shared memory concurrency is likely necessary. If that is not the case,
the other two patterns are likely better fits, as they tend to lead to simpler
and more performant designs.
**NOTE** Some problems have known algorithms that can provide concurrent shared memory
operations without the use of locks. As the number of cores grows, these lock-free
algorithms may scale better than lock-based algorithms, though they also often have
slower per-core performance due to their complexity. As always with performance matters,
benchmark first, then look for alternative solutions.
Worker Pools
In the worker pool model, many identical threads receive jobs from a
shared job queue, which they then execute entirely independently. Web
servers, for example, often have a worker pool handling incoming connections,
and multithreaded runtimes for asynchronous code tend to use a
worker pool to collectively execute all of an application’s futures (or, more
accurately, its top-level tasks).
The lines between shared memory concurrency and worker pools are
often blurry, as worker pools tend to use shared memory concurrency to
coordinate how they take jobs from the queue and how they return incomplete
jobs back to the queue. For example, say you’re using the data parallelism
library rayon to perform some function over every element of a vector
in parallel. Behind the scenes rayon spins up a worker pool, splits the vector
into subranges, and then hands out subranges to the threads in the pool.
When a thread in the pool finishes a range, rayon arranges for it to start
working on the next unprocessed subrange. The vector is shared among all
the worker threads, and the threads coordinate through a shared memory
queue–like data structure that supports work stealing.
Work stealing is a key feature of most worker pools. The basic premise
is that if one thread finishes its work early, and there’s no more unassigned
work available, that thread can steal jobs that have already been assigned to
a different worker thread but haven’t been started yet. Not all jobs take the
same amount of time to complete, so even if every worker is given the same
number of jobs, some workers may end up finishing their jobs more quickly
than others. Rather than sit around and wait for the threads that drew
longer-running jobs to complete, those threads that finish early should help
the stragglers so the overall operation is completed sooner.
It’s quite a task to implement a data structure that supports this kind
of work stealing without incurring significant overhead from threads constantly
trying to steal work from one another, but this feature is vital to a
174 Chapter 10
high-performance worker pool. If you find yourself in need of a worker
pool, your best bet is usually to use one that has already seen a lot of work
go into it, or at least reuse data structures from an existing one, rather than
to write one yourself from scratch.
Worker pools are a good fit when the work that each thread performs is
the same, but the data it performs it on varies. In a rayon parallel map operation,
every thread performs the same map computation; they just perform
it on different subsets of the underlying data. In a multithreaded asynchronous
runtime, each thread simply calls Future::poll; they just call it on different
futures. If you start having to distinguish between the threads in your
thread pool, a different design is probably more appropriate.
CONNECTION POOLS
A connection pool is a shared memory construct that keeps a set of established
connections and hands them out to threads that need a connection. It’s a common
design pattern in libraries that manage connections to external services.
If a thread needs a connection but one isn’t available, either a new connection
is established or the thread is forced to block. When a thread is done with a
connection, it returns that connection to the pool, and thus makes it available to
other threads that may be waiting.
Usually, the hardest task for a connection pool is managing connection
life cycles. A connection can be returned to the pool in whatever state it was
put in by the last thread that used it. The connection pool therefore has to make
sure any state associated with the connection, whether on the client or on the
server, has been reset so that when the connection is subsequently used by
another thread, that thread can act as though it was given a fresh, dedicated
connection.
Actors
The actor concurrency model is, in many ways, the opposite of the worker
pool model. Whereas the worker pool has many identical threads that
share a job queue, the actor model has many separate job queues, one
for each job “topic.” Each job queue feeds into a particular actor, which
handles all jobs that pertain to a subset of the application’s state. That state
might be a database connection, a file, a metrics collection data structure,
or any other structure that you can imagine many threads may need to be
able to access. Whatever it is, a single actor owns that state, and if some
task wants to interact with that state, it needs to send a message to the
owning actor summarizing the operation it wishes to perform. When the
owning actor receives that message, it performs the indicated action and
responds to the inquiring task with the result of the operation, if relevant.
Concurrency (and Parallelism) 175
Since the actor has exclusive access to its inner resource, no locks or other
synchronization mechanisms are required beyond what’s needed for the
messaging.
A key point in the actor pattern is that actors all talk to one another.
If, say, an actor that is responsible for logging needs to write to a file and a
database table, it might send off messages to the actors responsible for each
of those, asking them to perform the respective actions, and then proceed
to the next log event. In this way, the actor model more closely resembles a
web than spokes on a wheel—a user request to a web server might start as a
single request to the actor responsible for that connection but might transitively
spawn tens, hundreds, or even thousands of messages to actors deeper
in the system before the user’s request is satisfied.
Nothing in the actor model requires that each actor is its own thread. To
the contrary, most actor systems suggest that there should be a large number
of actors, and so each actor should map to a task rather than a thread. After
all, actors require exclusive access to their wrapped resources only when
they execute, and do not care whether they are on a thread of their own
or not. In fact, very frequently, the actor model is used in conjunction with
the worker pool model—for example, an application that uses the multithreaded
asynchronous runtime Tokio can spawn an asynchronous task for
each actor, and Tokio will then make the execution of each actor a job in its
worker pool. Thus, the execution of a given actor may move from thread to
thread in the worker pool as the actor yields and resumes, but every time the
actor executes it maintains exclusive access to its wrapped resource.
The actor concurrency model is well suited for when you have many
resources that can operate relatively independently, and where there is
little or no opportunity for concurrency within each resource. For example,
an operating system might have an actor responsible for each hardware
device, and a web server might have an actor for each backend database
connection. The actor model does not work so well if you need only a few
actors, if work is skewed significantly among the actors, or if some actors
grow large—in all of those cases, your application may end up being bottlenecked
on the execution speed of a single actor in the system. And since
actors each expect to have exclusive access to their little slice of the world,
you can’t easily parallelize the execution of that one bottleneck actor.
Asynchrony and Parallelism
As we discussed in Chapter 8, asynchrony in Rust enables concurrency
without parallelism—we can use constructs like selects and joins to have
a single thread poll multiple futures and continue when one, some, or all
of them complete. Because there is no parallelism involved, concurrency
with futures does not fundamentally require those futures to be Send. Even
spawning a future to run as an additional top-level task does not fundamentally
require Send, since a single executor thread can manage the polling of
many futures at once.
176 Chapter 10
However, in most cases, applications want both concurrency and parallelism.
For example, if a web application constructs a future for each incoming
connection and so has many active connections at once, it probably wants the
asynchronous executor to be able to take advantage of more than one core
on the host computer. That won’t happen naturally: your code has to explicitly
tell the executor which futures can run in parallel and which cannot.
In particular, two pieces of information must be given to the executor
to let it know that it can spread the work in the futures across a worker pool
of threads. The first is that the futures in question are Send—if they aren’t,
the executor is not allowed to send the futures to other threads for processing,
and no parallelism is possible; only the thread that constructed such
futures can poll them.
The second piece of information is how to split the futures into tasks
that can operate independently. This ties back to the discussion of tasks versus
futures from Chapter 8: if one giant Future contains a number of Future
instances that themselves correspond to tasks that can run in parallel, the
executor must still call poll on the top-level Future, and it must do so from
a single thread, since poll requires &mut self. Thus, to achieve parallelism
with futures, you have to explicitly spawn the futures you want to be able to
run in parallel. Also, because of the first requirement, the executor function
you use to do so will require that the passed-in Future is Send.
ASYNCHRONOUS SYNCHRONIZATION PRIMITIVES
Most of the synchronization primitives that exist for blocking code (think
std::sync) also have asynchronous counterparts. There are asynchronous variants
of channels, mutexes, reader/writer locks, barriers, and all sorts of other
similar constructs. We need these because, as discussed in Chapter 8, blocking
inside a future will hold up other work the executor may need to do, and so is
inadvisable.
However, the asynchronous versions of these primitives are often slower
than their synchronous counterparts because of the additional machinery
needed to perform the necessary wake-ups. For that reason, you may want to
use synchronous synchronization primitives even in asynchronous contexts whenever
the use does not risk blocking the executor. For example, while it’s generally
true that acquiring a Mutex might block for a long time, that might not be true for
a particular Mutex that, perhaps, is acquired only rarely, and only ever for short
periods of time. In that case, blocking for the short time until the Mutex becomes
available again might not actually cause any problems. You will want to make
sure that you never yield or perform other long-running operations while holding
the MutexGuard, but barring that you shouldn’t run into problems.
As always with such optimizations, though, make sure you measure first,
and choose only the synchronous primitive if it nets you significant performance
improvements. If it does not, the additional footguns introduced by using a synchronous
primitive in an asynchronous context are probably not worth it.

Concurrency (and Parallelism) 177
Lower-Level Concurrency
The standard library provides the std::sync::atomic module, which provides
access to the underlying CPU primitives, higher-level constructs like
channels and mutexes are built with. These primitives come in the form
of atomic types with names starting with Atomic—AtomicUsize, AtomicI32,
AtomicBool, AtomicPtr, and so on—the Ordering type, and two functions
called fence and compiler_fence. We’ll look at each of these over the next few
sections.
These types are the blocks used to build any code that has to communicate
between threads. Mutexes, channels, barriers, concurrent hash tables,
lock-free stacks, and all other synchronization constructs ultimately rely on
these few primitives to do their jobs. They also come in handy on their own
for lightweight cooperation between threads where heavyweight synchronization
like a mutex is excessive—for example, to increment a shared counter
or set a shared Boolean to true.
The atomic types are special in that they have defined semantics for
what happens when multiple threads try to access them concurrently. These
types all support (mostly) the same API: load, store, fetch_*, and compare_
exchange. In the rest of this section, we’ll look at what those do, how to use
them correctly, and what they’re useful for. But first, we have to talk about
low-level memory operations and memory ordering.
Memory Operations
Informally, we often refer to accessing variables as “reading from” or “writing
to” memory. In reality, a lot of machinery between code uses a variable
and the actual CPU instructions that access your memory hardware. It’s
important to understand that machinery, at least at a high level, in order to
understand how concurrent memory accesses behave.
The compiler decides what instructions to emit when your program
reads the value of a variable or assigns a new value to it. It is permitted to
perform all sorts of transformations and optimizations on your code and
may end up reordering your program statements, eliminating operations
it deems redundant, or using CPU registers rather than actual memory to
store intermediate computations. The compiler is subject to a number of
restrictions on these transformations, but ultimately only a subset of your
variable accesses actually end up as memory access instructions.
At the CPU level, memory instructions come in two main shapes: loads
and stores. A load pulls bytes from a location in memory into a CPU register,
and a store stores bytes from a CPU register into a location in memory.
Loads and stores operate on small chunks of memory at a time: usually
8 bytes or less on modern CPUs. If a variable access spans more bytes than
can be accessed with a single load or store, the compiler automatically turns
it into multiple load or store instructions, as appropriate. The CPU also has
some leeway in how it executes a program’s instructions to make better use
of the hardware and improve program performance. For example, modern
CPUs often execute instructions in parallel, or even out of order, when they
don’t have dependencies on each other. There are also several layers of caches
178 Chapter 10
between each CPU and your computer’s DRAM, which means that a load
of a given memory location may not necessarily see the latest store to that
memory location, going by wall-clock time.
In most code, the compiler and CPU are permitted to transform the
code only in ways that don’t affect the semantics of the resulting program,
so these transformations are invisible to the programmer. However, in the
context of parallel execution, these transformations can have a significant
impact on application behavior. Therefore, CPUs typically provide multiple
different variations of the load and store instructions, each with different
guarantees about how the CPU may reorder them and how they may be
interleaved with parallel operations on other CPUs. Similarly, compilers (or
rather, the language the compiler compiles) provide different annotations
you can use to force particular execution constraints for some subset of
their memory accesses. In Rust, those annotations come in the form of the
atomic types and their methods, which we’ll spend the rest of this section
picking apart.
Atomic Types
Rust’s atomic types are so called because they can be accessed atomically—
that is, the value of an atomic-type variable is written all at once and will
never be written using multiple stores, guaranteeing that a load of that variable
cannot observe that only some of the bytes composing the value have
changed while others have not (yet). This is easiest understood by way of
contrast with non-atomic types. For example, reassigning a new value to a
tuple of type (i64, i64) typically requires two CPU store instructions, one
for each 8-byte value. If one thread were to perform both of those stores,
another thread could (if we ignore the borrow checker for a second) read
the tuple’s value after the first store but before the second, and thus end up
with an inconsistent view of the tuple’s value. It would end up reading the
new value for the first element and the old value for the second element, a
value that was never actually stored by any thread.
The CPU can atomically access values only of certain sizes, so there are
only a few atomic types, all of which live in the atomic module. Each atomic
type is of one of the sizes the CPU supports atomic access to, with multiple
variations for things like whether the value is signed and to differentiate
between an atomic usize and a pointer (which is of the same size as usize).
Furthermore, the atomic types have explicit methods for loading and storing
the values they hold, and a handful of more complex methods we’ll get
back to later, so that the mapping between the code the programmer writes
and the resulting CPU instructions is clearer. For example, AtomicI32::load
performs a single load of a signed 32-bit value, and AtomicPtr::store performs
a single store of a pointer-sized (64 bits on a 64-bit platform) value.
Memory Ordering
Most of the methods on the atomic types take an argument of type Ordering,
which dictates the memory ordering restrictions the atomic operation is
subject to. Across different threads, loads and stores of an atomic value
Concurrency (and Parallelism) 179
may be sequenced by the compiler and CPU only in interleavings that are
compatible with the requested memory ordering of each of the atomic
operations on that atomic value. Over the next few sections, we’ll see some
examples of why control over the ordering is important and necessary to get
the expected semantics out of the compiler and CPU.
Memory ordering often comes across as counterintuitive, because we
humans like to read programs from top to bottom and imagine that they
execute line by line—but that’s not how the code actually executes when
it hits the hardware. Memory accesses can be reordered, or even entirely
elided, and writes on one thread may not immediately be visible to other
threads, even if later writes in program order have already been observed.
Think of it like this: each memory location sees a sequence of modifications
coming from different threads, and the sequences of modifications for
different memory locations are independent. If two threads T1 and T2 both
write to memory location M, then even if T1 executed first as measured by
a user with a stopwatch, T2’s write to M may still appear to have happened
first for M absent any other constraints between the two threads’ execution.
Essentially, the computer does not take wall-clock time into account when it determines
the value of a given memory location—all that matter are the execution
constraints the programmer puts on what constitutes a valid execution.
For example, if T1 writes to M and then spawns thread T2, which then
writes to M, the computer must recognize T1’s write as having happened
first because T2’s existence depends on T1.
If that’s hard to follow, don’t fret—memory ordering can be mindbending,
and language specifications tend to use very precise but not very
intuitive wording to describe it. We can construct a mental model that’s
easier to grasp, if a little simplified, by instead focusing on the underlying
hardware architecture. Very basically, your computer memory is structured
as a treelike hierarchy of storage where the leaves are CPU registers and
the roots are the storage on your physical memory chips, often called main
memory. Between the two are several layers of caches, and different layers
of the hierarchy can reside on different pieces of hardware. When a
thread performs a store to a memory location, what really happens is that
the CPU starts a write request for the value in a given CPU register that
then has to make its way up the memory hierarchy toward main memory.
When a thread performs a load, the request flows up the hierarchy until it
hits a layer that has the value available, and returns from there. Herein lies
the problem: writes aren’t visible everywhere until all caches of the written
memory location have been updated, but other CPUs can execute instructions
against the same memory location at the same time, and weirdness
ensues. Memory ordering, then, is a way to request precise semantics for
what happens when multiple CPUs access a particular memory location for
a particular operation.
With this in mind, let’s take a look at the Ordering type, which is the
primary mechanism by which we, as programmers, can dictate additional
constraints on what concurrent executions are valid.
Ordering is defined as an enum with the variants shown in Listing 10-1.
180 Chapter 10
enum Ordering {
Relaxed,
Release,
Acquire,
AcqRel,
SeqCst
}
Listing 10-1: The definition of Ordering
Each of these places different restrictions on the mapping from source
code to execution semantics, and we’ll explore each one in turn in the
remainder of this section.
Relaxed Ordering
Relaxed ordering essentially guarantees nothing about concurrent
access to the value beyond the fact that the access is atomic. In particular,
relaxed ordering gives no guarantees about the relative ordering of memory
accesses across different threads. This is the weakest form of memory
ordering. Listing 10-2 shows a simple program in which two threads access
two atomic variables using Ordering::Relaxed.
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
1 let r1 = Y.load(Ordering::Relaxed);
2 X.store(r1, Ordering::Relaxed);
});
let t2 = spawn(|| {
3 let r2 = X.load(Ordering::Relaxed);
4 Y.store(true, Ordering::Relaxed)
});
Listing 10-2: Two racing threads with Ordering::Relaxed
Looking at the thread spawned as t2, you might expect that r2 can
never be true, since all values are false until the same thread assigns true to
Y on the line after reading X. However, with a relaxed memory ordering, that
outcome is completely possible. The reason is that the CPU is allowed to
reorder the loads and stores involved. Let’s walk through exactly what happens
here to make r2 = true possible.
First, the CPU notices that 4 doesn’t have to happen after 3, since 4
doesn’t use any output or side effect of 3. That is, 4 has no execution dependency
on 3. So, the CPU decides to reorder them for _waves hands_ reasons
that’ll make your program go faster. The CPU thus goes ahead and executes 4
first, setting Y = true, even though 3 hasn’t run yet. Then, t2 is put to sleep
by the operating system and thread t1 executes a few instructions, or t1 simply
executes on another core. In t1, the compiler must indeed run 1 first and
then 2, since 2 depends on the value read in 1. Therefore, t1 reads true from
Concurrency (and Parallelism) 181
Y (written by 4) into r1 and then writes that back to X. Finally, t2 executes 3,
which reads X and gets true, as was written by 2.
The relaxed memory ordering allows this execution because it imposes
no additional constraints on concurrent execution. That is, under relaxed
memory ordering, the compiler must ensure only that execution dependencies
on any given thread are respected (just as if atomics weren’t involved);
it need not make any promises about the interleaving of concurrent operations.
Reordering 3 and 4 is permitted for a single-threaded execution, so
it is permitted under relaxed ordering as well.
In some cases, this kind of reordering is fine. For example, if you have a
counter that just keeps track of metrics, it doesn’t really matter when exactly
it executes relative to other instructions, and Ordering::Relaxed is fine. In
other cases, this could be disastrous: say, if your program uses r2 to figure
out if security protections have already been set up, and thus ends up erroneously
believing that they already have been.
You don’t generally notice this reordering when writing code that
doesn’t make fancy use of atomics—the CPU has to promise that there is
no observable difference between the code as written and what each thread
actually executes, so everything seems like it runs in order just as you wrote
it. This is referred to as respecting program order or evaluation order; the
terms are synonyms.
Acquire/Release Ordering
At the next step up in the memory ordering hierarchy, we have
Ordering::Acquire, Ordering::Release, and Ordering::AcqRel (acquire plus
release). At a high level, these establish an execution dependency between
a store in one thread and a load in another and then restrict how operations
can be reordered with respect to that load and store. Crucially, these
dependencies not only establish a relationship between a store and a load of
a single value, but also put ordering constraints on other loads and stores in
the threads involved. This is because every execution must respect the program
order; if a load in thread B has a dependency on some store in thread
A (the store in A must execute before the load in B), then any read or write
in B after that load must also happen after that store in A.
**NOTE** The Acquire memory ordering can be applied only to loads, Release only to stores,
and AcqRel only to operations that both load and store (like fetch_add).
Concretely, these memory orderings place the following restrictions on
execution:

1. Loads and stores cannot be moved forward past a store with
Ordering::Release.
2. Loads and stores cannot be moved back before a load with
Ordering::Acquire.
3. An Ordering::Acquire load of a variable must see all stores that happened
before an Ordering::Release store that stored what the load loaded.
182 Chapter 10
To see how these memory orderings change things, Listing 10-3 shows
Listing 10-2 again but with the memory ordering swapped out for Acquire
and Release.
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
let r1 = Y.load(Ordering::Acquire);
X.store(r1, Ordering::Release);
});
let t2 = spawn(|| {
1 let r2 = X.load(Ordering::Acquire);
2 Y.store(true, Ordering::Release)
});
Listing 10-3: Listing 10-2 with Acquire/Release memory ordering
These additional restrictions mean that it is no longer possible for t2 to
see r2 = true. To see why, consider the primary cause of the weird outcome
in Listing 10-2: the reordering of 1 and 2. The very first restriction, on
stores with Ordering::Release, dictates that we cannot move 1 below 2, so
we’re all good!
But these rules are useful beyond this simple example. For example,
imagine that you implement a mutual exclusion lock. You want to make
sure that any loads and stores a thread runs while it holds the lock are executed
only while it’s actually holding the lock, and visible to any thread that
takes the lock later. This is exactly what Release and Acquire enable you to
do. By performing a Release store to release the lock and an Acquire load to
acquire the lock, you can guarantee that the loads and stores in the critical
section are never moved to before the lock was actually acquired or to after
the lock was released!
**NOTE** On some CPU architectures, like x86, Acquire/Release ordering is guaranteed
by the hardware, and there is no additional cost to using Ordering::Release and
Ordering::Acquire over Ordering::Relaxed. On other architectures that is not the
case, and your program may see speedups if you switch to Relaxed for atomic operations
that can tolerate the weaker memory ordering guarantees.
Sequentially Consistent Ordering
Sequentially consistent ordering (Ordering::SeqCst) is the strongest memory
ordering we have access to. Its exact guarantees are somewhat hard to nail
down, but very broadly, it requires not only that each thread sees results
consistent with Acquire/Release, but also that all threads see the same ordering
as one another. This is best seen by way of contrast with the behavior of
Acquire and Release. Specifically, Acquire/Release ordering does not guarantee
that if two threads A and B atomically load values written by two other
threads X and Y, A and B will see a consistent pattern of when X wrote
relative to Y. That’s fairly abstract, so consider the example in Listing 10-4,
Concurrency (and Parallelism) 183
which shows a case where Acquire/Release ordering can produce unexpected
results. Afterwards, we’ll see how sequentially consistent ordering avoids
that particular unexpected outcome.
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
static Z: AtomicI32 = AtomicI32::new(0);
let t1 = spawn(|| {
X.store(true, Ordering::Release);
});
let t2 = spawn(|| {
Y.store(true, Ordering::Release);
});
let t3 = spawn(|| {
while (!X.load(Ordering::Acquire)) {}
1 if (Y.load(Ordering::Acquire)) {
Z.fetch_add(1, Ordering::Relaxed); }
});
let t4 = spawn(|| {
while (!Y.load(Ordering::Acquire)) {}
2 if (X.load(Ordering::Acquire)) {
Z.fetch_add(1, Ordering::Relaxed); }
});
Listing 10-4: Weird results with Acquire/Release ordering
The two threads t1 and t2 set X and Y to true, respectively. Thread t3
waits for X to be true; once X is true, it checks if Y is true and, if so, adds 1 to
Z. Thread t4 instead waits for Y to become true, and then checks if X is true
and, if so, adds 1 to Z. At this point the question is: what are the possible
values for Z after all the threads terminate? Before I show you the answer,
try to work your way through it given the definitions of Release and Acquire
ordering in the previous section.
First, let’s recap the conditions under which Z is incremented. Thread t3
increments Z if it sees that Y is true after it observes that X is true, which can
happen only if t2 runs before t3 evaluates the load at 1. Conversely, thread
t4 increments Z if it sees that X is true after it observes that Y is true, so only if
t1 runs before t4 evaluates the load at 2. To simplify the explanation, let’s
assume for now that each thread runs to completion once it runs.
Logically, then, Z can be incremented twice if the threads run in the
order 1, 2, 3, 4—both X and Y are set to true, and then t3 and t4 run to find
that their conditions for incrementing Z are met. Similarly, Z can trivially
be incremented just once if the threads run in the order 1, 3, 2, 4. This satisfies
t4’s condition for incrementing Z, but not t3’s. Getting Z to be 0, however,
seems impossible: if we want to prevent t3 from incrementing Z, t2 has
to run after t3. Since t3 runs only after t1, that implies that t2 runs after t1.
However, t4 won’t run until after t2 has run, so t1 must have run and set X
to true by the time t4 runs, and so t4 will increment Z.
Our inability to get Z to be 0 stems mostly from our human inclination
for linear explanations; this happened, then this happened, then this
184 Chapter 10
happened. Computers aren’t limited in the same way and have no need to
box all events into a single global order. There’s nothing in the rules for
Release and Acquire that says that t3 must observe the same execution order
for t1 and t2 as t4 observes. As far as the computer is concerned, it’s fine
to let t3 observe t1 as having executed first, while having t4 observe t2 as
having executed first. With that in mind, an execution in which t3 observes
that Y is false after it observes that X is true (implying that t2 runs after t1),
while in the same execution t4 observes that X is false after it observes that
Y is true (implying that t2 runs before t1), is completely reasonable, even if
that seems outrageous to us mere humans.
As we discussed earlier, Acquire/Release requires only that an
Ordering::Acquire load of a variable must see all stores that happened before
an Ordering::Release store that stored what the load loaded. In the ordering
just discussed, the computer did uphold that property: t3 sees X == true,
and indeed sees all stores by t1 prior to it setting X = true—there are none.
It also sees Y == false, which was stored by the main thread at program
startup, so there aren’t any relevant stores to be concerned with. Similarly,
t4 sees Y = true and also sees all stores by t2 prior to setting Y = true—again,
there are none. It also sees X == false, which was stored by the main thread
and has no preceding store. No rules are broken, yet it just seems wrong
somehow.
Our intuitive expectation was that we could put the threads in some
global order to make sense of what every thread saw and did, but that was
not the case for Acquire/Release ordering in this example. To achieve something
closer to that intuitive expectation, we need sequential consistency.
Sequential consistency requires all the threads taking part in an atomic
operation to coordinate to ensure that what each thread observes corresponds
to (or at least appears to correspond to) some single, common execution
order. This makes it easier to reason about but also makes it costly.
Atomic loads and stores marked with Ordering::SeqCst instruct the compiler
to take any extra precautions (such as using special CPU instructions)
needed to guarantee sequential consistency for those loads and stores. The
exact formalism around this is fairly convoluted, but sequential consistency
essentially ensures that if you looked at all the related SeqCst operations
from across all your threads, you could put the thread executions in some
order so that the values that were loaded and stored would all match up.
If we replaced all the memory ordering arguments in Listing 10-4 with
SeqCst, Z could not possibly be 0 after all the threads have exited, just as we
originally expected. Under sequential consistency, it must be possible to say
either that t1 definitely ran before t2 or that t2 definitely ran before t1, so
the execution where t3 and t4 see different orders is not allowed, and thus Z
cannot be 0.
Compare and Exchange
In addition to load and store, all of Rust’s atomic types provide a method
called compare_exchange. This method is used to atomically and conditionally
replace a value. You provide compare_exchange with the last value you
Concurrency (and Parallelism) 185
observed for an atomic variable and the new value you want to replace the
original value with, and it will replace the value only if it is still the same as
it was when you last observed it. To see why this is important, take a look
at the (broken) implementation of a mutual exclusion lock in Listing 10-5.
This implementation keeps track of whether the lock is held in the static
atomic variable LOCK. We use the Boolean value true to represent that the
lock is held. To acquire the lock, a thread waits for LOCK to be false, then
sets it to true again; it then enters its critical section and sets LOCK to false
to release the lock when its work (f) is done.
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
// Wait for the lock to become free (false).
while LOCK.load(Ordering::Acquire)
{ /* .. TODO: avoid spinning .. _/ }
// Store the fact that we hold the lock.
LOCK.store(true, Ordering::Release);
// Call f while holding the lock.
f();
// Release the lock.
LOCK.store(false, Ordering::Release);
}
Listing 10-5: An incorrect implementation of a mutual exclusion lock
This mostly works, but it has a terrible flaw—two threads might both
see LOCK == false at the same time and both leave the while loop. Then they
both set LOCK to true and both enter the critical section, which is exactly
what the mutex function was supposed to prevent!
The issue in Listing 10-5 is that there is a gap between when we load
the current value of the atomic variable and when we subsequently update
it, during which another thread might get to run and read or touch its
value. It is exactly this problem that compare_exchange solves—it swaps out the
value behind the atomic variable only if its value still matches the previous
read, and otherwise notifies you that the value has changed. Listing 10-6
shows the corrected implementation using compare_exchange.
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
// Wait for the lock to become free (false).
loop {
let take = LOCK.compare_exchange(
false,
true,
Ordering::AcqRel,
Ordering::Relaxed
);
match take {
Ok(false) => break,
Ok(true) | Err(false) => unreachable!(),
186 Chapter 10
Err(true) => { /_ .. TODO: avoid spinning .. _/ }
}
}
// Call f while holding the lock.
f();
// Release the lock.
LOCK.store(false, Ordering::Release);
}
Listing 10-6: A corrected implementation of a mutual exclusion lock
This time around, we use compare_exchange in the loop, and it takes care
of both checking that the lock is currently not held and storing true to take
the lock as appropriate. This happens through the first and second arguments
to compare_exchange, respectively: in this case, false and then true. You
can read the invocation as “Store true only if the current value is false.”
The compare_exchange method returns a Result that indicates either that the
value was successfully updated (Ok) or that it could not be updated (Err).
In either case, it also returns the current value. This isn’t too useful with
an AtomicBool since we know what the value must be if the operation failed,
but for something like an AtomicI32, the updated current value will let you
quickly recompute what to store and then try again without having to do
another load.
**NOTE** Note that compare_exchange checks only whether the value is the same as the one that
was passed in as the current value. If some other thread modifies the atomic variable’s
value and then resets it to the original value again, a compare_exchange on that variable
will still succeed. This is often referred to as the A-B-A problem.
Unlike simple loads and stores, compare_exchange takes two Ordering arguments.
The first is the “success ordering,” and it dictates what memory
ordering should be used for the load and store that the compare_exchange
represents in the case that the value was successfully updated. The second
is the “failure ordering,” and it dictates the memory ordering for the load
if the loaded value does not match the expected current value. These two
orderings are kept separate so that the developer can give the CPU leeway
to improve execution performance by reordering loads and stores on failure
when appropriate, but still get the correct ordering on success. In this
case, it’s okay to reorder loads and stores across failed iterations of the lock
acquisition loop, but it’s not okay to reorder loads and stores inside the critical
section in such a way that they end up outside of it.
Even though its interface is simple, compare_exchange is a very powerful
synchronization primitive—so much so that it’s been theoretically proven
that you can build all other distributed consensus primitives using only
compare_exchange! For that reason, it is the workhorse of many, if not most,
synchronization constructs when you really dig into the implementation
details.
Be aware, though, that a compare_exchange requires that a single CPU has
exclusive access to the underlying value, and it is therefore a form of mutual
exclusion at the hardware level. This in turn means that compare_exchange
Concurrency (and Parallelism) 187
can quickly become a scalability bottleneck: only one CPU can make progress
at a time, so there’s a portion of your code that will not scale with the
number of cores. In fact, it’s probably worse than that—the CPUs have to
coordinate to ensure that only one CPU succeeds at a compare_exchange for a
variable at a time (take a look at the MESI protocol if you’re curious about
how that works), and that coordination grows quadratically more costly the
more CPUs are involved!
COMPARE_EXCHANGE_WEAK
The careful documentation reader will notice that compare_exchange has a suspiciously
named cousin, compare_exchange_weak, and wonder what the difference
is. The weak variant of compare_exchange is allowed to fail even if the atomic
variable’s value does still match the expected value that the user passed in,
whereas the strong variant must succeed in this case.
This might seem odd—how could an atomic value swap fail except if the
value has changed? The answer lies in system architectures that do not have a
native compare_exchange operation. For example, ARM processors instead have
locked load and conditional store operations, where a conditional store will fail
if the value read by an associated locked load has not been written to since
the load. The Rust standard library implements compare_exchange on ARM by
calling this pair of instructions in a loop and returning only once the conditional
store succeeds. This makes the code in Listing 10-6 needlessly inefficient—we
end up with a nested loop, which requires more instructions and is harder to
optimize. Since we already have a loop in this case, we could instead use compare_
exchange_weak, remove the unreachable!() on Err(false), and get better
machine code on ARM and the same compiled code on x86!
The Fetch Methods
Fetch methods (fetch_add, fetch_sub, fetch_and, and the like) are designed to
allow more efficient execution of atomic operations that commute—that
is, operations that have meaningful semantics regardless of the order they
execute in. The motivation for this is that the compare_exchange method
is powerful, but also costly—if two threads both want to update a single
atomic variable, one will succeed, while the other will fail and have to retry.
If many threads are involved, they all have to mediate sequential access to
the underlying value, and there will be plenty of spinning while threads
retry on failure.
For simple operations that commute, rather than fail and retry just
because another thread modified the value, we can tell the CPU what
operation to perform on the atomic variable. It’ll then perform that operation
on whatever the current value happens to be when the CPU eventually
gets exclusive access. Think of an AtomicUsize that counts the number of
188 Chapter 10
operations a pool of threads has completed. If two threads both complete a
job at the same time, it doesn’t matter which one updates the counter first
as long as both their increments are counted.
The fetch methods implement these kinds of commutative operations.
They perform a read and a store operation in a single step and
guarantee that the store operation was performed on the atomic variable
when it held exactly the value returned by the method. As an example,
AtomicUsize::fetch_add(1, Ordering::Relaxed) never fails—it always adds 1 to
the current value of the AtomicUsize, no matter what it is, and returns the
value of the AtomicUsize precisely when this thread’s 1 was added.
The fetch methods tend to be more efficient than compare_exchange
because they don’t require threads to fail and retry when multiple threads
contend for access to a variable. Some hardware architectures even have
specialized fetch method implementations that scale much better as the
number of involved CPUs grows. Nevertheless, if enough threads try to
operate on the same atomic variable, those operations will begin to slow
down and exhibit sublinear scaling due to the coordination required. In
general, the best way to significantly improve the performance of a concurrent
algorithm is to split contended variables into more atomic variables
that are each less contended, rather than switching from compare_exchange to
a fetch method.
**NOTE** The fetch_update method is somewhat deceptively named—behind the scenes, it is
really just a compare_exchange_weak loop, so its performance profile will more closely
match that of compare_exchange than the other fetch methods.
Sane Concurrency
Writing correct and performant concurrent code is harder than writing
sequential code; you have to consider not only possible execution interleavings
but also how your code interacts with the compiler, the CPU, and the
memory subsystem. With such a wide array of footguns at your disposal, it’s
easy to want to throw your hands in the air and just give up on concurrency
altogether. In this section we’ll explore some techniques and tools that can
help ensure that you write correct concurrent code without (as much) fear.
Start Simple
It is a fact of life that simple, straightforward, easy-to-follow code is more
likely to be correct. This principle also applies to concurrent code—always
start with the simplest concurrent design you can think of, then measure,
and only if measurement reveals a performance problem should you optimize
your algorithm.
To follow this tip in practice, start out with concurrency patterns that
do not require intricate use of atomics or lots of fine-grained locks. Begin
with multiple threads that run sequential code and communicate over
channels, or that cooperate through locks, and then benchmark the resulting
performance with the workload you care about. You’re much less likely
Concurrency (and Parallelism) 189
to make mistakes this way than by implementing fancy lockless algorithms
or by splitting your locks into a thousand pieces to avoid false sharing. For
many use cases, these designs are plenty fast enough; it turns out a lot of
time and effort has gone into making channels and locks perform well!
And if the simple approach is fast enough for your use case, why introduce
more complex and error-prone code?
If your benchmarks indicate a performance problem, then figure out
exactly which part of your system scales poorly. Focus on fixing that bottleneck
in isolation where you can, and try to do so with small adjustments
where possible. Maybe it’s enough to split a lock in two rather than move
to a concurrent hash table, or to introduce another thread and a channel
rather than implement a lock-free work stealing queue. If so, do that.
Even when you do have to work directly with atomics and the like, keep
things simple until there’s a proven need to optimize—use Ordering::SeqCst
and compare_exchange at first, and then iterate if you find concrete evidence
that those are becoming bottlenecks that must be taken care of.
Write Stress Tests
As the author, you have a lot of insight into where bugs in your code
may hide, without necessarily knowing what those bugs are (yet, anyway).
Writing stress tests is a good way to shake out some of the hidden bugs.
Stress tests don’t necessarily perform a complex sequence of steps but
instead have lots of threads doing relatively simple operations in parallel.
For example, if you were writing a concurrent hash map, one stress test
might be to have N threads insert or update keys and M threads read keys
in such a way that those M+N threads are likely to often choose the same
keys. Such a test doesn’t test for a particular outcome or value but instead
tries to trigger many possible interleavings of operations in the hopes that
buggy interleavings might reveal themselves.
Stress tests resemble fuzz tests in many ways; whereas fuzzing generates
many random inputs to a given function, the stress test instead generates
many random thread and memory access schedules. Just like fuzzers,
stress tests are therefore only as good as the assertions in your code; they
can’t tell you about a bug that doesn’t manifest in some easy-to-spot way
like an assertion failure or some other kind of panic. For that reason, it’s a
good idea to litter your low-level concurrency code with assertions, or debug_
assert__ if you’re worried about runtime cost in particularly hot loops.
Use Concurrency Testing Tools
The primary challenge in writing concurrent code is to handle all the possible
ways the execution of different threads can interleave. As we saw in the
Ordering::SeqCst example in Listing 10-4, it’s not just the thread scheduling
that matters, but also which memory values are possible for a given thread
to observe at any given point in time. Writing tests that execute every possible
legal execution is not only tedious but also difficult—you need very
low-level control over which threads execute when and what values their
reads return, which the operating system likely doesn’t provide.
190 Chapter 10
Model Checking with Loom
Luckily, a tool already exists that can simplify this execution exploration
for you in the form of the loom crate. Given the relative release cycles of this
book and that of a Rust crate, I won’t give any examples of how to use Loom
here, as they’d likely be out of date by the time you read this book, but I will
give an overview of what it does.
Loom expects you to write dedicated test cases in the form of closures
that you pass into a Loom model. The model keeps track of all cross-thread
interactions and tries to intelligently explore all possible iterations of those
interactions by executing the test case closure multiple times. To detect
and control thread interactions, Loom provides replacement types for all
the types in the standard library that allow threads to coordinate with one
another; that includes most types under std::sync and std::thread as well
as UnsafeCell and a few others. Loom expects your application to use those
replacement types whenever you run the Loom tests. The replacement
types tie into the Loom executor and perform a dual function: they act as
rescheduling points so that Loom can choose which operation to run next
after each possible thread interaction point, and they inform Loom of new
possible interleavings to consider. Essentially, Loom builds up a tree of all
the possible future executions for each point at which multiple execution
interleavings are possible and then tries to execute all of them, one after
the other.
Loom attempts to fully explore all possible executions of the test
cases you provide it with, which means it can find bugs that occur only in
extremely rare executions that stress testing would not find in a hundred
years. While that’s great for smaller test cases, it’s generally not feasible
to apply that kind of rigorous testing to larger test cases that test more
involved sequences of operations or require many threads to run at once.
Loom would simply take too long to get decent coverage of the code. In
practice, you may therefore want to tell Loom to consider only a subset of
the possible executions, which Loom’s documentation has more details on.
Like with stress tests, Loom can catch only bugs that manifest as panics,
so that’s yet another reason to spend some time placing strategic assertions
in your concurrent code! In many cases, it may even be worthwhile to add
additional state tracking and bookkeeping instructions to your concurrent
code to give you better assertions.
Runtime Checking with ThreadSanitizer
For larger test cases, your best bet is to run the test through a couple of iterations
under Google’s excellent ThreadSanitizer, also known as TSan. TSan
automatically augments your code by placing extra bookkeeping instructions
prior to every memory access. Then, as your code runs, those bookkeeping
instructions update and check a special state machine that flags any concurrent
memory operations that indicate a problematic race condition. For
example, if thread B writes to some atomic value X, but has not synchronized
(lots of hand waving here) with the thread that wrote the previous value of X
that indicates a write/write race, which is nearly always a bug.
Concurrency (and Parallelism) 191
Since TSan only observes your code running and does not execute
it over and over again like Loom, it generally only adds a constant-factor
overhead to the runtime of your program. While that factor can be significant
(5–15 times at the time of writing), it’s still small enough that you can
execute even most complex test cases in a reasonable amount of time.
At the time of writing, to use TSan you need to use a nightly version of
the Rust compiler and pass in the -Zsanitizer=thread command-line argument
(or set it in RUSTFLAGS), though hopefully in time this will be a standard
supported option. Other sanitizers are also available that check things like
out-of-bounds memory accesses, use-after-free, memory leaks, and reads of
uninitialized memory, and you may want to run your concurrent test suite
through those too!
HEISENBUGS
Heisenbugs are bugs that seem to disappear when you try to study them. This
happens quite frequently when trying to debug highly concurrent code; the
additional instrumentation to debug the problem changes the relative timing of
concurrent events and might cause the execution interleaving that triggered the
bug to no longer happen.
A particularly common cause of disappearing concurrency bugs is using
print statements, which is by far one of the most common debugging techniques.
There are two reasons why print statements have such an outsized effect on
concurrency bugs. The first, and perhaps most obvious, is that relatively speaking,
printing something to the user’s terminal (or wherever standard output
points) takes quite a long time, especially if your program is producing a lot
of output. Writing to the terminal requires, at the very least, a round-trip to the
operating system kernel to perform the write, but the write may also have to
wait for the terminal itself to read from the process’s output into its own buffers.
All that extra time might so much delay the operation that previously raced with
an operation in some other thread that the race condition disappears.
The second reason why print statements disturb concurrent execution patterns
is that writing to standard output is (generally) guarded by a lock. If you look
inside the Stdout type in the standard library, you’ll see that it holds a Mutex that
guards access to the output stream. It does this so that the output isn’t garbled too
badly if multiple threads try to write at the same time—without a lock, a given
line might have characters interspersed from multiple thread writes, but with the
lock the threads will take turns writing instead. Unfortunately, acquiring the output
lock, is another thread synchronization point, and one that every printing thread is
involved in. This means that if your code was previously broken due to missing synchronization
between two threads, or just because a particular race between two
threads was possible, adding print statements might fix that bug as a side effect!
In general, when you spot what seems like a Heisenbug, try to find other
ways to narrow down the problem. That might involve using Loom or TSan,
(continued)

192 Chapter 10
using gdb or lldb, or using a per-thread in-memory log that you print only at the
end. Many logging frameworks also work hard to avoid synchronization points
on the critical path of issuing log events, so switching to one of those might
make your life easier. As an added bonus, good logging that you leave behind
after fixing a particular bug might come in handy later. Personally I’m a big fan
of the tracing crate, but there are many good options out there.
Summary
In this chapter, we first covered common correctness and performance pitfalls
in concurrent Rust, and some of the high-level concurrency patterns
that successful concurrent applications tend to use to work around them.
We also explored how asynchronous Rust enables concurrency without parallelism,
and how to explicitly introduce parallelism in asynchronous Rust
code. We then dove deeper into Rust’s many different lower-level concurrency
primitives, including how they work, how they differ, and what they’re
all for. Finally, we explored techniques for writing better concurrent code
and looked at tools like Loom and TSan that can help you vet that code. In
the next chapter we’ll continue our journey through the lower levels of Rust
by digging into foreign function interfaces, which allow Rust code to link
directly against code written in other languages.

### FOREIGN FUNCTION I NTERFACES

Not all code is written in Rust. It’s shocking,
I know. Every so often, you’ll need to interact
with code written in other languages,
either by calling into such code from Rust or
by allowing that code to call your Rust code. You can
achieve this through foreign function interfaces (FFI).
In this chapter we’ll first look at the primary mechanism Rust provides
for FFI: the extern keyword. We’ll see how to use extern both to expose Rust
functions and statics to other languages and to give Rust access to functions
and static variables provided from outside the Rust bubble. Then, we’ll
walk through how to align Rust types with types defined in other languages
and explore some of the intricacies of allowing data to flow across the FFI
boundary. Finally, we’ll talk about some of the tools you’ll likely want to use
if you’re doing any nontrivial amount of FFI.
194 Chapter 11
**NOTE** While I often refer to FFI as being about crossing the boundary between one language
and another, FFI can also occur entirely inside Rust-land. If one Rust program shares
memory with another Rust program but the two aren’t compiled together—say, if you’re
using a dynamically linked library in your Rust program that happens to be written in
Rust, but you just have the C-compatible .so file—the same complications arise.
Crossing Boundaries with extern
FFI is, ultimately, all about accessing bytes that originate somewhere outside
your application’s Rust code. For that, Rust provides two primary building
blocks: symbols, which are names assigned to particular addresses in a
given segment of your binary that allow you to share memory (be it for data
or code) between the external origin and your Rust code, and calling conventions
that provide a common understanding of how to call functions stored
in such shared memory. We’ll look at each of these in turn.
Symbols
Any binary artifact that the compiler produces from your code is filled with
symbols—every function or static variable you define has a symbol that
points to its location in the compiled binary. Generic functions may even
have multiple symbols, one for each monomorphization of the function the
compiler generates!
Normally, you don’t have to think about symbols—they’re used internally
by the compiler to pass around the final address of a function or
static variable in your binary. This is how the compiler knows what location
in memory each function call should target when it generates the final
machine code, or where to read from if your code accesses a static variable.
Since you don’t usually refer to symbols directly in your code, the compiler
defaults to choosing semirandom names for them—you may have two functions
called foo in different parts of your code, but the compiler will generate
distinct symbols from them so that there’s no confusion.
However, using random names for symbols won’t work when you want
to call a function or access a static variable that isn’t compiled at the same
time, such as code that’s written in a different language and thus compiled
by a different compiler. You can’t tell Rust about a static variable defined in
C if the symbol for that variable has a semirandom name that keeps changing.
Conversely, you can’t tell Python’s FFI interface about a Rust function if
you can’t produce a stable name for it.
To use a symbol with an external origin, we also need some way to tell
Rust about a variable or function in such a manner that the compiler will
look for that same symbol defined elsewhere rather than defining its own
(we’ll talk about how that search happens later). Otherwise, we would just
end up with two identical symbols for that function or static variable, and
no sharing would take place. In fact, in all likelihood, compilation would
fail since any code that referred to that symbol wouldn’t know which definition
(that is, which address) to use for it!
Foreign Function Interfaces 195
**NOTE** A quick note about terminology: a symbol can be declared multiple times but
defined only once. Every declaration of a symbol will link to the same single definition
for that symbol at linking time. If no definition for a declaration is found, or if
there are multiple definitions, the linker will complain.
An Aside on Compilation and Linking
Compiler crash course time! Having a rough idea of the complicated process
of turning code into a runnable binary will help you understand FFI
better. You see, the compiler isn’t one monolithic program but is (typically)
broken down into a handful of smaller programs that each perform distinct
tasks and run one after the other. At a high level, there are three distinct
phases to compilation—compilation, code generation, and linking—handled by
three different components.
The first phase is performed by what most people tend to think of
as “the compiler”; it deals with type checking, borrow checking, monomorphization,
and other features we associate with a given programming
language. This phase generates no machine code but rather a low-level
representation of the code that uses heavily annotated abstract machine
operations. That low-level representation is then passed to the code generation
tool, which is what produces machine code that can actually run on a
given CPU.
These two operations, taken together, do not have to be run in a single
big pass over the whole codebase all at once. Instead, the codebase can be
sliced into smaller chunks that are then run through compilation concurrently.
For example, Rust generally compiles different crates independently
and in parallel as long as there isn’t a dependency between them. It can also
invoke the code generation tool for independent crates separately to process
them in parallel. Rust can often even compile multiple smaller slices of
a single crate separately!
Once the machine code for every piece of the application has been
generated, those pieces can then be wired together. This is done in the
linking phase by, unsurprisingly, the linker. The linker’s primary job is to
take all the binary artifacts, called object files, produced by code generation,
stitch them together into a single file, and then replace every reference to a
symbol with the final memory address of that symbol. This is how you can
define a function in one crate and call it from another but still compile the
two crates separately.
The linker is what enables FFI to work. It doesn’t care how each of the
input object files were constructed; it just dutifully links together all the
object files and then resolves any shared symbols. One object file may originally
have been Rust code, one originally C code, and one may be a binary
blob downloaded from the internet; as long as they all use the same symbol
names, the linker will make sure that the resulting machine code uses the
correct cross-referenced addresses for any shared symbols.
A symbol can be linked either statically or dynamically. Static linking
is the simplest, as each reference to a symbol is simply replaced with the
address of that symbol’s definition. Dynamic linking, on the other hand,
196 Chapter 11
ties each reference to a symbol to a bit of generated code that tries to find
the symbol’s definition when the program runs. We’ll talk more about these
linking modes a little later. Rust generally defaults to static linking for Rust
code, and dynamic linking for FFI.
Using extern
The extern keyword is the mechanism that allows us to declare a symbol as
residing within a foreign interface. Specifically, it declares the existence of
a symbol that’s defined elsewhere. In Listing 11-1 we define a static variable
called RS_DEBUG in Rust that we make available to other code via FFI. We also
declare a static variable called FOREIGN_DEBUG whose definition is unspecified
but will be resolved at linking time.

# [no_mangle]

pub static RS_DEBUG: bool = true;
extern {
static FOREIGN_DEBUG: bool;
}
Listing 11-1: Exposing a Rust static variable, and accessing one declared elsewhere,
through FFI
The #[no_mangle] attribute ensures that RS_DEBUG retains that name during
compilation rather than having the compiler assign it another symbol
name to, for example, distinguish it from another (non-FFI) RS_DEBUG static
variable elsewhere in the program. The variable is also declared as pub since
it’s a part of the crate’s public API, though that annotation isn’t strictly
necessary on items marked #[no_mangle]. Note that we don’t use extern for
RS_DEBUG, since it’s defined here. It will still be accessible to link against from
other languages.
The extern block surrounding the FOREIGN_DEBUG static variable denotes
that this declaration refers to a location that Rust will learn at linking
time based on where the definition of the same symbol is located. Since
it’s defined elsewhere, we don’t give it an initialization value, just a type,
which should match the type used at the definition site. Because Rust
doesn’t know anything about the code that defines the static variable,
and thus can’t check that you’ve declared the correct type for the symbol,
FOREIGN_DEBUG can be accessed only inside an unsafe block.
**NOTE** Static variables in Rust aren’t mutable by default, regardless of whether they’re in an
extern block. These variables are always available from any thread, so mutable access
would pose a data race risk. You can declare a static as mut, but if you do, it becomes
unsafe to access.
The procedure to declare FFI functions is very similar. In Listing 11-2,
we make hello_rust accessible to non-Rust code and pull in the external
hello_foreign function.
Foreign Function Interfaces 197

# [no_mangle]

pub extern fn hello_rust(i: i32) { ... }
extern {
fn hello_foreign(i: i32);
}
Listing 11-2: Exposing a Rust function, and accessing one defined elsewhere, through FFI
The building blocks are all the same as in Listing 11-1 with the exception
that the Rust function is declared using extern fn, which we’ll explore
in the next section.
If there are multiple definitions of a given extern symbol like FOREIGN_
DEBUG or hello_foreign, you can explicitly specify which library the symbol
should link against using the #[link] attribute. If you don’t, the linker will
give you an error saying that it’s found multiple definitions for the symbol
in question. For example, if you prefix an extern block with #[link(name =
"crypto")], you’re telling the linker to resolve any symbols (whether statics
or functions) against a linked library named “crypto.” You can also rename
an external static or function in your Rust code by annotating its declaration
with #[link_name = "<actual_symbol_name>"], and then the item links to
whatever name you wish. Similarly, you can rename a Rust item for export
using #[export_name = "<export_symbol_name>"].
Link Kinds

# [link] also accepts the argument kind, which dictates how the items in the

block should be linked. The argument defaults to "dylib", which signifies
C-compatible dynamic linking. The alternative kind value is "static", which
indicates that the items in the block should be linked fully at compile time
(that is, statically). This essentially means that the external code is wired
directly into the binary produced by the compiler , and thus doesn’t need to
exist at runtime. There are a few other kinds as well, but they are much less
common and outside the scope of this book.
There are several trade-offs between static and dynamic linking, but the
main considerations are security, binary size, and distribution. First, dynamic
linking tends to be more secure because it makes it easier to upgrade libraries
independently. Dynamic linking allows whoever deploys a binary that contains
your code to upgrade libraries your code links against without having
to recompile your code. If, say, libcrypto gets a security update, the user can
update the crypto library on the host and restart the binary, and the updated
library code will be used automatically. With static compilation, the library’s
code is hardwired into the binary, so the user would have to recompile your
code against an upgraded version of the library to get the update.
Dynamic linking also tends to produce smaller binaries. Since static
compilation includes any linked code into the final binary output, and any
code that code in turn pulls in, it produces larger binaries. With dynamic
linking, each external item includes just a small bit of wrapper code that
loads the indicated library at runtime and then forwards the access.
198 Chapter 11
So far, static linking may not seem very attractive, but it has one big advantage
over dynamic linking: ease of distribution. With dynamic linking, anyone
who wants to run a binary that includes your code must also have any libraries
your code links against. Not only that, but they must make sure the version of
each such library they have is compatible with what your code expects. This
may be fine for libraries like glibc or OpenSSL that are available on most systems,
but it poses a problem for more obscure libraries. The user then needs
to be aware that they should install that library and must hunt for it in order to
run your code! With static linking, the library’s code is embedded directly into
the binary output, so the user doesn’t need to install it themselves.
Ultimately, there isn’t a right choice between static and dynamic linking.
Dynamic linking is usually a good default, but static compilation may be a
better option for particularly constrained deployment environments or for
very small or niche library dependencies. Use your best judgment!
Calling Conventions
Symbols dictate where a given function or variable is defined, but that’s not
enough to allow function calls across FFI boundaries. To call a foreign
function in any language, the compiler also needs to know its calling convention,
which dictates the assembly code to use to invoke the function. We
won’t get into the actual technical details of each calling convention here,
but as a general overview, the convention dictates:
• How the stack frame for the call is set up
• How arguments are passed (whether on the stack or in registers, in
order or in reverse)
• How the function is told where to jump back to when it returns
• How various CPU states, like registers, are restored in the caller after
the function completes
Rust has its own unique calling convention that isn’t standardized and
is allowed to be changed by the compiler over time. This works fine as long
as all function definitions and calls are compiled by the same Rust compiler,
but it is problematic if you want interoperability with external code
because that external code doesn’t know about the Rust calling convention.
Every Rust function is implicitly declared with extern "Rust" if you don’t
declare anything else. Using extern on its own, as in Listing 11-2, is shorthand
for extern "C", which means “use the standard C calling convention.”
The shorthand is there because the C calling convention is what you want
in nearly every case of FFI.
**NOTE** Unwinding generally works only with regular Rust functions. If you unwind
across the end of a Rust function that isn’t extern "Rust", your program will abort.
Unwinding across the FFI boundary into external code is undefined behavior. With
RFC 2945, Rust gained a new extern declaration, extern "C-unwind"; this permits
unwinding across FFI boundaries in particular situations, but if you wish to use it
you should read the RFC carefully.
Foreign Function Interfaces 199
Rust also supports a number of other calling conventions that you supply
as a string following the extern keyword (in both fn and block context). For
example, extern "system" says to use the calling convention of the operating
system’s standard library interface, which at the time of writing is the same
as "C" everywhere except on Win32, which uses the "stdcall" calling convention.
In general, you’ll rarely need to supply a calling convention explicitly
unless you’re working with particularly platform-specific or highly optimized
external interfaces, so just extern (which is extern "C") will be fine.
**NOTE** A function’s calling convention is part of its type. That is, the type extern "C" fn()
is not the same as fn() (or extern "Rust" fn()), which is different again from extern
"system" fn().
OTHER BINARY ARTIFACTS
Normally, you compile Rust code only to run its tests or build a binary that
you’re then going to distribute or run. Unlike in many other languages, you
don’t generally compile a Rust library to distribute it to others—if you run a command
like cargo publish, it just wraps up your crate’s source code and uploads
it to crates.io. This is mostly because it is difficult to distribute generic code as
anything but source code. Since the compiler monomorphizes each generic
function to the provided type arguments, and those types may be defined in
the caller’s crate, the compiler must have access to the function’s generic form,
which means no optimized machine code!
Technically speaking, Rust does compile binary library artifacts, called rlibs,
of each dependency that it combines in the end. These rlibs include the information
necessary to resolve generic types, but they are specific to the exact compiler
used and can’t generally be distributed in any meaningful way.
So what do you do if you want to write a library in Rust that you then
want to interface with from another programming language? The solution is to
produce C-compatible library files in the form of dynamically linked libraries
(.so files on Unix, .dylib files on macOS, and .dll files on Windows) and statically
linked libraries (.a files on Unix/macOS and .lib files on Windows). Those
files look like files produced by C code, so they can also be used by other languages
that know how to interact with C.
To produce these C-compatible binary artifacts, you set the crate-type
field of the [lib] section of your Cargo.toml file. The field takes an array of values,
which would normally just be "lib" to indicate a standard Rust library (an
rlib). Cargo applies some heuristics that will set this value automatically if your
crate is clearly not a library (for example, if it's a procedural macro), but best
practice is to set this value explicitly if you’re producing anything but a good ol’
Rust library.
There are a number of different crate types, but the relevant ones here are
"cdylib" and "staticlib", which produce C-compatible library files that are
dynamically and statically linked, respectively. Keep in mind that when you
(continued)
200 Chapter 11
produce one of these artifact types, only publicly available symbols are available—
that is, public and #[no_mangle] static variables and functions. Things like
types and constants won’t be available, even if they’re marked pub, since they
have no meaningful representation in a binary library file.
Types Across Language Boundaries
With FFI, type layout is crucial; if one language lays out the memory for
some shared data one way but the language on the other side of the FFI
boundary expects it to be laid out differently, then the two sides will interpret
the data inconsistently. In this section, we’ll look at how to make types
match up over FFI, and other aspects of types to be aware of when you cross
the boundaries between languages.
Type Matching
Types aren’t shared across the FFI boundary. When you declare a type in
Rust, that type information is lost entirely upon compilation. All that’s communicated
to the other side is the bits that make up values of that type.
You therefore need to declare the type for those bits on both sides of the
boundary. When you declare the Rust version of the type, you first must
make sure the primitives contained within the type match up. For example,
if C is used on the other side of the boundary, and the C type uses an int,
the Rust code had better use the exact Rust equivalent: an i32. To take
some of the guesswork out of that process, for interfaces that use C-like
types the Rust standard library provides you with the correct C types in
the std::os::raw module, which defines type c_int = i32, type c_char = i8/
u8 depending on whether char is signed, type c_long = i32/i64 depending on
the target pointer width, and so on.
**NOTE** Take particular note of quirky integer types in C like __be32. These often do not translate
directly to Rust types and may be best left as something like [u8; 4]. For example,
__be32 is always encoded as big-endian, whereas Rust’s i32 uses the endianness of the
current platform.
With more complex types like vectors and strings, you usually need
to do the mapping manually. For example, since C tends to represent
a string as a sequence of bytes terminated with a 0 byte, rather than a
UTF-8–encoded string with the length stored separately, you cannot generally
use Rust’s string types over FFI. Instead, assuming the other side
uses a C-style string representation, you should use the std::ffi::CStr and
std::ffi::CString types for borrowed and owned strings, respectively. For
vectors, you’ll likely want to use a raw pointer to the first element and then
pass the length separately—the Vec::into_raw_parts method may come in
handy for that.
Foreign Function Interfaces 201
For types that contain other types, such as structs and unions, you also
need to deal with layout and alignment. As we discussed in Chapter 2, Rust
lays out types in an undefined way by default, so at the very least you will
want to use #[repr(C)] to ensure that the type has a deterministic layout and
alignment that mirrors what’s (likely and hopefully) used across the FFI
boundary. If the interface also specifies other configurations for the type,
such as manually setting its alignment or removing padding, you’ll need to
adjust your #[repr] accordingly.
A Rust enum has multiple possible C-style representations depending
on whether the enum contains data or not. Consider an enum without data,
like this:
enum Foo { Bar, Baz }
With #[repr(C)], the type Foo is encoded using just a single integer of
the same size that a C compiler would choose for an enum with the same
number of variants. The first variant has the value 0, the second the value 1,
and so on. You can also manually assign values to each variant, as shown in
Listing 11-3.

# [repr(C)]

enum Foo {
Bar = 1,
Baz = 2,
}
Listing 11-3: Defining explicit variant values for a dataless enum
**NOTE** Technically, the specification says that the first variant’s value is 0 and every subsequent
variant’s value is one greater than that of the previous one. This makes a difference
if you manually set the value for some variants but not others—those you do
not set will continue from the last one you did set.
You should be careful about mapping enum-like types in C to Rust
this way, however, as only the values for defined variants are valid for an
instance of the enum type. This tends to get you into trouble with C-style
enumerations that often function more like bitsets, where variants can be
bitwise ORed together to produce a value that encapsulates multiple variants
at once. In the example from Listing 11-3, for instance, a value of 3 produced
by taking Bar | Baz would not be valid for Foo in Rust! If you need to
model a C API that uses an enumeration for a set of bitflags that can be set
and unset individually, consider using a newtype wrapper around an integer
type, with associated constants for each variant and implementations of
the various Bit*traits for improved ergonomics. Or use the bitflags crate.
**NOTE** For fieldless enums, you can also pass a numeric type to #[repr] to use a different
type than isize for the discriminator. For example, #[repr(u8)] will encode the discriminator
using a single unsigned byte. For a data-carrying enum, you can pass

# [repr(C, u8)] to get the same effect

202 Chapter 11
On an enum that contains data, the #[repr(C)] attribute causes the enum
to be represented using a tagged union. That is, it is represented in memory
by a #[repr(C)] struct with two fields, where the first is the discriminator as
it would be encoded if none of the variants had fields, and the second is a
union of the data structures for each variant. For a concrete example, consider
the enum and associated representation in Listing 11-4.

# [repr(C)]

enum Foo {
Bar(i32),
Baz { a: bool, b: f64 }
}
// is represented as

# [repr(C)]

enum FooTag { Bar, Baz }

# [repr(C)]

struct FooBar(i32);

# [repr(C)]

struct FooBaz{ a: bool, b: f64 }

# [repr(C)]

union FooData {
bar: FooBar,
baz: FooBaz,
}

# [repr(C)]

struct Foo {
tag: FooTag,
data: FooData
}
Listing 11-4: Rust enums with #[repr(C)] are represented as tagged unions.
THE NICHE OPTIMIZATION IN FFI
In Chapter 9 we talked about the niche optimization, where the Rust compiler
uses invalid bit patterns to represent enum variants that hold no data. The fact
that this optimization is guaranteed leads to an interesting interaction with FFI.
Specifically, it means that nullable pointers can always be represented in FFI types
using an Option-wrapped pointer type. For example, a nullable function pointer
can be represented as Option<extern fn(...)>, and a nullable data pointer can
be represented as Option<*mut T>. These will transparently do the right thing if an
all-zero bit pattern value is provided, and will represent it as None in Rust.
Allocations
When you allocate memory, that allocation belongs to its allocator and can
be freed only by that same allocator. This is the case if you use multiple

Foreign Function Interfaces 203
allocators within Rust and also if you are allocating memory both in Rust
and with some allocator on the other side of the FFI boundary. You’re
free to send pointers across the boundary and access that memory to your
heart’s content, but when it comes to releasing the memory again, it needs
to be returned to the appropriate allocator.
Most FFI interfaces will have one of two configurations for handling
allocation: either the caller provides data pointers to chunks of memory
or the interface exposes dedicated freeing methods to which any allocated
resources should be returned when they are no longer needed. Listing 11-5
shows an example of Rust declarations of some signatures from the
OpenSSL library that use implementation-managed memory.
// One function allocates memory for a new object.
extern fn ECDSA_SIG_new() -> *mut ECDSA_SIG;
// And another accepts a pointer created by new
// and deallocates it when the caller is done with it.
extern fn ECDSA_SIG_free(sig:*mut ECDSA_SIG);
Listing 11-5: An implementation-managed memory interface
The functions ECDSA_SIG_new and ECDSA_SIG_free form a pair, where the
caller is expected to call the new function, use the returned pointer for as
long as it needs (likely by passing it to other functions in turn), and then
finally pass the pointer to the free function once it’s done with the referenced
resource. Presumably, the implementation allocates memory in
the new function and deallocates it in the free function. If these functions
were defined in Rust, the new function would likely use Box::new, and the
free function would invoke Box::from_raw and then drop the value to run its
destructor.
Listing 11-6 shows an example of caller-managed memory.
// An example of caller-managed memory.
// The caller provides a pointer to a chunk of memory,
// which the implementation then uses to instantiate its own types.
// No free function is provided, as that happens in the caller.
extern fn BIO_new_mem_buf(buf: *const c_void, len: c_int) ->*mut BIO
Listing 11-6: A caller-managed memory interface
Here, the BIO_new_mem_buf function instead has the caller supply the
backing memory. The caller can choose to allocate memory on the heap,
or use whatever other mechanism it deems fit for obtaining the required
memory, and then passes it to the library. The onus is then on the caller to
ensure that the memory is later deallocated, but only once it is no longer
needed by the FFI implementation!
You can use either of these approaches in your FFI APIs or even mix
and match them if you wish. As a general rule of thumb, allow the caller to
pass in memory when doing so is feasible, since it gives the caller more freedom
to manage memory as it deems appropriate. For example, the caller
may be using a highly specialized allocator on some custom operating
204 Chapter 11
system, and may not want to be forced to use the standard allocator your
implementation would use. If the caller can pass in the memory, it might
even avoid allocations entirely if it can instead use stack memory or reuse
already allocated memory. However, keep in mind that the ergonomics of a
caller-managed interface are often more convoluted, since the caller must
now do all the work to figure out how much memory to allocate and then
set that up before it can call into your library.
In some instances, it may even be impossible for the caller to know
ahead of time how much memory to allocate—for example, if your library’s
types are opaque (and thus not known to the caller) or can change over
time, the caller won’t be able to predict the size of the allocation. Similarly,
if your code has to allocate more memory while it is running, such as if
you’re building a graph on the fly, the amount of memory needed may vary
dynamically at runtime. In such cases, you will have to use implementationmanaged
memory.
When you’re forced to make a trade-off, go with caller-allocated memory
for anything that is either large or frequent. In those cases the caller is
likely to care the most about controlling the allocations itself. For anything
else, it’s probably okay for your code to allocate and then expose destructor
functions for each relevant type.
Callbacks
You can pass function pointers across the FFI boundary and call the referenced
function through those pointers as long as the function pointer’s
type has an extern annotation that matches the function’s calling convention.
That is, you can define an extern "C" fn(c_int) -> c_int in Rust and
then pass a reference to that function to C code as a callback that the C
code will eventually invoke.
You do need to be careful using callbacks around panics, as having a
panic unwind past the end of a function that is anything but extern "Rust"
is undefined behavior. The Rust compiler will currently automatically abort
if it detects such a panic, but that may not always be the behavior you want.
Instead, you may want to use std::panic::catch_unwind to detect the panic in
any function marked extern, and then translate the panic into an error that
is FFI-compatible.
Safety
When you write Rust FFI bindings, most of the code that actually interfaces
with the FFI will be unsafe and will mainly revolve around raw pointers.
However, your goal should be to ultimately present a safe Rust interface on
top of the FFI. Doing so mainly comes down to reading carefully through
the invariants of the unsafe interface you are wrapping and then ensuring
you uphold them all through the Rust type system in the safe interface. The
three most important elements of safely encapsulating a foreign interface
are capturing & versus &mut accurately, implementing Send and Sync appropriately,
and ensuring that pointers cannot be accidentally confused. I’ll go
over how to enforce each of these next.
Foreign Function Interfaces 205
References and Lifetimes
If there’s a chance external code will modify data behind a given pointer,
make sure that the safe Rust interface has an exclusive reference to the
relevant data by taking &mut. Otherwise a user of your safe wrapper might
accidentally read from memory that the external code is simultaneously
modifying, and all hell will break loose!
You’ll also want to make good use of Rust lifetimes to ensure that all
pointers live for as long as the FFI requires. For example, imagine an external
interface that lets you create a Context and then lets you create a Device from
that Context with the requirement that the Context remain valid for as long as
the Device lives. In that case, any safe wrapper for the interface should enforce
that requirement in the type system by having Device hold a lifetime associated
with the borrow of Context that the Device was created from.
Send and Sync
Do not implement Send and Sync for types from an external library unless
that library explicitly documents that those types are thread-safe! It is the
safe Rust wrapper’s job to ensure that safe Rust code cannot violate the
invariants of the external code and thus trigger undefined behavior.
Sometimes, you may even want to introduce dummy types to enforce
external invariants. For example, say you have an event loop library with the
interface given in Listing 11-7.
extern fn start_main_loop();
extern fn next_event() -> *mut Event;
Listing 11-7: A library that expects single-threaded use
Now suppose that the documentation for the external library states that
next_event may be called only by the same thread that called start_main_loop.
However, here we have no type that we can avoid implementing Send for!
Instead, we can take a page out of Chapter 3 and introduce additional
marker state to enforce the invariant, as shown in Listing 11-8.
pub struct EventLoop(std::marker::PhantomData<*const ()>);
pub fn start() -> EventLoop {
unsafe { ffi::start_main_loop() };
EventLoop(std::marker::PhantomData)
}
impl EventLoop {
pub fn next_event(&self) -> Option<Event> {
let e = unsafe { ffi::next_event() };
// ...
}
}
Listing 11-8: Enforcing an FFI invariant by introducing auxiliary types
The empty type EventLoop doesn’t actually connect with anything in the
underlying external interface but rather enforces the contract that you call
206 Chapter 11
next_event only after calling start_main_loop, and only on the same thread.
You enforce the “same thread” part by making EventLoop neither Send nor
Sync, by having it hold a phantom raw pointer (which itself is neither Send
nor Sync).
Using PhantomData<_const ()> to “undo” the Send and Sync auto-traits as
we do here is a bit ugly and indirect. Rust does have an unstable compiler
feature that enables negative trait implementations like impl !Send for
EventLoop {}, but it’s surprisingly difficult to get its implementation right,
and it likely won’t stabilize for some time.
You may have noticed that nothing prevents the caller from invoking
start_main_loop multiple times, either from the same thread or from another
thread. How you’d handle that would depend on the semantics of the
library in question, so I’ll leave it to you as an exercise.
Pointer Confusion
In many FFI APIs, you don’t necessarily want the caller to know the internal
representation for each and every chunk of memory you give it pointers to.
The type might have internal state that the caller shouldn’t fiddle with, or
the state might be difficult to express in a cross-language-compatible way.
For these kinds of situations, C-style APIs usually expose void pointers, written
out as the C type void_, which is equivalent to *mut std::ffi::c_void in
Rust. A type-erased pointer like this is, effectively, just a pointer, and does
not convey anything about the thing it points to. For that reason, these
kinds of pointers are often referred to as opaque.
Opaque pointers effectively serve the role of visibility modifiers for
types across FFI boundaries—since the method signature does not say
what’s being pointed to, the caller has no option but to pass around the
pointer as is and use any available FFI methods to provide visibility into
the referenced data. Unfortunately, since one*mut c_void is indistinguishable
from another, there’s nothing stopping a user from taking an opaque
pointer as is returned from one FFI method and supplying it to a method
that expects a pointer to a different opaque type.
We can do better than this in Rust. To mitigate this kind of pointer
type confusion, we can avoid using _mut c_void directly for opaque pointers
in FFI, even if the actual interface calls for a void_, and instead construct
different empty types for each distinct opaque type. For example,
in Listing 11-9 I use two distinct opaque pointer types that cannot be
confused.

# [non_exhaustive] #[repr(transparent)] pub struct Foo(c_void)

# [non_exhaustive] #[repr(transparent)] pub struct Bar(c_void)

extern {
pub fn foo() -> *mut Foo;
pub fn take_foo(arg:*mut Foo);
pub fn take_bar(arg: *mut Bar);
}
Listing 11-9: Opaque pointer types that cannot be confused
Foreign Function Interfaces 207
Since Foo and Bar are both zero-sized types, they can be used in place of
() in the extern method signatures. Even better, since they are now distinct
types, Rust won’t let you use one where the other is required, so it’s now
impossible to call take_bar with a pointer you got back from foo. Adding the

# [non_exhaustive] annotation ensures that the Foo and Bar types cannot be

constructed outside of this crate.
bindgen and Build Scripts
Mapping out the Rust types and externs for a larger external library can be
quite a chore. Big libraries tend to have a large enough number of type and
method signatures to match up that writing out all the Rust equivalents is
time-consuming. They also have enough corner cases and C oddities that
some patterns are bound to require more careful thought to translate.
Luckily, the Rust community has developed a tool called bindgen that
significantly simplifies this process as long as you have C header files available
for the library you want to interface with. bindgen essentially encodes all
the rules and best practices we’ve discussed in this chapter, plus a number
of others, and wraps them up in a configurable code generator that takes in
C header files and spits out appropriate Rust equivalents.
bindgen provides a stand-alone binary that generates the Rust code for C
headers once, which is convenient when you want to check in the bindings.
This process allows you to hand-tune the generated bindings, should that
be necessary. If, on the other hand, you want to generate the bindings automatically
on every build and just include the C header files in your source
code, bindgen also ships as a library that you can invoke in a custom build
script for your package.
**NOTE** If you check in the bindings directly, keep in mind that they will be correct only on
the platform they were generated for. Generating the bindings in a build script will
generate them specifically for the current target platform, which is less likely to cause
platform-related layout inconsistencies.
You declare a build script by adding build = "<some-file.rs>" to the
[package] section of your Cargo.toml. This tells Cargo that, before compiling
your crate, it should compile <some-file.rs> as a stand-alone Rust program
and run it; only then should it compile the source code of your crate. The
build script also gets its own dependencies, which you declare in the [builddependencies]
section of your Cargo.toml.
**NOTE** If you name your build script build.rs, you don’t need to declare it in your Cargo.toml.
Build scripts come in very handy with FFI—they can compile a bundled
C library from source, dynamically discover and declare additional build
flags to be passed to the compiler, declare additional files that Cargo
should check for changes for the purposes of recompilation, and, you
guessed it, generate additional source files on the fly!
208 Chapter 11
Though build scripts are very versatile, beware of making them too
aware of the environment they run in. While you can use a build script
to detect if the Rust compiler version is a prime or if it’s going to rain in
Istanbul tomorrow, making your compilation dependent on such conditions
may make builds fail unexpectedly for other developers, which leads to a
poor development experience.
The build script can write files to a special directory supplied through
the OUT_DIR environment variable. The same directory and environment
variable are also accessible in the Rust source code at compile time so that
it can pick up files generated by the build script. To generate and use Rust
types from a C header, you first have your build script use the library version
of bindgen to read in a .h file and turn it into a file called, say, bindings.rs
inside OUT_DIR. You then add the following line to any Rust file in your crate
to include bindings.rs at compilation time:
include!(concat!(env!("OUT_DIR"), "/bindings.rs"));
Since the code in bindings.rs is autogenerated, it’s generally best practice
to place the bindings in their own crate and give the crate the same
name as the library the bindings are for, with the suffix -sys (for example,
openssl-sys). If you don’t follow this practice, releasing new versions of your
library will be much more painful, as it is illegal for two crates that link
against the same external library through the links key in Cargo.toml to
coexist in a given build. You would essentially have to upgrade the entire
ecosystem to the new major version of your library all at once. Separating
just the bindings into their own crate allows you to issue new major versions
of the wrapper crate that can be adopted incrementally. The separation
also allows you to cut a breaking release of the crate with those bindings if
the Rust bindings change—say, if the header files themselves are upgraded
or a bindgen upgrade causes the generated Rust code to change slightly—
without also having to cut a breaking release of the crate that safely wraps
the FFI bindings.
**NOTE** Remember that if you include any of the types from the -sys crate in the public interface
of your main library crate, changing the dependency on the -sys crate to a new
major version still constitutes a breaking change for your main library!
If your crate instead produces a library file that you intend others to
use through FFI, you should also publish a C header file for its interface
to make it easier to generate native bindings to your library from other
languages. However, that C header file then needs to be kept up to date as
your crate changes, which can become cumbersome as your library grows in
size. Fortunately, the Rust community has also developed a tool to automate
this task: cbindgen. Like bindgen, cbindgen is a build tool, and it also comes
as both a binary and a library for use in build scripts. Instead of taking
in a C header file and producing Rust, it takes Rust in and produces a C
header file. Since the C header file represents the main computer-readable
Foreign Function Interfaces 209
description of your crate’s FFI, I recommend manually looking it over to
make sure the autogenerated C code isn’t too unwieldy, though in general
cbindgen tends to produce fairly reasonable code. If it doesn’t, file a bug!
C++
I’ve mainly focused on C in this chapter as it’s the language most commonly
used to describe cross-language interfaces for libraries you can link against.
Nearly every programming language provides some way to interact with C
libraries, since they are so ubiquitous. While C++ feels closely related to C,
and many high-profile libraries are written in C++, it’s a very different beast
when it comes to FFI. Generating types and signatures to match a C header is
relatively straightforward, but that is not at all the case for C++. At the time of
writing, bindgen has decent support for generating bindings to C++, but they
are often lacking in ergonomics. For example, you generally have to manually
call constructors, destructors, overloaded operators, and the like. Some C++
features like template specialization also aren’t supported at all. If you do have
to interface with C++, I recommend you give the cxx crate a try.
Summary
In this chapter, we’ve covered how to use the extern keyword to call out of
Rust into external code, as well as how to use it to make Rust code accessible
to external code. We’ve also discussed how to align Rust types with types on
the other side of the FFI boundary, and some of the common pitfalls in trying
to get code written in two different languages to mesh well. Finally, we
talked about the bindgen and cbindgen tools, which make the experience of
keeping FFI bindings up to date much more pleasant. In the next chapter,
we’ll look at how to use Rust in more restricted environments, like embedded
devices, where the standard library may not be available and where
even a simple operation like allocating memory may not be possible.

### 12 RUST WITHOUT THE STANDARD LIBRARY

Rust is intended to be a language for systems
programming, but it isn’t always clear
what that really means. At the very least,
a systems programming language is usually
expected to allow the programmer to write programs
that do not rely on the operating system and can run
directly on the hardware, whether that is a thousandcore
supercomputer or an embedded device with
a single-
core ARM processor with a clock speed of
72MHz and 256KiB of memory.
In this chapter, we’ll take a look at how you can use Rust in unorthodox
environments, such as those without an operating system, or those that
don’t even have the ability to dynamically allocate memory! Much of our
discussion will focus on the #![no_std] attribute, but we’ll also investigate
212 Chapter 12
Rust’s alloc module, the Rust runtime (yes, Rust does technically have a
runtime), and some of the tricks you have to play to write up a Rust binary
for use in such an environment.
Opting Out of the Standard Library
As a language, Rust consists of multiple independent pieces. First there’s
the compiler, which dictates the grammar of the Rust language and implements
type checking, borrow checking, and the final conversion into
machine-runnable code. Then there’s the standard library, std, which
implements all the useful common functionality that most programs
need—things like file and network access, a notion of time, facilities for
printing and reading user input, and so on. But std itself is also a composite,
building on top of two other, more fundamental libraries called core
and alloc. In fact, many of the types and functions in std are just re-exports
from those two libraries.
The core library sits at the bottom of the standard library pyramid and
contains any functionality that depends on nothing but the Rust language
itself and the hardware the resulting program is running on—things like
sorting algorithms, marker types, fundamental types such as Option and
Result, low-level operations such as atomic memory access methods, and
compiler hints. The core library works as if the operating system does
not exist, so there is no standard input, no filesystem, and no network.
Similarly, there is no memory allocator, so types like Box, Vec, and HashMap
are nowhere to be seen.
Above core sits alloc, which holds all the functionality that depends
on dynamic memory allocation, such as collections, smart pointers, and
dynamically allocated strings (String). We’ll get back to alloc in the next
section.
Most of the time, because std re-exports everything in core and
alloc, developers do not need to know about the differences among the
three libraries. This means that even though Option technically lives in
core::option::Option, you can access it through std::option::Option.
However, in an unorthodox environment, such as on an embedded
device where there is no operating system, the distinction is crucial. While
it’s fine to use an Iterator or to sort a list of numbers, an embedded device
may simply have no meaningful way to access a file (as that requires a filesystem)
or print to the terminal (as that requires a terminal)—so there’s
no File or println!. Furthermore, the device may have so little memory that
dynamic memory allocation is a luxury you can’t afford, and thus anything
that allocates memory on the fly is a no-go—say goodbye to Box and Vec.
Rather than force developers to carefully avoid those basic constructs
in such environments, Rust provides a way to opt out of anything but the
core functionality of the language: the #![no_std] attribute. This is a cratelevel
attribute (#!) that switches the prelude (see the box on page 213) for
the crate from std::prelude to core::prelude so that you don’t accidentally
depend on anything outside of core that might not work in your target
environment.
Rust Without the Standard Library 213
However, that is all the #![no_std] attribute does—it does not prevent
you from bringing in the standard library explicitly with extern std. This
may be surprising, as it means a crate marked #![no_std] may in fact not be
compatible with a target environment that does not support std, but this
design decision was intentional: it allows you to mark your crate as being
no_std-compatible but to still use features from the standard library when
certain features are enabled. For example, many crates have a feature named
std that, when enabled, gives access to more sophisticated APIs and integrations
with types that live in std. This allows crate authors to both supply the
core implementation for constrained use cases and add bells and whistles
for consumers on more standard platforms.
**NOTE** Since features should be additive, prefer an std-enabling feature to an std-disabling
one. Otherwise, if any crate in a consumer’s dependency graph enables the no-std
feature, all consumers will be given access only to the bare-bones API without std support,
which may then mean that APIs they depend on aren’t available, causing them
to no longer compile.
THE PRELUDE
Have you ever wondered why there are some types and traits—like Box, Iterator,
Option, and Clone—that are available in every Rust file without you needing to
use them? Or why you don’t need to use any of the macros in the standard library
(like vec![])? The reason is that every Rust module automatically imports the Rust
standard prelude with an implicit use std::prelude::rust_2021::* (or similar
for other editions), which brings all the exports from the crate’s chosen edition’s
prelude into scope. The prelude modules themselves aren’t special beyond this
auto-
inclusion—they are merely collections of pub use statements for key types,
traits, and macros that the Rust developers expect to be commonly used.
Dynamic Memory Allocation
As we discussed in Chapter 1, a machine has many different regions of
memory, and each one serves a distinct purpose. There’s static memory for
your program code and static variables, there’s the stack for function-local
variables and function arguments, and there’s the heap for, well, everything
else. The heap supports allocating variably sized regions of memory
at runtime, and those allocations stick around for however long you want
them to. This makes heap memory extremely versatile, and as a result, you
find it used everywhere. Vec, String, Arc and Rc, and the collection types are
all implemented in heap memory, which allows them to grow and shrink
over time and to be returned from functions without the borrow checker
complaining.
214 Chapter 12
Behind the scenes, the heap is really just a huge chunk of contiguous
memory that is managed by an allocator. It’s the allocator that provides the
illusion of distinct allocations in the heap, ensuring that those allocations do
not overlap and that regions of memory that are no longer in use are reused.
By default Rust uses the system allocator, which is generally the one dictated
by the standard C library. This works well for most use cases, but if necessary,
you can override which allocator Rust will use through the GlobalAlloc trait
combined with the #[global_allocator] attribute, which requires an implementation
of an alloc method for allocating a new segment of memory and
dealloc for returning a past allocation to the allocator to reuse.
In environments without an operating system, the standard C library
is also generally not available, and so neither is the standard system allocator.
For that reason, #![no_std] also excludes all types that rely on dynamic
memory allocation. But since it’s entirely possible to implement a memory
allocator without access to a full-blown operating system, Rust allows you
to opt back into just the part of the Rust standard library that requires an
allocator without opting into all of std through the alloc crate. The alloc
crate comes with the standard Rust toolchain (just like core and std) and
contains most of your favorite heap-allocation types, like Box, Arc, String,
Vec, and BTreeMap. HashMap is not among them, since it relies on random number
generation for its key hashing, which is an operating system facility. To
use types from alloc in a no_std context, all you have to do is replace any
imports of those types that previously had use std:: with use alloc:: instead.
Do keep in mind, though, that depending on alloc means your #![no_std]
crate will no longer be usable by any program that disallows dynamic memory
allocation, either because it doesn’t have an allocator or because it has
too little memory to permit dynamic memory allocation in the first place.
**NOTE** Some programming domains, like the Linux kernel, may allow dynamic memory
allocation only if out-of-memory errors are handled gracefully (that is, without panicking).
For such use cases, you’ll want to provide try_ versions of any methods you
expose that might allocate. The try_methods should use fallible methods of any inner
types (like the currently unstable Box::try_new or Vec::try_reserve) rather than ones
that just panic (like Box::new or Vec::reserve) and propagate those errors out to the
caller, who can then handle them appropriately.
It might strike you as odd that it’s possible to write nontrivial crates that
use only core. After all, they can’t use collections, the String type, the network,
or the filesystem, and they don’t even have a notion of time! The trick
to core-only crates is to utilize the stack and static allocations. For example,
for a heapless vector, you allocate enough memory up front—either in static
memory or in a function’s stack frame—for the largest number of elements
you expect the vector to be able to hold, and then augment it with a usize
that tracks how many elements it currently holds. To push to the vector, you
write to the next element in the (statically sized) array and increment a variable
that tracks the number of elements. If the vector’s length ever reaches
the static size, the next push fails. Listing 12-1 gives an example of such a
heapless vector type implemented using const generics.
Rust Without the Standard Library 215
struct ArrayVec<T, const N: usize> {
values: [Option<T>; N],
len: usize,
}
impl<T, const N: usize> ArrayVec<T, N> {
fn try_push(&mut self, t: T) -> Result<(), T> {
if self.len == N {
return Err(t);
}
self.values[self.len] = Some(t);
self.len += 1;
return Ok(());
}
}
Listing 12-1: A heapless vector type
We make ArrayVec generic over both the type of its elements, T, and the
maximum number of elements, N, and then represent the vector as an array
of N optional Ts. This structure always stores N Option<T>, so it has a size known
at compile time and can be stored on the stack, but it can still act like a vector
by using runtime information to inform how we access the array.
**NOTE** We could have implemented ArrayVec using [MaybeUninit<T>; N] to avoid the overhead
of the Option, but that would require using unsafe code, which isn’t warranted
for this example.
The Rust Runtime
You may have heard the claim that Rust doesn’t have a runtime. While
that’s true at a high level—it doesn’t have a garbage collector, an interpreter,
or a built-in user-level scheduler—it’s not really true in the strictest
sense. Specifically, Rust does have some special code that runs before your
main function and in response to certain special conditions in your code,
which really is a form of bare-bones runtime.
The Panic Handler
The first bit of such special code is Rust’s panic handler. When Rust code
panics by invoking panic! or panic_any, the panic handler dictates what
happens next. When the Rust runtime is available—as is the case on most
targets that supply std—the panic handler first invokes the panic hook set
via std::panic::set_hook, which prints a message and optionally a backtrace
to standard error by default. It then either unwinds the current thread’s
stack or aborts the process, depending on the panic setting chosen for current
compilation (either through Cargo configuration or arguments passed
directly to rustc).
However, not all targets provide a panic handler. For example, most
embedded targets do not, as there isn’t necessarily a single implementation
that makes sense across all the uses for such a target. For targets that don’t
216 Chapter 12
supply a panic handler, Rust still needs to know what to do when a panic
occurs. To that end, we can use the #[panic_handler] attribute to decorate a
single function in the program with the signature fn(&PanicInfo) -> !. That
function is called whenever the program invokes a panic, and it is passed
information about the panic in the form of a core::panic::PanicInfo. What
the function does with that information is entirely unspecified, but it can
never return (as indicated by the ! return type). This is important, since the
Rust compiler assumes that no code that follows a panic is run.
There are many valid ways for a panic handler to avoid returning. The
standard panic handler unwinds the thread’s stack and then terminates the
thread, but a panic handler can also halt the thread using loop {}, abort the
program, or do anything else that makes sense for the target platform, even
as far as resetting the device.
Program Initialization
Contrary to popular belief, the main function is not the first thing that runs
in a Rust program. Instead, the main symbol in a Rust binary actually points
to a function in the standard library called lang_start. That function performs
the (fairly minimal) setup for the Rust runtime, including stashing
the program’s command-line arguments in a place where std::env::args can
get to them, setting the name of the main thread, handling panics in the
main function, flushing standard output on program exit, and setting up signal
handlers. The lang_start function in turn calls the main function defined
in your crate, which then doesn’t need to think about how, for example,
Windows and Linux differ in how command-line arguments are passed in.
This arrangement works well on platforms where all of that setup is sensible
and supported, but it presents a problem on embedded platforms where
main memory may not even be accessible when the program starts. On such
platforms, you’ll generally want to opt out of the Rust initialization code
entirely using the #![no_main] crate-level attribute. This attribute completely
omits lang_start, meaning you as the developer must figure out how the program
should be started, such as by declaring a function with #[export_name =
"main"] that matches the expected launch sequence for the target platform.
**NOTE** On platforms that truly run no code before they jump to the defined start symbol,
like most embedded devices, the initial values of static variables may not even match
what’s specified in the source code. In such cases, your initialization function will
need to explicitly initialize the various static memory segments with the initial data
values specified in your program binary.
The Out-of-Memory Handler
If you write a program that wishes to use alloc but is built for a platform
that does not supply an allocator, you must dictate which allocator to use
using the #[global_allocator] attribute mentioned earlier in the chapter.
But you also have to specify what happens if that global allocator fails
Rust Without the Standard Library 217
to allocate memory. Specifically, you need to define an out-of-memory handler
to say what should happen if an infallible operation like Vec::push needs to
allocate more memory, but the allocator cannot supply it.
The default behavior of the out-of-memory handler on std-enabled
platforms is to print an error message to standard error and then abort the
process. However, on a platform that, for example, doesn’t have standard
error, that obviously won’t work. At the time of writing, on such platforms
your program must explicitly define an out-of-memory handler using the
unstable attribute #[lang = "oom"]. Keep in mind that the handler should
almost certainly prevent future execution, as otherwise the code that tried
to allocate will continue executing without knowing that it did not receive
the memory it asked for!
**NOTE** By the time you read this, the out-of-memory handler may already have been stabilized
under a permanent name (#[alloc_error_handler], most likely). Work is also underway
to give the default std out-of-memory handler the same kind of “hook” functionality
as Rust’s panic handler, so that code can change the out-of-memory behavior on
the fly through a method like set_alloc_error_hook.
Low-Level Memory Accesses
In Chapter 10, we discussed the fact that the compiler is given a fair amount
of leeway in how it turns your program statements into machine instructions,
and that the CPU is allowed some wiggle room to execute instructions
out of order. Normally, the shortcuts and optimizations that the
compiler and CPU can take advantage of are invisible to the semantics of
the program—you can’t generally tell whether, say, two reads have been
reordered relative to each other or whether two reads from the same memory
location actually result in two CPU load instructions. This is by design.
The language and hardware designers carefully specified what semantics
programmers commonly expect from their code when it runs so that your
code generally does what you expect it to.
However, no_std programming sometimes takes you beyond the usual
border of “invisible optimizations.” In particular, you’ll often communicate
with hardware devices through memory mapping, where the internal state
of the device is made available in carefully chosen regions in memory. For
example, while your computer starts up, the memory address range 0xA0000–
0xBFFFF maps to a crude graphics rendering pipeline; writes to individual
bytes in that range will change particular pixels (or blocks, depending on
the mode) on the screen.
When you’re interacting with device-mapped memory, the device
may implement custom behavior for each memory access to that region of
memory, so the assumptions your CPU and compiler make about regular
memory loads and stores may no longer hold. For instance, it is common
for hardware devices to have memory-mapped registers that are modified
218 Chapter 12
when they’re read, meaning the reads have side effects. In such cases, the
compiler can’t safely elide a memory store operation if you read the same
memory address twice in a row!
A similar issue arises when program execution is suddenly diverted in
ways that aren’t represented in the code and thus that the compiler cannot
expect. Execution might be diverted if there is no underlying operating
system to handle processor exceptions or interrupts, or if a process
receives a signal that interrupts execution. In those cases, the execution
of the active segment of code is stopped, and the CPU starts executing
instructions in the event handler for whatever event triggered the diversion
instead. Normally, since the compiler can anticipate all possible executions,
it arranges its optimizations so that executions cannot observe when operations
have been performed out of order or optimized away. However, since
the compiler can’t predict these exceptional jumps, it also cannot plan for
them to be oblivious to its optimizations, so these event handlers might
actually observe instructions that have run in a different order than those
in the original program code.
To deal with these exceptional situations, Rust provides volatile memory
operations that cannot be elided or reordered with respect to other volatile
operations. These operations take the form of std::ptr::read_volatile
and std::ptr::write_volatile. Volatile operations are exactly the right fit
for accessing memory-mapped hardware resources: they map directly to
memory access operations with no compiler trickery, and the guarantee
that volatile operations aren’t reordered relative to one another ensures
that hardware operations with possible side effects don’t happen out of
order even when they would normally look interchangeable (such as a load
of one address and a store to a different address). The no-reordering guarantee
also helps the exceptional execution situation, as long as any code
that touches memory accessed in an exceptional context uses only volatile
memory operations.
**NOTE** There is also a std::sync::atomic::compiler_fence function that prevents the compiler
from reordering non-volatile memory accesses. You’ll very rarely need a compiler
fence, but its documentation is an interesting read.
INCLUDING ASSEMBLY CODE
These days, you rarely need to drop down to writing assembly code to accomplish
any given task. But for low-level hardware programming where you need
to initialize CPUs at boot or issue strange instructions to manipulate memory
mappings, assembly code is still sometimes required. At the time of writing,
there is an RFC and a mostly complete implementation of inline assembly syntax
on nightly Rust, but nothing has been stabilized yet, so I won’t discuss the syntax
in this book.
Rust Without the Standard Library 219
It’s still possible to write assembly on stable Rust—you just need to get a little
creative. Specifically, remember build scripts from Chapter 11? Well, Cargo
build scripts can emit certain special directives to standard output to augment
Cargo’s standard build process, including cargo:rustc-link-lib=static=xyz
to link the static library file libxyz.a into the final binary, and cargo:rustc-linksearch:/
some/path to add /some/path to the search path for link objects.
Using those, we can add a build.rs to the project that compiles a standalone
assembly file (.s) to an object file (.o) using the target platform’s compiler and
then repackages it into a static archive (.a) using the appropriate archiving
tool (usually ar). The project then emits those two Cargo directives, pointing at
where it placed the static archive—probably in OUT_DIR—and we’re off to the
races! If the target platform doesn’t change, you can even include the precompiled
.a when publishing your crate so that consumers don’t need to rebuild it.
Misuse-Resistant Hardware Abstraction
Rust’s type system excels at encapsulating unsafe, hairy, and otherwise
unpleasant code behind safe, ergonomic interfaces. Nowhere is that
more important than in the infamously complex world of low-level systems
programming, littered with magic hardware-defined values pulled
from obscure manuals and mysterious undocumented assembly instruction
incantations to get devices into just the right state. And all that in a
space where a runtime error might crash more than just a user program!
In no_std programs, it is immensely important to use the type system to
make illegal states impossible to represent, as we discussed in Chapter 3. If
certain combinations of register values cannot occur at the same time, then
create a single type whose type parameters indicate the current state of the
relevant registers, and implement only legal transitions on it, like we did for
the rocket example in Listing 3-2.
**NOTE** Make sure to also review the advice from Chapter 3 on API design—all of that
applies in the context of no_std programs as well!
For example, consider a pair of registers where at most one register
should be “on” at any given point in time. Listing 12-2 shows how you can
represent that in a (single-threaded) program in a way makes it impossible
to write code that violates that invariant.
// raw register address -- private submodule
mod registers;
pub struct On;
pub struct Off;
pub struct Pair<R1, R2>(PhantomData<(R1, R2)>);
impl Pair<Off, Off> {
pub fn get() -> Option<Self> {
220 Chapter 12
static mut PAIR_TAKEN: bool = false;
if unsafe { PAIR_TAKEN } {
None
} else {
// Ensure initial state is correct.
registers::off("r1");
registers::off("r2");
unsafe { PAIR_TAKEN = true };
Some(Pair(PhantomData))
}
}
pub fn first_on(self) -> Pair<On, Off> {
registers::set_on("r1");
Pair(PhantomData)
}
// .. and inverse for -> Pair<Off, On>
}
impl Pair<On, Off> {
pub fn off(self) -> Pair<Off, Off> {
registers::set_off("r1");
Pair(PhantomData)
}
}
// .. and inverse for Pair<Off, On>
Listing 12-2: Statically ensuring correct operation
There are a few noteworthy patterns in this code. The first is that we
ensure only a single instance of Pair ever exists by checking a private static
Boolean in its only constructor and making all methods consume self. We
then ensure that the initial state is valid and that only valid state transitions
are possible to express, and therefore the invariant must hold globally.
The second noteworthy pattern in Listing 12-2 is that we use PhantomData
to take advantage of zero-sized types and represent runtime information
statically. That is, at any given point in the code the types tell us what the
runtime state must be, and therefore we don’t need to track or check any
state related to the registers at runtime. There’s no need to check that r2
isn’t already on when we’re asked to enable r1, since the types prevent writing
a program in which that is the case.
Cross-Compilation
Usually, you’ll write no_std programs on a computer with a full-fledged
operating system running and all the niceties of modern hardware, but ultimately
run it on a dinky hardware device with 93/4 bits of RAM and a sock
for a CPU. That calls for cross-compilation—you need to compile the code in
your development environment, but compile it for the sock. That’s not the
only context in which cross-compilation is important, though. For example,
it’s increasingly common to have one build pipeline produce binary
Rust Without the Standard Library 221
artifacts for all consumer platforms rather than trying to have a build pipeline
for every platform your consumers may be using, and that means using
cross-compilation.
**NOTE** If you’re actually compiling for something sock-like with limited memory, or even
something as fancy as a potato, you may want to set the opt-level Cargo configuration
to "s" to optimize for smaller binary sizes.
Cross-compiling involves two platforms: the host platform and the target
platform. The host platform is the one doing the compiling, and the target
platform is the one that will eventually run the output of the compilation.
We specify platforms as target triples, which take the form machine-vendor-
os.
The machine part dictates the machine architecture the code will run on,
such as x86_64, armv7, or wasm32, and tells the compiler what instruction set to
use for the emitted machine code. The vendor part generally takes the value
of pc on Windows, apple on macOS and iOS, and unknown everywhere else,
and doesn’t affect compilation in any meaningful way; it’s mostly irrelevant
and can even be left out. The os part tells the compiler what format to use
for the final binary artifacts, so a value of linux dictates Linux .so files, windows
dictates Windows .dll files, and so on.
**NOTE** By default, Cargo assumes that the target platform is the same as the host platform,
which is why you generally never have to tell Cargo to, say, compile for Linux when
you’re already on Linux. Sometimes you may want to use --target even if the CPU
and OS of the target are the same, though, such as to target the musl implementation
of libc.
To tell Cargo to cross-compile, you simply pass it the --target <target
triple>
argument with your triple of choice. Cargo will then take care of
forwarding that information to the Rust compiler so that it generates binary
artifacts that will work on the given target platform. Cargo will also take care
to use the appropriate version of the standard library for that platform—after
all, the standard library contains a lot of conditional compilation directives
(using #[cfg(...)]) so that the right system calls get invoked and the right
architecture-specific implementations are used, so we can’t use the standard
library for the host platform on the target.
The target platform also dictates what components of the standard
library are available. For example, while x86_64-unknown-linux-gnu includes
the full std library, something like thumbv7m-none-eabi does no, and doesn’t
even define an allocator, so if you use alloc without defining one explicitly,
you’ll get a build error. This comes in handy for testing that code you write
actually doesn’t require std (recall that even with #![no_std] you can still have
use std::, since no_std opts out of only the std prelude). If you have your continuous
integration pipeline build your crate with --target thumbv7m-none-eabi,
any attempt to access components from anything but core will trigger a build
failure. Crucially, this will also check that your crate doesn’t accidentally
bring in dependencies that themselves use items from std (or alloc).
222 Chapter 12
PLATFORM SUPPORT
The standard Rust installer, Rustup, doesn’t install the standard library for all the
target triples that Rust supports by default. That would be a waste of space and
bandwidth. Instead, you have to use the command rustup target add to install
the appropriate standard library versions for additional targets. If no version of
the standard library exists for your target platform, you’ll have to compile it from
source yourself by adding the rust-src Rustup component and using Cargo’s
(currently unstable) build-std feature to also build std (and/or core and alloc)
when building any crate.
If your target is not supported by the Rust compiler—that is, if rustc doesn’t
even know about your target triple—you’ll have to go one step further and teach
rustc about the properties of the triple using a custom target specification. How
you do that is both currently unstable and beyond the scope of this book, but a
search for “custom target specification json” is a good place to start.
Summary
In this chapter, we’ve covered what lies beneath the standard library—or,
more precisely, beneath std. We’ve gone over what you get with core, how
you can extend your non-std reach with alloc, and what the (tiny) Rust runtime
adds to your programs to make fn main work. We’ve also taken a look
at how you can interact with device-mapped memory and otherwise handle
the unorthodox execution patterns that can happen at the very lowest level
of hardware programming, and how to safely encapsulate at least some of
the oddities of hardware in the Rust type system. Next, we’ll move from the
very small to the very large by discussing how to navigate, understand, and
maybe even contribute to the larger Rust ecosystem.

### 13 THE RUST ECOSYSTEM

Programming rarely happens in a vacuum
these days—nearly every Rust crate you
build is likely to take dependencies on some
code that wasn’t written by you. Whether this
trend is good, bad, or a little of both is a subject of
heavy debate, but either way, it’s a reality of today’s
developer experience.
In this brave new interdependent world, it’s more important than ever
to have a solid grasp of what libraries and tools are available and to stay up
to date on the latest and greatest of what the Rust community has to offer.
This chapter is dedicated to how you can leverage, track, understand, and
contribute back to the Rust ecosystem. Since this is the final chapter, in the
closing section I’ll also provide some suggestions of additional resources
you can explore to continue developing your Rust skills.
224 Chapter 13
What’s Out There?
Despite its relative youth, Rust already has an ecosystem large enough that
it’s hard to keep track of everything that’s available. If you know what you
want, you may be able to search your way to a set of appropriate crates and
then use download statistics and superficial vibe-checks on each crate’s
repository to determine which may make for reasonable dependencies.
However, there’s also a plethora of tools, crates, and general language features
that you might not necessarily know to look for that could potentially
save you countless hours and difficult design decisions.
In this section, I’ll go through some of the tools, libraries, and Rust features
I have found helpful over the years in the hopes that they may come
in useful for you at some point too!
Tools
First off, here are some Rust tools I find myself using regularly that you
should add to your toolbelt:
cargo-deny
Provides a way to lint your dependency graph. At the time of writing,
you can use cargo-deny to allow only certain licenses, deny-list crates or
specific crate versions, detect dependencies with known vulnerabilities
or that use Git sources, and detect crates that appear multiple times
with different versions in the dependency graph. By the time you’re
reading this, there may be even more handy lints in place.
cargo-expand
Expands macros in a given crate and lets you inspect the output, which
makes it much easier to spot mistakes deep down in macro transcribers
or procedural macros. cargo-expand is an invaluable tool when you’re
writing your own macros.
cargo-hack
Helps you check that your crate works with any combination of features
enabled. The tool presents an interface similar to that of Cargo itself
(like cargo check, build, and test) but gives you the ability to run a given
command with all possible combinations (the powerset) of the crate’s
features.
cargo-llvm-lines
Analyzes the mapping from Rust code to the intermediate representation
(IR) that’s passed to the part of the Rust compiler that actually
generates machine code (LLVM), and tells you which bits of Rust code
produce the largest IR. This is useful because a larger IR means longer
compile times, so identifying what Rust code generates a bigger IR (due
The Rust Ecosystem 225
to, for example, monomorphization) can highlight opportunities for
reducing compile times.
cargo-outdated
Checks whether any of your dependencies, either direct or transitive,
have newer versions available. Crucially, unlike cargo update, it even
tells you about new major versions, so it’s an essential tool for checking
if you’re missing out on newer versions due to an outdated major
version specifier. Just keep in mind that bumping the major version of
a dependency may be a breaking change for your crate if you expose
that dependency’s types in your interface!
cargo-udeps
Identifies any dependencies listed in your Cargo.toml that are never actually
used. Maybe you used them in the past but they’ve since become
redundant, or maybe they should be moved to dev-dependencies; whatever
the case, this tool helps you trim down bloat in your dependency
closure.
While they’re not specifically tools for developing Rust, I highly recommend
fd and ripgrep too—they’re excellent improvements over their predecessors
find and grep and also happen to be written in Rust themselves. I use
both every day.
Libraries
Next up are some useful but lesser-known crates that I reach for regularly,
and that I suspect I will continue to depend on for a long time:
bytes
Provides an efficient mechanism for passing around subslices of a single
piece of contiguous memory without having to copy or deal with lifetimes.
This is great in low-level networking code where you may need
multiple views into a single chunk of bytes, and copying is a no-no.
criterion
A statistics-driven benchmarking library that uses math to eliminate
noise from benchmark measurements and reliably detect changes in
performance over time. You should almost certainly be using it if you’re
including micro-benchmarks in your crate.
cxx
Provides a safe and ergonomic mechanism for calling C++ code from
Rust and Rust code from C++. If you’re willing to invest some time into
declaring your interfaces more thoroughly in advance in exchange for
much nicer cross-language compatibility, this library is well worth your
attention.
226 Chapter 13
flume
Implements a multi-producer, multi-consumer channel that is faster,
more flexible, and simpler than the one included with the Rust standard
library. It also supports both asynchronous and synchronous operation
and so is a great bridge between those two worlds.
hdrhistogram
A Rust port of the High Dynamic Range (HDR) histogram data structure,
which provides a compact representation of histograms across a
wide range of values. Anywhere you currently track averages or min/
max values, you should most likely be using an HDR histogram instead;
it can give you much better insight into the distribution of your metrics.
heapless
Supplies data structures that do not use the heap. Instead, heapless’s
data structures are all backed by static memory, which makes them
perfect for embedded contexts or other situations in which allocation is
undesirable.
itertools
Extends the Iterator trait from the standard library with lots of new
convenient methods for deduplication, grouping, and computing powersets.
These extension methods can significantly reduce boilerplate in
code, such as where you manually implement some common algorithm
over a sequence of values, like finding the min and max at the same
time (Itertools::minmax), or where you use a common pattern like checking
that an iterator has exactly one item (Itertools::exactly_one).
nix
Provides idiomatic bindings to system calls on Unix-like systems,
which allows for a much better experience than trying to cobble
together the C-compatible FFI types yourself when working with
something like libc directly.
pin-project
Provides macros that enforce the pinning safety invariants for annotated
types, which in turn provide a safe pinning interface to those
types. This allows you to avoid most of the hassle of getting Pin and
Unpin right for your own types. There’s also pin-project-lite, which
avoids the (currently) somewhat heavy dependency on the procedural
macro machinery at the cost of slightly worse ergonomics.
ring
Takes the good parts from the cryptography library BoringSSL,
written in C, and brings them to Rust through a fast, simple, and
The Rust Ecosystem 227
hard-to-misuse interface. It’s a great starting point if you need to use
cryptography in your crate. You’ve already most likely come across this
in the rustls library, which uses ring to provide a modern, secure-bydefault
TLS stack.
slab
Implements an efficient data structure to use in place of HashMap<Token, T>,
where Token is an opaque type used only to differentiate between entries in
the map. This kind of pattern comes up a lot when managing resources,
where the set of current resources must be managed centrally but individual
resources must also be accessible somehow.
static_assertions
Provides static assertions—that is, assertions that are evaluated at, and
thus may fail at, compile time. You can use it to assert things like that
a type implements a given trait (like Send) or is of a given size. I highly
recommend adding these kinds of assertions for code where those
guarantees are likely to be important.
structopt
Wraps the well-known argument parsing library clap and provides a way
to describe your application’s command line interface entirely using the
Rust type system (plus macro annotations). When you parse your application’s
arguments, you get a value of the type you defined, and you
thus get all the type checking benefits, like exhaustive matching and
IDE auto-complete.
thiserror
Makes writing custom enumerated error types, like the ones we discussed
in Chapter 4, a joy. It takes care of implementing the recommended traits
and following the established conventions and leaves you to define just
the critical bits that are unique to your application.
tower
Effectively takes the function signature async fn(Request) -> Response and
implements an entire ecosystem on top of it. At its core is the Service
trait, which represents a type that can turn a request into a response
(something I suspect may make its way into the standard library one
day). This is a great abstraction to build anything that looks like a service
on top of.
tracing
Provides all the plumbing needed to efficiently trace the execution of
your applications. Crucially, it is agnostic to the types of events you’re
tracing and what you want to do with those events. This library can be
228 Chapter 13
used for logging, metrics collection, debugging, profiling, and obviously
tracing, all with the same machinery and interfaces.
Rust Tooling
The Rust toolchain has a few features up its sleeve that you may not know
to look for. These are usually for very specific use cases, but if they match
yours, they can be lifesavers!
Rustup
Rustup, the Rust toolchain installer, does its job so efficiently that it tends
to fade into the background and get forgotten about. You’ll occasionally
use it to update your toolchain, set a directory override, or install a component,
but that’s about it. However, Rustup supports one very handy trick
that it’s worthwhile to know about: the toolchain override shorthand. You
can pass +toolchain as the first argument to any Rustup-managed binary,
and the binary will work as if you’d set an override for the given toolchain,
run the command, and then reset the override back to what it was previously.
So, cargo +nightly miri will run Miri using the nightly toolchain, and
cargo +1.53.0 check will check if the code compiles with Rust 1.53.0. The latter
comes in particularly handy for checking that you haven’t broken your
minimum supported Rust version contract.
Rustup also has a neat subcommand, doc, that opens a local copy of the
Rust standard library documentation for the current version of the Rust
compiler in your browser. This is invaluable if you’re developing on the go
without an internet connection!
Cargo
Cargo also has some handy features that aren’t always easy to discover.
The first of these is cargo tree, a Cargo subcommand built right into Cargo
itself for inspecting a crate’s dependency graph. This command’s primary
purpose is to print the dependency graph as a tree. This can be useful on
its own, but where cargo tree really shines is through the --invert option:
it takes a crate identifier and produces an inverted tree showing all the
dependency paths from the current crate that bring in that dependency.
So, for example, cargo tree -i rand will print all of the ways in which the
current crate depends on any version of rand, including through transitive
dependencies. This is invaluable if you want to eliminate a dependency, or
a particular version of a dependency, and wonder why it still keeps being
pulled in. You can also pass the -e features option to include information
about why each Cargo feature of the crate in question is enabled.
Speaking of Cargo subcommands, it’s really easy to write your own,
whether for sharing with other people or just for your own local development.
When Cargo is invoked with a subcommand it doesn’t recognize, it
checks whether a program by the name cargo-$subcommand exists. If it does,
Cargo invokes that program and passes it any arguments that were passed
The Rust Ecosystem 229
on the command line—so, cargo foo bar will invoke cargo-foo with the argument
bar. Cargo will even integrate this command with cargo help by translating
cargo help foo into a call to cargo-foo --help.
As you work on more Rust projects, you may notice that Cargo (and
Rust more generally) isn’t exactly forgiving when it comes to disk space.
Each project gets its own target directory for its compilation artifacts, and
over time you end up accumulating several identical copies of compiled
artifacts for common dependencies. Keeping artifacts for each project separate
is a sensible choice, as they aren’t necessarily compatible across projects
(say, if one project uses different compiler flags than another). But in most
developer environments, sharing build artifacts is entirely reasonable and
can save a fair amount of compilation time when switching between projects.
Luckily, configuring Cargo to share build artifacts is simple: just set
[build] target in your ~/.cargo/config.toml file to the directory you want those
shared artifacts to go in, and Cargo will take care of the rest. No more target
directories in sight! Just make sure you clean out that directory every
now and again too, and be aware that cargo clean will now clean all of your
projects’ build artifacts.
**NOTE** Using a shared build directory can cause problems for projects that assume that compiler
artifacts will always be under the target/ subdirectory, so watch out for that.
Also note that if a project does use different compiler flags, you’ll end up recompiling
affected dependencies every time you move into or out of that project. In such cases,
you’re best off overriding the target directory in that project’s Cargo configuration to a
distinct location.
Finally, if you ever feel like Cargo is taking a suspiciously long time to
build your crate, you can reach for the currently unstable Cargo -Ztimings
flag. Running Cargo with that flag outputs information about how long it
took to process each crate, how long build scripts took to run, what crates
had to wait for what other crates to finish compiling, and tons of other
useful metrics. This might highlight a particularly slow dependency chain
that you can then work to eliminate, or reveal a build script that compiles
a native dependency from scratch that you can make use system libraries
instead. If you want to dive even deeper, there’s also rustc -Ztime-passes,
which emits information about where time is spent inside of the compiler
for each crate—though that information is likely only useful if you’re looking
to contribute to the compiler itself.
rustc
The Rust compiler also has some lesser-known features that can prove useful
to enterprising developers. The first is the currently unstable -Zprinttype-
sizes argument, which prints the sizes of all the types in the current
crate. This produces a lot of information for all but the tiniest crates but
is immensely valuable when trying to determine the source of unexpected
time spent in calls to memcpy or to find ways to reduce memory use when allocating
lots of objects of a particular type. The -Zprint-type-sizes argument
230 Chapter 13
also displays the computed alignment and layout for each type, which may
point you to places where turning, say, a usize into a u32 could have a significant
impact on a type’s in-memory representation. After you debug a
particular type’s size, alignment, and layout, I recommend adding static
assertions to make sure that they don’t regress over time. You may also be
interested in the variant_size_differences lint, which issues a warning if a
crate contains enum types whose variants significantly differ in size.
**NOTE** To call rustc with particular flags, you have a few options: you can either set them in
the RUSTFLAGS environment variable or [build] rustflags in your .cargo/config.toml
to have them apply to every invocation of rustc from Cargo, or you can use cargo
rustc, which will pass any arguments you provide only to the rustc invocation for the
current crate.
If your profiling samples look weird, with stack frames reordered or
entirely missing, you could also try -Cforce-frame-pointers = yes. Frame pointers
provide a more reliable way to unwind the stack—which is done a lot
during profiling—at the cost of an extra register being used for function
calls. Even though stack unwinding should work fine with just regular debug
symbols enabled (remember to set debug = true when using the release profile),
that’s not always the case, and frame pointers may take care of any
issues you do encounter.
The Standard Library
The Rust standard library is generally considered to be small compared
to those of other programming languages, but what it lacks in breadth, it
makes up for in depth; you won’t find a web server implementation or an
X.509 certificate parser in Rust’s standard library, but you will find more
than 40 different methods on the Option type alongside over 20 trait implementations.
For the types it does include, Rust does its best to make available
any relevant functionality that meaningfully improves ergonomics, so
you avoid all that verbose boilerplate that can so easily arise otherwise. In
this section, I’ll present some types, macros, functions, and methods from
the standard library that you may not have come across before, but that can
often simplify or improve (or both) your code.
Macros and Functions
Let’s start off with a few free-standing utilities. First up is the write! macro,
which lets you use format strings to write into a file, a network socket, or anything
else that implements Write. You may already be familiar with it—but
one little-known feature of write! is that it works with both std::io::Write and
std::fmt::Write, which means you can use it to write formatted text directly into
a String. That is, you can write use std::fmt::Write; write!(&mut s, "{}+1={}", x,
x + 1); to append the formatted text to the String s!
The iter::once function takes any value and produces an iterator that
yields that value once. This comes in handy when calling functions that take
The Rust Ecosystem 231
iterators if you don’t want to allocate, or when combined with Iterator::chain
to append a single item to an existing iterator.
We briefly talked about mem::replace in Chapter 1, but it’s worth bringing
up again in case you missed it. This function takes an exclusive reference
to a T and an owned T, swaps the two so that the referent is now the
owned T, and returns ownership of the previous referent. This is useful
when you need to take ownership of a value in a situation where you have
only an exclusive reference, such as in implementations of Drop. See also
mem::take for when T: Default.
Types
Next, let’s look at some handy standard library types. The BufReader and
BufWriter types are a must for I/O operations that issue many small read or
write calls to the underlying I/O resource. These types wrap the respective
underlying Read or Write and implement Read and Write themselves, but
they additionally buffer the operations to the I/O resource such that many
small reads do only one large read, and many small writes do only one large
write. This can significantly improve performance as you don’t have to cross
the system call barrier into the operating system as often.
The Cow type, mentioned in Chapter 3, is useful when you want flexibility
in what types you hold or need flexibility in what you return. You’ll
rarely use Cow as a function argument (recall that you should let the caller
allocate if necessary), but it’s invaluable as a return type as it allows you
to accurately represent the return types of functions that may or may not
allocate. It’s also a perfect fit for types that can be used as inputs or outputs,
such as core types in RPC-like APIs. Say we have a type EntityIdentifier like
in Listing 13-1 that is used in an RPC service interface.
struct EntityIdentifier {
namespace: String,
name: String,
}
Listing 13-1: A representation of a combined input/output type that requires allocation
Now imagine two methods: get_entity takes an EntityIdentifier as an
argument, and find_by returns an EntityIdentifier based on some search
parameters. The get_entity method requires only a reference since the
identifier will (presumably) be serialized before being sent to the server.
But for find_by, the entity will be deserialized from the server response and
must therefore be represented as an owned value. If we make get_entity
take &EntityIdentifier, it will mean callers must still allocate owned Strings
to call get_entity even though that’s not required by the interface, since
it’s required to construct an EntityIdentifier in the first place! We could
instead introduce a separate type for get_entity, EntityIdenifierRef, that
holds only &str types, but then we’d have two types to represent one thing.
Cow to the rescue! Listing 13-2 shows an EntityIdentifier that instead holds
Cows internally.
232 Chapter 13
struct EntityIdentifier<'a> {
namespace: Cow<'a, str>,
name: Cow<'a str>,
}
Listing 13-2: A representation of a combined input/output type that does not require
allocation
With this construction, get_entity can take any EntityIdentifier<'_>,
which allows the caller to use just references to call the method. And find_
by can return EntityIdentifier<'static>, where all the fields are Cow::Owned.
One type shared across both interfaces, with no unnecessary allocation
requirements!
**NOTE** If you implement a type this way, I recommend you also provide an into_owned
method that turns an <'a> instance into a <'static> instance by calling Cow::into_
owned on all the fields. Otherwise, users will have no way to make longer-lasting
clones of your type when all they have is an <'a>.
The std::sync::Once type is a synchronization primitive that lets you run
a given piece of code exactly once, at initialization time. This is great for
initialization that’s part of an FFI where the library on the other side of the
FFI boundary requires that the initialization is performed only once.
The VecDeque type is an oft-neglected member of std::collections that
I find myself reaching for surprisingly often—basically, whenever I need
a stack or a queue. Its interface is similar to that of a Vec, and like Vec its
in-memory representation is a single chunk of memory. The difference is
that VecDeque keeps track of both the start and end of the actual data in that
single allocation. This allows constant-time push and pop from either side of
the VecDeque, meaning it can be used as a stack, as a queue, or even both at
the same time. The cost you pay is that the values are no longer necessarily
contiguous in memory (they may have wrapped around), which means that
VecDeque<T> does not implement AsRef<[T]>.
Methods
Let’s round off with a rapid-fire look at some neat methods. First up is
Arc::make_mut, which takes a &mut Arc<T> and gives you a &mut T. If the Arc is
the last one in existence, it gives you the T that was behind the Arc; otherwise,
it allocates a new Arc<T> that holds a clone of the T, swaps that in for
the currently referenced Arc, and then gives &mut to the T in the new singleton
Arc.
The Clone::clone_from method is an alternative form of .clone() that
lets you reuse an instance of the type you clone rather than allocate a new
one. In other words, if you already have an x: T, you can do x.clone_from(y)
rather than x = y.clone(), and you might save yourself some allocations.
std::fmt::Formatter::debug_*is by far the easiest way to implement Debug
yourself if #[derive(Debug)] won’t work for your use case, such as if you want
to include only some fields or expose information that isn’t exposed by the
The Rust Ecosystem 233
Debug implementations of your type’s fields. When implementing the fmt
method of Debug, simply call the appropriate debug_method on the Formatter
that’s passed in (debug_struct or debug_map, for example), call the included
methods on the resulting type to fill in details about the type (like field to
add a field or entries to add a key/value entry), and then call finish.
Instant::elapsed returns the Duration since an Instant was created. This
is much more concise than the common approach of creating a new Instant
and subtracting the earlier instance.
Option::as_deref takes an Option<P> where P: Deref and returns
Option<&P::Target> (there’s also an as_deref_mut method). This simple operation
can make functional transformation chains that operate on Option
much cleaner by avoiding the inscrutable .as_ref().map(|r| &*_r).
Ord::clamp lets you take any type that implements Ord and clamp it
between two other values of a given range. That is, given a lower limit min
and an upper limit max, x.clamp(min, max) returns min if x is less than min, max
if x is greater than max, and x otherwise.
Result::transpose and its counterpart Option::transpose invert types that
nest Result and Option. That is, transposing a Result<Option<T>, E> gives an
Option<Result<T, E>>, and vice versa. When combined with ?, this operation
can make for cleaner code when working with Iterator::next and similar
methods in fallible contexts.
Vec::swap_remove is Vec::remove’s faster twin. Vec::remove preserves the
order of the vector, which means that to remove an element in the middle,
it must shift all the later elements in the vector down by one. This can be
very slow for large vectors. Vec::swap_remove, on the other hand, swaps the
to-be-removed element with the last element and then truncates the vector’s
length by one, which is a constant-time operation. Be aware, though, that it
will shuffle your vector around and thus invalidate old indexes!
Patterns in the Wild
As you start exploring codebases that aren’t your own, you’ll likely come
across a couple of common Rust patterns that we haven’t discussed in the
book so far. Knowing about them will make it easier to recognize them, and
thus understand their purpose, when you do encounter them. You may even
find use for them in your own codebase one day!
Index Pointers
Index pointers allow you to store multiple references to data within a data
structure without running afoul of the borrow checker. For example, if you
want to store a collection of data so that it can be efficiently accessed in
more than one way, such as by keeping one HashMap keyed by one field and
one keyed by a different field, you don’t want to store the underlying data
multiple times too. You could use Arc or Rc, but they use dynamic reference
counting that introduces unnecessary overhead, and the extra bookkeeping
requires you to store additional bytes per entry. You could use references,
but the lifetimes become difficult if not impossible to manage because the
234 Chapter 13
data and the references live in the same data structure (it’s a self-referential
data structure, as we discussed in Chapter 8). You could use raw pointers
combined with Pin to ensure the pointers remain valid, but that introduces
a lot of complexity as well as unsafety you then need to carefully consider.
Most crates use index pointers—or, as I like to call them, indeferences—
instead. The idea is simple: store each data entry in some indexable data
structure like a Vec, and then store just the index in a derived data structure.
To then perform an operation, first use the derived data structure
to efficiently find the data index, and then use the index to retrieve the
referenced data. No lifetimes needed—and you can even have cycles in the
derived data representation if you wish!
The indexmap crate, which provides a HashMap implementation where the
iteration order matches the map insertion order, provides a good example
of this pattern. The implementation has to store the keys in two places,
both in the map of keys to values and in the list of all the keys, but it obviously
doesn’t want to keep two copies in case the key type itself is large. So,
it uses index pointers. Specifically, it keeps all the key/value pairs in a single
Vec and then keeps a mapping from key hashes to Vec indexes. To iterate
over all the elements of the map, it just walks the Vec. To look up a given
key, it hashes that key, looks that hash up in the mapping, which yields the
key’s index in the Vec (the index pointer), and then uses that to get the key’s
value from the Vec.
The petgraph crate, which implements graph data structures and algorithms,
also uses this pattern. The crate stores one Vec of all node values
and another of all edge values and then only ever uses the indexes into
those Vecs to refer to a node or edge. So, for example, the two nodes associated
with an edge are stored in that edge simply as two u32s, rather than as
references or reference-counted values.
The trick lies in how you support deletions. To delete a data entry, you
first need to search for its index in all of the derived data structures and
remove the corresponding entries, and then you need to remove the data
from the root data store. If the root data store is a Vec, removing the entry
will also change the index of one other data entry (when using swap_remove),
so you then need to go update all the derived data structures to reflect the
new index for the entry that moved.
Drop Guards
Drop guards provide a simple but reliable way to ensure that a bit of code
runs even in the presence of panics, which is often essential in unsafe code.
An example is a function that takes a closure f: FnOnce and executes it under
mutual exclusion using atomics. Say the function uses compare_exchange (discussed
in Chapter 10) to set a Boolean from false to true, calls f, and then
sets the Boolean back to false to end the mutual exclusion. But consider
what happens if f panics—the function will never get to run its cleanup, and
no other call will be able to enter the mutual exclusion section ever again.
It’s possible to work around this using catch_unwind, but drop guards
provide an alternative that is often more ergonomic. Listing 13-3 shows
The Rust Ecosystem 235
how, in our current example, we can use a drop guard to ensure the
Boolean always gets reset.
fn mutex(lock: &AtomicBool, f: impl FnOnce()) {
// .. while lock.compare_exchange(false, true).is_err() ..
struct DropGuard<'a>(&'a AtomicBool);
impl Drop for DropGuard<'_> {
fn drop(&mut self) {
lock.store(true, Ordering::Release);
}
}
let _guard = DropGuard(lock);
f();
}
Listing 13-3: Using a drop guard to ensure code gets run after an unwinding panic
We introduce the local type DropGuard that implements Drop and place
the cleanup code in its implementation of Drop::drop. Any necessary state
can be passed in through the fields of DropGuard. Then, we construct an
instance of the guard type just before we call the function that might
panic, which is f here. When f returns, whether due to a panic or because
it returns normally, the guard is dropped, its destructor runs, the lock is
released, and all is well.
It’s important that the guard is assigned to a variable that is dropped
at the end of the scope, after the user-provided code has been executed.
This means that even though we never refer to the guard’s variable again, it
needs to be given a name, as let _ = DropGuard(lock) would drop the guard
immediately—before the user-provided code even runs!
**NOTE** Like catch_unwind, drop guards work only when panics unwind. If the code is compiled
with panic=abort, no code gets to run after the panic.
This pattern is frequently used in conjunction with thread locals,
when library code may wish to set the thread local state so that it’s valid
only for the duration of the execution of the closure, and thus needs to
be cleared afterwards. For example, at the time of writing, Tokio uses this
pattern to provide information about the executor calling Future::poll to
leaf resources like TcpStream without having to propagate that information
through function signatures that are visible to users. It’d be no good if the
thread local state continued to indicate that a particular executor thread
was active even after Future::poll returned due to a panic, so Tokio uses a
drop guard to ensure that the thread local state is reset.
**NOTE** You’ll often see Cell or Rc<RefCell> used in thread locals. This is because thread
locals are accessible only through shared references, since a thread might access a
thread local again that it is already referencing somewhere higher up in the call stack.
Both types provide interior mutability without incurring much overhead because
they’re intended only for single-threaded use, and so are ideal for this use case.
236 Chapter 13
Extension Traits
Extension traits allow crates to provide additional functionality to types
that implement a trait from a different crate. For example, the itertools
crate provides an extension trait for Iterator, which adds a number of convenient
shortcuts for common (and not so common) iterator operations. As
another example, tower provides ServiceExt, which adds several more ergonomic
operations to wrap the low-level interface in the Service trait from
tower-service.
Extension traits tend to be useful either when you do not control the
base trait, as with Iterator, or when the base trait lives in a crate of its own
so that it rarely sees breaking releases and thus doesn’t cause unnecessary
ecosystem splits, as with Service.
An extension trait extends the base trait it is an extension of (trait
ServiceExt: Service) and consists solely of provided methods. It also comes
with a blanket implementation for any T that implements the base trait
(impl<T> ServiceExt for T where T: Service {}). Together, these conditions
ensure that the extension trait’s methods are available on anything that
implements the base trait.
Crate Preludes
In Chapter 12, we talked about the standard library prelude that makes a
number of types and traits automatically available without you having to
write any use statements. Along similar lines, crates that export multiple
types, traits, or functions that you’ll often use together sometimes define
their own prelude in the form of a module called prelude, which re-exports
some particularly common subset of those types, traits, and functions.
There’s nothing magical about that module name, and it doesn’t get used
automatically, but it serves as a signal to users that they likely want to add
use somecrate::prelude::_ to files that want to use the crate in question. The *
is a glob import and tells Rust to use all publicly available items from the indicated
module. This can save quite a bit of typing when the crate has a lot of
items you’ll usually need to name.
**NOTE** Items used through* have lower precedence than items that are used explicitly by
name. This is what allows you to define items in your own crate that overlap with
what’s in the standard library prelude without having to specify which one to use.
Preludes are also great for crates that expose a lot of extension traits,
since trait methods can be called only if the trait that defines them is in
scope. For example, the diesel crate, which provides ergonomic access to
relational databases, makes extensive use of extension traits so you can
write code like:
posts.filter(published.eq(true)).limit(5).load::<Post>(&connection)
This line will work only if all the right traits are in scope, which the prelude
takes care of.
The Rust Ecosystem 237
In general, you should be careful when adding glob imports to your
code, as they can potentially turn additions to the indicated module into
backward-incompatible changes. For example, if someone adds a new trait
to a module you glob-import from, and that new trait makes a method foo
available on a type that already had some other foo method, code that calls
foo on that type will no longer compile as the call to foo is now ambiguous.
Interestingly enough, while the existence of glob imports makes any module
addition a technically breaking change, the Rust RFC on API evolution
(RFC 1105; see <https://rust-lang.github.io/rfcs/1105-api-evolution.html>) does not
require a library to issue a new major version for such a change. The RFC
goes into great detail about why, and I recommend you read it, but the gist
is that minor releases are allowed to require minimally invasive changes to
dependents, like having to add type annotations in edge cases, because otherwise
a large fraction of changes would require new major versions despite
being very unlikely to actually break any consumers.
Specifically in the case of preludes, using glob imports is usually fine
when recommended by the vending crate, since its maintainers know that
their users will use glob imports for the prelude module and thus will take
that into account when deciding whether a change requires a major version
bump.
Staying Up to Date
Rust, being such a young language, is evolving rapidly. The language itself,
the standard library, the tooling, and the broader ecosystem are all still in
their infancy, and new developments happen every day. While staying on
top of all the changes would be infeasible, it’s worth your time to keep up
with significant developments so that you can take advantage of the latest
and greatest features in your projects.
For monitoring improvements to Rust itself, including new language
features, standard library additions, and core tooling upgrades, the official
Rust blog at <https://blog.rust-lang.org/> is a good, low-volume place to start. It
mainly features announcements for each new Rust release. I recommend
you make a habit of reading these, as they tend to include interesting tidbits
that will slowly but surely deepen your knowledge of the language. To
dig a little deeper, I highly recommend reading the detailed changelogs
for Rust and Cargo as well (links can usually be found near the bottom of
each release announcement). The changelogs surface changes that weren’t
large enough to warrant a paragraph in the release notes but that may be
just what you need two weeks from now. For a less frequently updated news
source, check in on The Edition Guide at <https://doc.rust-lang.org/edition-guide/>,
which outlines what’s new in each Rust edition. Rust editions tend to be
released every three years.
**NOTE** Clippy is often able to tell you when you can take advantage of a new language or
standard library feature—always enable Clippy!
238 Chapter 13
If you’re curious about how Rust itself is developed, you may also want
to subscribe to the Inside Rust blog at <https://blog.rust-lang.org/inside-rust/>. It
includes updates from the various Rust teams, as well as incident reports,
larger change proposals, edition planning information, and the like. To get
involved in Rust development yourself—which I highly encourage, as it’s
a lot of fun and a great learning experience—you can check out the various
Rust working groups at <https://www.rust-lang.org/governance/>, which each
focus on improving a specific aspect of Rust. Find one that appeals to you,
check in with the group wherever it meets and ask how you may be able to
help. You can also join the community discussion about Rust internals over
at <https://internals.rust-lang.org/>; this is another great way to get insight into
the thought that goes into every part of Rust’s design and development.
As is the case for most programming languages, much of Rust’s value
is derived from its community. Not only do the members of the Rust community
constantly develop new work-saving crates and discover new Rustspecific
techniques and design patterns, but they also collectively and
continuously help one another understand, document, and explain how
to take best advantage of the Rust language. Everything I have covered in
this book, and much more, has already been discussed by the community
in thousands of comment threads, blog posts, and Twitter and Discord conversations.
Dipping into these discussions even just once in a while is almost
guaranteed to show you new things about a language feature, a technique,
or a crate that you didn’t already know.
The Rust community lives in a lot of places, but some good places to
start are the Users forum (<https://users.rust-lang.org/>), the Rust subreddit
(<https://www.reddit.com/r/rust/>), the Rust Community Discord (<https://discord>
.gg/rust-lang-community), and the Rust Twitter account (<https://twitter.com/>
rustlang). You don’t have to engage with all of these, or all of the time—
pick one you like the vibe of, and check in occasionally!
A great single location for staying up to date with ongoing developments
is the This Week in Rust blog (<https://this-week-in-rust.org/>), a “weekly summary
of [Rust’s] progress and community.” It links to official announcements and
changelogs as well as popular community discussions and resources, interesting
new crates, opportunities for contributions, upcoming Rust events, and
Rust job opportunities. It even lists interesting language RFCs and compiler
PRs, so this site truly has it all! Discerning what information is valuable to
you and what isn’t may be a little daunting, but even just scrolling through
and clicking occasional links that appear interesting is a good way to keep a
steady stream of new Rust knowledge trickling into your brain.
**NOTE** Want to look up when a particular feature landed on stable? Can I Use…
(<https://caniuse.rs/>) has you covered.
What Next?
So, you’ve read this book front to back, absorbed all the knowledge it
imparts, and are still hungry for more? Great! There are a number of other
The Rust Ecosystem 239
excellent resources out there for broadening and deepening your knowledge and understanding of Rust, and in this very final section I’ll give you a survey of some of my favorites so that you can keep learning. I’ve divided them into subsections based on how different people prefer to learn so that you can find resources that’ll work for you.
**NOTE** A challenge with learning on your own, especially in the beginning, is that progress
is hard to perceive. Implementing even the simplest of things can take an outsized
amount of time when you have to constantly refer to documentation and other
resources, ask for help, or debug to learn how some aspect of Rust works. All of that
non-coding work can make it seem like you’re treading water and not really improving. But you’re learning, which is progress in and of itself—it’s just harder to notice
and appreciate.
Learn by Watching
Watching experienced developers code is essentially a life hack to remedy
the slow starting phase of solo learning. It allows you to observe the process of designing and building while utilizing someone else’s experience.
Listening to experienced developers articulate their thinking and explain
tricky concepts or techniques as they come up can be an excellent alternative to struggling through problems on your own. You’ll also pick up a
variety of auxiliary knowledge like debugging techniques, design patterns,
and best practices. Eventually you will have to sit down and do things yourself—it’s the only way to check that you actually understand what you’ve
observed—but piggybacking on the experience of others will almost certainly make the early stages more pleasant. And if the experience is interactive, that’s even better!
So, with that said, here are some Rust video channels that I recommend:
Perhaps unsurprisingly, my own channel: <https://www.youtube.com/c/>
JonGjengset/. I have a mix of long-form coding videos and short(er) code-
based theory/concept explanation videos, as well as occasional videos
that dive into interesting Rust coding stories.
The Awesome Rust Streaming listing: <https://github.com/jamesmunns/awesome-rust-streaming/>. This resource lists a wide variety of developers
who stream Rust coding or other Rust content.
The channel of Tim McNamara, the author of Rust in Action: https://
<www.youtube.com/c/timClicks/>. Tim’s channel, like mine, splits its time
between implementation and theory, though Tim has a particular
knack for creative visual projects, which makes for fun viewing.
Jonathan Turner’s Systems with JT channel: <https://www.youtube.com/c/>
SystemswithJT/. Jonathan’s videos document their work on Nushell, their take on a “new type of shell,” providing a great sense of what it’s
like to work on a nontrivial existing codebase.
Ryan Levick’s channel: <https://www.youtube.com/c/RyanLevicksVideos/>. Ryan mainly posts videos that tackle particular Rust concepts and walks
240 Chapter 13
through them using concrete code examples, but he also occasionally
does implementation videos (like FFI for Microsoft Flight Simulator!)
and deep dives into how well-known crates work under the hood.
Given that I make Rust videos, it should come as no surprise that I am
a fan of this approach to teaching. But this kind of receptive or interactive
learning doesn’t have to come in the form of videos. Another great avenue
for learning from experienced developers is pair programming. If you have
a colleague or friend with expertise in a particular aspect of Rust you’d like
to learn, ask if you can do a pair-programming session with them to solve a
problem together!
Learn by Doing
Since your ultimate goal is to get better at writing Rust, there’s no substitute
for programming experience. No matter what or how many resources you
learn from, you need to put that learning into practice. However, finding a
good place to start can be tricky, so here I’ll give some suggestions.
Before I dive into the list, I want to provide some general guidance on
how to pick projects. First, choose a project that you care about, without
worrying too much whether others care about it. While there are plenty
of popular and established Rust projects out there that would love to have
you as a contributor, and it’s fun to be able to say “I contributed to the wellknown
library X,” your first priority must be your own interest. Without
concrete motivation, you’ll quickly lose steam and find contributing to be
a chore. The very best targets are projects that you use yourself and have
experienced problems with—go fix them! Nothing is more satisfying than
getting rid of a long-standing personal nuisance while also contributing
back to the community.
Okay, so back to project suggestions. First and foremost, consider contributing
to the Rust compiler and its associated tools. It’s a high-quality
codebase with good documentation and an endless supply of issues (you
probably know of some yourself), and there are several great mentors who
can provide outlines for how to approach solving issues. If you look through
the issue tracker for issues marked E-easy or E-mentor, you’ll likely find a
good candidate quickly. As you gain more experience, you can keep leveling
up to contribute to trickier parts.
If that’s not your cup of tea, I recommend finding something you use
frequently that’s written in another language and porting it to Rust—not
necessarily with the intention of replacing the original library or tool, but
just because the experience will allow you to focus on writing Rust without
having to spend too much time coming up with all the functionality yourself.
If it turns out well, the fact that it already exists suggests that someone
else also needed it, so there may be a wider audience for your port too! Data
structures and command-line tools often make for great porting subjects,
but find a niche that appeals to you.
Should you be more of a “build it from scratch” kind of person, I recommend
looking back at your own development experience so far and thinking
about similar code you’ve ended up writing in multiple projects (whether
The Rust Ecosystem 241
in Rust or in other languages). Such repetition tends to be a good signal
that something is reusable and could be turned into a library. If nothing
comes to mind, David Tolnay maintains a list of smaller utility crates that
other Rust developers have requested at <https://github.com/dtolnay/request-forimplementation/>
that may provide a source of inspiration. If you’re looking for
something more substantial and ambitious, there’s also the Not Yet Awesome
list at <https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/> that lists
things that should exist in Rust but don’t (yet).
Learn by Reading
Although the state of affairs is constantly improving, finding good Rust
reading material beyond the beginner level can still be tricky. Here’s a collection
of pointers to some of my favorite resources that continue to teach
me new things or serve as good references when I have particularly niche or
nuanced questions.
First, I recommend looking through the official virtual Rust books
linked from <https://www.rust-lang.org/learn/>. Some, like the Cargo book, are
more reference-like while others, like the Embedded book, are more guidelike,
but they’re all deep sources of solid technical information about their
respective topics. The Rustonomicon (<https://doc.rust-lang.org/nomicon/>), in particular,
is a lifesaver when you’re writing unsafe code.
Two more books that are worth checking out are the Guide to rustc
Development (<https://rustc-dev-guide.rust-lang.org/>) and the Standard Library
Developers Guide (<https://std-dev-guide.rust-lang.org/>). These are fantastic
resources if you’re curious about how the Rust compiler does what it does
or how the standard library is designed, or if you want some pointers before
you try your hand at contributing to Rust itself. The official Rust guidelines
are also a treasure trove of information; I’ve already mentioned the Rust
API Guidelines (<https://rust-lang.github.io/api-guidelines/>) in the book, but a
Rust Unsafe Code Guidelines Reference is also available (<https://rust-lang.github>
.io/unsafe-code-guidelines/), and by the time you read this book there may
be more.
**NOTE** One of the resources listed at <https://www.rust-lang.org/learn/> is the Rust
Reference, which is essentially a full specification for the Rust language. While parts
of it are quite dry, like the exact grammar used for parsing or basics about the inmemory
representations of the primitive types, some of it is fascinating reading, like
the section on type layout and the enumeration of behavior considered undefined.
There are also a number of unofficial virtual Rust books that are
enormously valuable collections of experience and knowledge. The Little
Book of Rust Macros (<https://veykril.github.io/tlborm/>), for example, is indispensable
if you want to write nontrivial declarative macros, and The Rust
Performance Book (<https://nnethercote.github.io/perf-book/>) is filled with tips and
tricks for improving the performance of Rust code both at the micro and
the macro level. Other great resources include the Rust Fuzz Book (https://
rust-fuzz.github.io/book/), which explores fuzz testing in more detail, and
242 Chapter 13
the Rust Cookbook (<https://rust-lang-nursery.github.io/rust-cookbook/>), which suggests
idiomatic solutions to common programming tasks. There’s even a
resource for finding more books, The Little Book of Rust Books (<https://lborb>.
github.io/book/unofficial.html)!
If you prefer more hands-on reading, the Tokio project has published
mini-redis (<https://github.com/tokio-rs/mini-redis/>), an incomplete but idiomatic
implementation of a Redis client and server that’s extremely well documented
and specifically written to serve as a guide to writing asynchronous
code. If you’re more of a data structures person, Learn Rust with Entirely Too
Many Linked Lists (<https://rust-unofficial.github.io/too-many-lists/>) is an enlightening
and fun read that gets into lots of gnarly details about ownership and
references. If you’re looking for something closer to the hardware, Philipp
Oppermann’s Writing an OS in Rust (<https://os.phil-opp.com/>) goes through
the whole operating system stack in great detail while teaching you good
Rust patterns in the process. I also highly recommend Amos’s collection of
articles (<https://fasterthanli.me/tags/rust/>) if you want a wide sampling of interesting
deep dives written in a conversational style.
When you feel more confident in your Rust abilities and need more of
a quick reference than a long tutorial, I’ve found the Rust Language Cheat
Sheet (<https://cheats.rs/>) great for looking things up quickly. It also provides
very nice visual explanations for most topics, so even if you’re looking up
something you’re not intimately familiar with already, the explanations are
pretty approachable.
And finally, if you want to put all of your Rust understanding to the
test, go give David Tolnay’s Rust Quiz (<https://dtolnay.github.io/rust-quiz/>) a try.
There are some real mind-benders in there, but each question comes with
a thorough explanation of what’s going on, so even if you get one wrong,
you’ll have learned from the experience!
Learn by Teaching
My experience has been that the best way to learn something well and
thoroughly, by far, is to try to teach it to others. I have learned an enormous
amount from writing this book, and I learn new things every time
I make a new Rust video or podcast episode. So, I wholeheartedly recommend
that you try your hand at teaching others about some of the things
you’ve learned from reading this book or that you learn from here on out.
It can take whatever form you prefer: in person, writing a blog post, tweeting,
making a video or podcast, or giving a talk. The important thing is
that you try to convey your newfound knowledge in your own words to
someone who doesn’t already understand the topic—in doing so, you also
give back to the community so that the next you that comes along has a
slightly easier time getting up to speed. Teaching is a humbling and deeply
educational experience, and I cannot recommend it highly enough.
**NOTE** Whether you’re looking to teach or be taught, make sure to visit Awesome Rust
Mentors (<https://rustbeginners.github.io/awesome-rust-mentors/>).
The Rust Ecosystem 243
Summary
In this chapter, we’ve covered Rust beyond what exists in your local workspace.
We surveyed useful tools, libraries, and Rust features; looked at how
to stay up to date as the ecosystem continues to evolve; and then discussed
how you can get your hands dirty and contribute back to the ecosystem
yourself. Finally, we discussed where you can go next to continue your Rust
journey now that this book has reached its end. And with that, there’s little
more to do than to declare:
}
