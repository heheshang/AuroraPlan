# Rust for Rustaceans

## Rust

### 致谢

- 由于以前从未写过书，我对这个过程的期望很少。我天真地以为它会像写一系列博客文章，或者像写广泛的文档一样，但写一本书是完全不同的事情。我花费了无数个小时进行研究、规划、写作、重写、丢弃、反思和编辑。我非常感激我的女朋友Talia在我为这个项目工作的许多个深夜中所表现出的支持和耐心——没有你，写作体验会变得更加黯淡。这本书绝不可能没有令人难以置信的Rust开发者和社区的支持，他们开发了一门语言和生态系统，我继续发现与之互动是一种乐趣，并受到启发，以我最好的能力来传播和教授。同样感谢多年来观看我的直播的了不起的人们；如果没有你们持续的支持、鼓励和无尽的好奇心，我想我永远不会有机会首先写这本书。

- 如果没有David Tolnay作为本书的技术审查者，这本书也不会有一半的好处。David在找出理论和代码错误方面显然是无价的，但他的丰富经验、注重细节和爱好教学真正留下了深刻的印象。他的深思熟虑和富有洞察力的评论有时让我决定重写整个章节，但总是以比之前更好得多的方式进行。
- 同样感谢我的编辑Liz Chadwick和No Starch出版团队的其他成员。看到Liz在书的编写过程中的成长过程非常有趣；她在阅读过程中学习了Rust，并且当她的评论表明她真正理解了中级内容时，我感到非常高兴。每当她遇到不理解的地方时的讨论总是启发人，导致了更易理解和全面的解释。
- 我也不能不提到Steve Klabnik和Carol Nichols，他们是《Rust编程语言》的作者，这是我对Rust的第一次介绍。在我看来，这本书在很大程度上是他们书的续集，如果没有他们对Rust基础知识的出色解释和易于理解的工作，这本书是无法存在的。
最后，我要向你们致敬。是的，就是你们！写这本书是一个非常漫长的过程，而正是那些希望阅读它的人们的支持和鼓励使我一直坚持下去。正是像你们这样的人，无论是虚拟还是实体地阅读这本书，都希望提高自己的理解和技能，驱使我尽我所能为Rust教学资源的积累做出贡献。
谢谢大家！

### 介绍

- **无论使用哪种编程语言，入门教材所教授的知识与多年实践经验后所掌握的知识之间的差距总是很大。随着时间的推移，你会熟悉惯用法，对核心概念有更好的心智模型，了解哪些设计和模式有效，哪些无效，并发现周围生态系统中有用的库和工具。综合起来，这种经验使你能够在更短的时间内编写更好的代码**

- 通过这本书，我希望将我多年编写Rust代码的经验浓缩成一本易于理解的资源。《Rust for Rustaceans》是《Rust编程语言》（即Rust书）的延续，但适用于任何想要超越基础知识的Rust程序员，无论你是在哪里学习这门语言。本书深入探讨了诸如不安全代码、特质系统、no_std代码和宏等概念。它还涵盖了新领域，如异步I/O、测试、嵌入式开发和人性化的API设计。我旨在解释和揭示Rust的这些更高级和强大的特性，并使您能够构建更快、更人性化和更健壮的应用程序。

### What’s in the Book

- 本书既是指南，也是参考资料。各章节相对独立，因此您可以直接跳转到特别感兴趣的主题（或者当前困扰您的主题），或者从头到尾阅读本书以获得更全面的体验。尽管如此，我建议您从阅读第1章和第2章开始，因为它们为后续章节和日常Rust开发中的许多主题奠定了基础。以下是每个章节的简要介绍：
  - 第1章《基础知识》深入描述了Rust的基本概念，如变量、内存、所有权、借用和生命周期，您需要熟悉这些概念才能理解本书的其余部分。
  - 第2章《类型》类似地提供了关于Rust中类型和特质的更详尽解释，包括编译器如何推理它们、它们的特性和限制，以及一些高级应用。
  - 第3章《设计接口》介绍了如何设计直观、灵活和防止误用的API，包括如何命名事物、如何使用类型系统来强制执行API合约，以及何时使用泛型和特质对象。
  - 第4章《错误处理》探讨了两种主要类型的错误（枚举和不透明错误），以及何时使用每种类型的适当情况，以及如何定义、构造、传播和处理这些错误。
  - 第5章《项目结构》关注Rust项目的非代码部分，例如Cargo元数据和配置、crate特性和版本控制。
  - 第6章《测试》详细介绍了标准的Rust测试框架的工作原理，并介绍了一些超出标准单元测试和集成测试的测试工具和技术，例如模糊测试和性能测试。
  - 第7章《宏》涵盖了声明式和过程式宏，包括它们的编写方式、它们的用途以及一些注意事项。
  - 第8章《异步编程》介绍了同步和异步接口之间的区别，然后深入探讨了在Rust中如何表示异步性，包括Future和Pin的底层表示以及async和await的高级表示。该章还解释了异步执行器的作用以及它如何使整个异步机制协同工作。
  - 第9章《不安全代码》解释了unsafe关键字所解锁的强大功能以及随之而来的巨大责任。您将了解到在不安全代码中常见的陷阱，以及可以用来减少不正确不安全代码风险的工具和技术。
  - 第10章《并发（和并行）》介绍了Rust中如何表示并发以及为什么在正确性和性能方面很难做到正确。它涵盖了并发和异步性的关系（但并不相同），以及在接近硬件时并发如何工作，以及在尝试编写正确并发程序时如何保持理智。
  - 第11章《外部函数接口》教你如何使Rust与其他语言良好地协作，以及extern关键字等FFI原语的实际作用。
  - 第12章《无标准库的Rust》介绍了在无法使用完整标准库的情况下使用Rust的情况，例如在嵌入式设备或其他受限平台上，您只能使用核心和alloc模块提供的功能。
  - 第13章《Rust生态系统》不涵盖特定的Rust主题，而是旨在提供关于在Rust生态系统中工作的更广泛指导。它包含常见设计模式的描述，关于保持对语言添加和最佳实践的最新信息的建议，有用工具的提示以及我多年来积累的其他有用的琐事，这些信息在任何单一地方都没有描述。
该书有一个网站，网址为<https://rust-for-rustaceans.com，其中包含书中的资源链接、未来的勘误等。您还可以在No> Starch Press网站上的该书页面<https://nostarch.com/rust-rustaceans/找到这些信息。>
现在，所有这些都已经解释清楚了，只剩下一件事要做：

    ```rust
    fn main() {}
    ```

### 1 基础知识

- **当你深入研究Rust的更高级领域时，确保你对基础知识有扎实的理解非常重要。在Rust中，就像在任何编程语言中一样，各种关键字和概念的确切含义在你开始以更复杂的方式使用语言时变得重要**。在本章中，我们将详细介绍Rust的许多基本概念，试图更清楚地定义它们的含义、工作原理以及它们为什么恰好是这样。具体而言，我们将了解变量和值的区别，它们如何在内存中表示，以及程序所具有的不同内存区域。然后，我们将讨论一些关于所有权、借用和生命周期的细微差别，这些是您在继续阅读本书之前需要掌握的。
- 如果你愿意，你可以从头到尾阅读本章，或者你可以将其作为参考，复习你对其中一些概念不太确定的内容。我建议你只有在对本章内容感到完全舒适时再继续前进，因为对这些基本概念的误解会很快妨碍你理解更高级的主题，或者导致你错误地使用它们。

#### Talking About Memory

并非所有的内存都是相同的。在大多数编程环境中，你的程序可以访问堆栈、堆、寄存器、文本段、内存映射寄存器、内存映射文件，以及可能的非易失性RAM。你在特定情况下选择使用哪种内存区域会对你可以在其中存储什么、它能保持多长时间可访问以及你使用什么机制来访问它产生影响。这些内存区域的具体细节因平台而异，超出了本书的范围，但其中一些对于你如何推理Rust代码非常重要，因此值得关注。
covering here.

##### Memory Terminology

在我们深入研究内存区域之前，您首先需要了解值、变量和指针之间的区别。在Rust中，值是类型和该类型值域中的元素的组合。可以使用值的类型表示将值转换为字节序列，但单独来看，您可以将值视为您作为程序员所指的内容。例如，类型为u8的数字6是数学整数6的一个实例，其内存表示为字节0x06。类似地，字符串"Hello world"是所有字符串域中的一个值，其表示为其UTF-8编码。值的含义与存储这些字节的位置无关。
值存储在一个位置中，这是Rust术语中“可以容纳值的位置”。这个位置可以在堆栈、堆或其他位置上。存储值的最常见位置是变量，它是堆栈上的一个命名值槽。
指针是一个保存内存区域地址的值，因此指针指向一个位置。可以解引用指针以访问存储在其指向的内存位置中的值。我们可以将相同的指针存储在多个变量中，因此可以有多个间接引用相同内存位置和相同基础值的变量。
请考虑代码清单1-1中的代码，它说明了这三个元素。

```rust
let x = 42;
let y = 43;
let var1 = &x;
let mut var2 = &x;
1 var2 = &y;
```

Listing 1-1: Values, variables, and pointers

在这里，有四个不同的值：42（一个i32），43（一个i32），x的地址（一个指针）和y的地址（一个指针）。还有四个变量：x，y，var1和var2。后两个变量都保存指针类型的值，因为引用就是指针。虽然var1和var2最初存储相同的值，但它们存储了独立的副本；当我们改变var2中存储的值时，var1中的值不会改变。特别是，=运算符将右侧表达式的值存储在左侧命名的位置中。
在变量、值和指针之间的区别变得重要的一个有趣例子是这样一个语句：

```rust
let string = "Hello world";
```

即使我们将字符串值赋给变量string，实际上变量的值是指向字符串值"Hello world"中第一个字符的指针，而不是字符串值本身。此时你可能会说：“但是等等，那么字符串值存储在哪里？指针指向哪里？”如果是这样，你的观察力很敏锐，我们马上就会讨论这个问题。

**注意** 从技术上讲，变量string的值还包括字符串的长度。我们将在第2章讨论宽指针类型时详细介绍这一点。

##### 变量的深入理解

在高级模型中，我们不将变量视为保存字节的位置。相反，我们将它们仅视为在程序中实例化、移动和使用时赋予值的名称。当将一个值赋给一个变量时，该值从那时起就由该变量命名。当稍后访问一个变量时，你可以想象从该变量的先前访问到新访问之间画一条线，从而建立两个访问之间的依赖关系。如果变量中的值被移动，就不能再从它画线了。

- 在这个模型中，变量只存在于它持有合法值的时间内；你不能从一个值未初始化或已被移动的变量画线，所以它实际上不存在。使用这个模型，你的整个程序由许多这些依赖线组成，通常被称为流，每个流追踪一个特定值的生命周期。当存在分支时，流可以分叉和合并，每个分叉追踪该值的不同生命周期。编译器可以检查在程序的任何给定点，所有可以并行存在的流都是兼容的。例如，不能有两个并行流同时对一个值进行可变访问。也不能在没有拥有该值的流的情况下借用一个值。清单1-2展示了这两种情况的示例。

##### 高级模型

在高级模型中，我们不将变量视为保存字节的位置。相反，我们将它们仅视为在程序中实例化、移动和使用时赋予值的名称。当将一个值赋给一个变量时，该值从那时起就由该变量命名。当稍后访问一个变量时，你可以想象从该变量的先前访问到新访问之间画一条线，从而建立两个访问之间的依赖关系。如果变量中的值被移动，就不能再从它画线了。

- 在这个模型中，变量只存在于它持有合法值的时间内；你不能从一个值未初始化或已被移动的变量画线，所以它实际上不存在。使用这个模型，你的整个程序由许多这些依赖线组成，通常被称为流，每个流追踪一个特定值的生命周期。当存在分支时，流可以分叉和合并，每个分叉追踪该值的不同生命周期。编译器可以检查在程序的任何给定点，所有可以并行存在的流都是兼容的。例如，不能有两个并行流同时对一个值进行可变访问。也不能在没有拥有该值的流的情况下借用一个值。清单1-2展示了这两种情况的示例。

```rust
let mut x;
// this access would be illegal, nowhere to draw the flow from:
// assert_eq!(x, 42);
1 x = 42;
// this is okay, can draw a flow from the value assigned above:
2 let y = &x;
// this establishes a second, mutable flow from x:
3 x = 43;
// this continues the flow from y, which in turn draws from x.
// but that flow conflicts

```

清单1-2：借用检查器将捕获的非法流

- 首先，我们不能在初始化之前使用x，因为我们没有地方可以从中绘制流。只有当我们给x赋值时，我们才能从中绘制流。这段代码有两个流：一个是从1到3的独占(&mut)流，另一个是从1经过2到4的共享(&)流。借用检查器检查每个流的每个顶点，并检查是否存在其他不兼容的流同时存在。在这种情况下，当借用检查器检查3处的独占流时，它看到终止于4的共享流。由于不能同时对一个值进行独占和共享使用，借用检查器（正确地）拒绝了这段代码。请注意，如果4不存在，这段代码将编译成功！共享流将在2处终止，当检查3处的独占流时，不会存在冲突的流。
- 如果使用与先前变量相同的名称声明一个新变量，它们仍然被视为不同的变量。这被称为遮蔽，后面的变量通过相同的名称“遮蔽”了前面的变量。这两个变量共存，尽管后续的代码不再有办法命名前面的变量。这个模型大致符合编译器的工作方式，特别是借用检查器，实际上在编译器内部用于生成高效的代码。

##### 低级模型

变量是可能或可能不持有合法值的内存位置。您可以将变量视为“值槽”。当您对其赋值时，槽被填充，其旧值（如果有）被丢弃并替换。当您访问它时，编译器会检查槽是否为空，因为这意味着变量未初始化或其值已被移动。指向变量的指针指向变量的后备内存，并可以通过解引用来访问其值。例如，在语句let x: usize中，变量x是堆栈上的一个内存区域的名称，该区域可以容纳一个usize大小的值，尽管它没有定义明确的值（其槽为空）。如果您将一个值赋给该变量，例如x = 6，那么该内存区域将保存表示值6的位。当您对x进行赋值时，&x不会改变。如果您使用相同的名称声明多个变量，它们最终将具有不同的内存块支持。这个模型与C和C++以及许多其他低级语言使用的内存模型相匹配，并且在需要明确处理内存时非常有用。

##### 栈

栈是程序用作函数调用的临时空间的一段内存。每次调用函数时，栈的顶部都会分配一个连续的内存块，称为帧。在栈的底部附近是主函数的帧，当函数调用其他函数时，额外的帧会被推入栈中。一个函数的帧包含该函数内的所有变量，以及函数接受的任何参数。当函数返回时，它的栈帧会被回收。

- 构成函数局部变量值的字节不会立即被清除，但是不安全访问它们是不安全的，因为它们可能已经被后续的函数调用覆盖，这些函数的帧与被回收的帧重叠。即使它们没有被覆盖，它们可能包含不合法使用的值，例如在函数返回时移动的值。
- 栈帧以及它们最终消失的事实与Rust中的生命周期概念密切相关。存储在栈帧上的任何变量在该帧消失后无法访问，因此对它的任何引用的生命周期最长不能超过帧的生命周期。

##### 堆

堆是一个与程序当前调用栈无关的内存池。堆内存中的值会一直存在，直到显式释放。当您希望一个值的生命周期超过当前函数帧的生命周期时，这非常有用。如果该值是函数的返回值，调用函数可以在其堆栈上留出一些空间，以便被调用函数在返回之前将该值写入其中。但是，如果您想将该值发送到与当前线程可能完全不共享堆栈帧的其他线程，您可以将其存储在堆上。

- 堆允许您显式分配连续的内存段。当您这样做时，您会得到指向该内存段开头的指针。该内存段为您保留，直到您稍后将其释放；这个过程通常被称为释放，以C标准库中相应函数的名称命名。由于从堆分配的内存在函数返回时不会消失，因此您可以在一个地方为一个值分配内存，将指针传递给另一个线程，并且该线程可以安全地继续操作该值。或者，换句话说，当您在堆上分配内存时，得到的指针具有无约束的生命周期 - 只要您的程序保持其存活，它就可以使用。
- 在Rust中与堆交互的主要机制是Box类型。当您编写Box::new(value)时，该值被放置在堆上，而您得到的`（Box<T>）`是指向堆上该值的指针。当Box最终被丢弃时，该内存将被释放。
- 如果您忘记释放堆内存，它将永远存在，并且您的应用程序最终会占用机器上的所有内存。这被称为内存泄漏，通常是您要避免的情况。但是，有些情况下您明确希望泄漏内存。例如，假设您有一个只读配置，整个程序都应该能够访问。您可以在堆上分配它，并使用Box::leak显式泄漏它，以获得对它的'static引用。

##### 静态内存

静态内存实际上是一个统称，用于描述程序编译后所在文件中的几个相关区域。当程序执行时，这些区域会自动加载到程序的内存中。静态内存中的值在整个程序执行期间都存在。程序的静态内存包含程序的二进制代码，通常被映射为只读。当程序执行时，它会按照指令逐步遍历文本段中的二进制代码，并在调用函数时跳转。静态内存还保存了使用static关键字声明的变量的内存，以及代码中的某些常量值，比如字符串。

- 特殊的生命周期'static得名于静态内存区域，它将引用标记为“在静态内存存在的时间内有效”，即直到程序关闭。由于静态变量的内存在程序启动时分配，对静态内存中变量的引用在定义上是'static的，因为它在程序关闭之前不会被释放。反之则不成立——可能存在指向静态内存之外的'static引用，但这个名称仍然适用：一旦创建了具有静态生命周期的引用，对于程序的其余部分来说，它指向的内容就好像在静态内存中一样，可以在程序希望的任何时间内使用。

- 在使用Rust时，您会经常遇到'static生命周期，而不是真正的静态内存（例如通过static关键字）。这是因为在处理Rust时，'static经常出现在类型参数的trait约束中。类似于T: 'static这样的约束表示类型参数T能够在我们保留它的时间内存活，直到程序的剩余执行结束。实质上，这个约束要求T是拥有所有权且自给自足的，要么它不借用其他（非静态）值，要么它借用的任何值也是'static的，因此会一直存在直到程序结束。一个很好的'static约束的例子是std::thread::spawn函数，它创建一个新线程，要求传递给它的闭包是'static的。由于新线程可能比当前线程存在更长的时间，新线程不能引用旧线程堆栈上的任何内容。新线程只能引用在其整个生命周期内都存在的值，这可能是程序剩余时间的持续时间。

**注意** 你可能想知道const与static有什么区别。const关键字声明了以下项为常量。常量项可以在编译时完全计算，并且在编译期间，任何引用它们的代码都会被替换为常量的计算值。常量没有与之关联的内存或其他存储（它不是一个位置）。你可以将常量视为特定值的方便名称。

##### 所有权

Rust的内存模型以所有值都有一个单一所有者的概念为中心，也就是说，每个值都有一个负责最终释放它的位置（通常是一个作用域）。这是通过借用检查器来实现的。如果值被移动，例如通过将其赋值给一个新变量、将其推入向量或将其放置在堆上，那么该值的所有权将从旧位置移动到新位置。在那时，您不能再通过从原始所有者流动的变量访问该值，即使构成该值的位实际上仍然存在。相反，您必须通过引用其新位置的变量来访问移动的值。

- 有些类型是特例，不遵循这个规则。如果一个值的类型实现了特殊的Copy trait，即使它被重新分配到一个新的内存位置，该值也不被认为已经移动。相反，该值被复制，旧的和新的位置仍然可访问。实质上，在移动的目标位置构造了另一个相同的值的实例。Rust中的大多数原始类型，如整数和浮点类型，都是Copy的。要成为Copy，必须能够通过复制其位来复制类型的值。这排除了所有包含非Copy类型的类型，以及在值被丢弃时必须释放资源的任何类型。
- 为了看清楚，考虑一下如果Box这样的类型是Copy会发生什么。如果我们执行box2 = box1，那么box1和box2都会认为它们拥有为box分配的堆内存，并且它们都会在作用域结束时尝试释放它。释放内存两次可能会产生灾难性的后果。
当一个值的所有者不再使用它时，所有者有责任通过丢弃它来进行任何必要的清理。在Rust中，当保存值的变量不再在作用域内时，丢弃会自动发生。类型通常会递归地丢弃它们包含的值，因此丢弃一个复杂类型的变量可能导致许多值被丢弃。
- 由于Rust的离散所有权要求，我们不能意外地多次丢弃相同的值。持有对另一个值的引用的变量不拥有该值，因此当变量丢弃时，该值不会被丢弃。
清单1-3中的代码简要总结了所有权、移动和复制语义以及丢弃的规则。

```rust
let x1 = 42;
let y1 = Box::new(84);
{ // starts a new scope
1 let z = (x1, y1);
// z goes out of scope, and is dropped;
// it in turn drops the values from x1 and y1
2 }
// x1's value is Copy, so it was not moved into z
3 let x2 = x1;
// y1's value is not Copy, so it was moved into z
4 // let y2 = y1;
```

清单1-3：移动和复制语义
我们从两个值开始，一个是数字42，另一个是包含数字84的Box（一个堆分配的值）。前者是`Copy`，而后者不是。当我们将x1和y1放入元组z中时 1，x1被复制到z中，而y1被移动到z中。此时，x1仍然可以访问并可以再次使用 3。另一方面，一旦y1的值被移动 4，它就变得不可访问，任何尝试访问它的操作都会导致编译器错误。当z超出作用域时 2，它包含的元组值被丢弃，这同时也丢弃了从x1复制的值和从y1移动的值。当来自y1的`Box`被丢弃时，它也会释放用于存储y1值的堆内存。

##### 丢弃顺序

Rust会自动在值超出作用域时丢弃它们，比如在Listing 1-3的内部作用域中的x1和y1。丢弃的顺序规则相当简单：变量（包括函数参数）按照反向顺序丢弃，嵌套值按照源代码顺序丢弃。

- 这可能一开始听起来很奇怪——为什么会有这种差异？但如果我们仔细看，这其实很有道理。假设你写了一个函数，声明了一个字符串，然后将该字符串的引用插入到一个新的哈希表中。当函数返回时，必须先丢弃哈希表；如果先丢弃字符串，那么哈希表就会持有一个无效的引用！一般来说，后面的变量可能包含对前面值的引用，而反过来则不可能，这是由于Rust的生命周期规则。因此，Rust按照反向顺序丢弃变量。
- 现在，我们可以对嵌套值有相同的行为，比如元组、数组或结构体中的值，但这可能会让用户感到惊讶。如果你构造了一个包含两个值的数组，如果数组的最后一个元素先被丢弃，那就会显得很奇怪。同样的情况也适用于元组和结构体，最直观的行为是先丢弃第一个元组元素或字段，然后是第二个，依此类推。对于变量来说，没有必要在这种情况下反转丢弃顺序，因为Rust不允许在单个值中自引用。所以，Rust选择了直观的选项。

#### 借用和生命周期

Rust允许值的所有者通过引用将该值借给其他人，而不放弃所有权。引用是带有额外使用约定的指针，例如引用是否提供对引用值的独占访问，或者引用值是否还可以有其他引用指向它。

##### 共享引用

- 共享引用 &T 是一个可以共享的指针。可以存在任意数量的其他引用指向相同的值，并且每个共享引用都是可复制的，因此您可以轻松地创建更多的引用。
- 共享引用指向的值是不可变的；您不能修改或重新分配共享引用指向的值，也不能将共享引用转换为可变引用。
- Rust编译器可以假设共享引用指向的值在引用存在期间不会发生变化。例如，如果Rust编译器看到共享引用后面的值在函数中被多次读取，它有权只读取一次并重用该值。更具体地说，清单1-4中的断言应该永远不会失败。

```rust
fn cache(input: &i32, sum: &mut i32) {
  *sum = *input + *input;
  assert_eq!(*sum, 2 * *input);
}

```

清单1-4：Rust假设共享引用是不可变的。

编译器是否选择应用给定的优化几乎是无关紧要的。编译器的启发式算法会随时间而变化，因此通常希望根据编译器允许的操作来编写代码，而不是根据编译器在特定情况下的实际操作。

##### 可变引用

与共享引用相对应的是可变引用：&mut T。对于可变引用，Rust编译器再次可以充分利用引用所带来的约定：编译器假设没有其他线程通过共享引用或可变引用访问目标值。换句话说，它假设可变引用是独占的。这使得一些在其他语言中不容易实现的优化成为可能。例如，看一下清单1-5中的代码。

```rust
fn noalias(input: &i32, output: &mut i32) {
  if *input == 1 {
1   *output = 2;
  }
2 if *input != 1 {
    *output = 3;
  }
}
```

清单1-5：Rust假设可变引用是独占的。

- 在Rust中，编译器可以假设input和output不指向相同的内存。因此，在1处对output的重新赋值不会影响到2处的检查，整个函数可以编译为一个单独的if-else块。如果编译器不能依赖于独占可变性的约定，那么这个优化将是无效的，因为在类似`noalias(&x, &mut x)`的情况下，输入为1可能导致输出为3。
- 可变引用只允许您修改引用指向的内存位置。您是否可以修改超出直接引用的值取决于位于之间的类型提供的方法。通过一个例子可能更容易理解，所以请参考清单1-6。

```rust
let x = 42;
let mut y = &x; // y is of type &i32
let z = &mut y; // z is of type &mut &i32
```

清单1-6：可变性仅适用于直接引用的内存。

- 在这个例子中，你可以通过将指针y引用到不同的变量来改变指针的值（即不同的指针），但是你不能改变指针所指向的值（即x的值）。同样地，你可以通过z改变y的指针值，但是你不能改变z本身来持有不同的引用。
拥有一个值和拥有对它的可变引用之间的主要区别在于，当不再需要该值时，所有者负责丢弃该值。除此之外，通过可变引用，你可以做任何你拥有该值时可以做的事情，但有一个例外：如果你移动了可变引用后面的值，那么你必须留下另一个值来代替它。如果没有这样做，所有者仍然会认为它需要丢弃该值，但是没有值可供丢弃！

清单1-7展示了通过可变引用移动值的方式的示例。

```rust
fn replace_with_84(s: &mut Box<i32>) {
// this is not okay, as *s would be empty:
1 // let was = *s;
// but this is:
2 let was = std::mem::take(s);
// so is this:
3 *s = was;
// we can exchange values behind &mut:
let mut r = Box::new(84);
4 std::mem::swap(s, &mut r);
  assert_ne!(*r, 84);
}

let mut s = Box::new(42);
replace_with_84(&mut s);
5
```

清单1-7：通过可变引用访问必须留下一个值。

- 我添加了注释的行，表示非法操作。您不能简单地将值移出 1，因为调用者仍然认为它们拥有该值，并且会在5处再次释放它，导致双重释放。如果您只想留下一些有效的值，`std::mem::take 2`是一个很好的选择。它等同于`std::mem::replace(&mut value, Default::default())`；它将值从可变引用后面移出，但在其位置留下一个新的默认值。默认值是一个单独的拥有值，因此在作用域结束时，调用者可以安全地丢弃它。
- 或者，如果您不需要可变引用后面的旧值，可以用您已经拥有的值覆盖它 `3`，将其留给调用者稍后丢弃。当您这样做时，原来在可变引用后面的值将立即被丢弃。
- 最后，如果您有两个可变引用，可以交换它们的值 4，因为两个引用最终都会拥有一个合法的拥有值来释放。

##### 内部可变性

某些类型提供内部可变性，意味着它们允许您通过共享引用来修改值。这些类型通常依赖于附加机制（如原子CPU指令）或不变量来提供安全的可变性，而不依赖于独占引用的语义。这些类型通常分为两类：一类是通过共享引用获取可变引用的类型，另一类是通过共享引用替换值的类型。

- 第一类包括`Mutex`和`RefCell`等类型，它们包含安全机制，以确保对其提供可变引用的任何值一次只能存在一个可变引用（且没有共享引用）。在底层，这些类型（以及类似它们的类型）都依赖于一种称为`UnsafeCell`的类型，其名称应立即使您犹豫使用它。我们将在`第9章中更详细地介绍UnsafeCell`，但现在您应该知道它是通过共享引用进行变异的唯一正确方式。

- 提供内部可变性的其他类型类别是那些不提供对内部值的可变引用，而只提供用于就地操作该值的方法的类型。std::sync::atomic中的原子整数类型和std::cell::Cell类型属于此类别。您无法直接获取usize或i32类型的引用，但可以在给定时间点读取和替换其值。
**注意** 标准库中的Cell类型是通过不变量实现安全内部可变性的有趣示例。它不能在线程之间共享，并且从不提供对Cell中包含的值的引用。相反，所有方法要么完全替换值，要么返回包含的值的副本。由于无法存在对内部值的引用，因此始终可以安全地移动它。并且由于Cell不能在线程之间共享，即使通过共享引用进行变异，内部值也永远不会被同时变异。

##### 生命周期

如果您正在阅读本书，那么您可能已经熟悉生命周期的概念，可能是通过编译器关于生命周期规则违规的重复通知。这种程度的理解将对您编写的大多数Rust代码有所帮助，但是当我们深入研究Rust的更复杂部分时，您将需要一个更严格的心智模型来处理。

新手Rust开发人员通常被教导将生命周期视为与作用域对应的：当您引用某个变量时，生命周期开始，当该变量被移动或超出作用域时，生命周期结束。这通常是正确的，也通常很有用，但实际情况要复杂一些。
生命周期实际上是对某个引用必须有效的代码区域的名称。虽然生命周期通常与作用域重合，但它不一定要这样，我们将在本节后面看到。

**生命周期和借用检查器**
Rust生命周期的核心是借用检查器。每当使用一个带有某个生命周期'a的引用时，借用检查器会检查'a是否仍然有效。它通过追踪从使用点回溯到'a开始的路径（即引用被获取的地方），并检查沿该路径是否存在冲突的使用。这确保引用仍然指向一个可以安全访问的值。这类似于我们在本章前面讨论的高级“数据流”心智模型；编译器检查我们正在访问的引用的流动是否与其他并行流动冲突。
清单1-8展示了一个简单的代码示例，其中包含对引用x的生命周期注解。

```rust
let mut x = Box::new(42);
1 let r = &x; // 'a
if rand() > 0.5 {
2  *x = 84;
} else {
3   println!("{}", r); // 'a
}4
```

清单1-8：生命周期不需要连续。

- 当我们对x进行引用时，生命周期从1开始。在第一个分支2中，我们立即尝试通过将其值更改为84来修改x，这需要一个&mut x。借用检查器获取了对x的可变引用，并立即检查其使用情况。它发现在引用被获取和使用之间没有冲突的使用，因此接受了这段代码。如果您习惯于将生命周期视为作用域，这可能会让您感到惊讶，因为`r`在2处仍然在作用域内（它在4处超出作用域）。但是借用检查器足够聪明，能够意识到如果执行此分支，则r以后永远不会被使用，因此在这里对x进行可变访问是可以的。或者换句话说，从1开始创建的生命周期不延伸到此分支：从r到2没有流动，因此没有冲突的流动。然后，借用检查器在3处找到了对r的使用。它沿着路径返回到1，并没有发现冲突的使用（2不在该路径上），因此也接受了这个使用。
- 如果我们在清单1-8中的4处添加了对r的另一个使用，代码将无法编译。生命周期'a将从1一直持续到4（对r的最后一次使用），当借用检查器检查我们对r的新使用时，它将在2处发现冲突的使用。
- 生命周期可能变得非常复杂。在清单1-9中，您可以看到一个具有间断的生命周期示例，它在开始和最终结束之间是不有效的。

```rust

let mut x = Box::new(42);
1 let mut z = &x; // 'a
for i in 0..100 {
2  println!("{}", z); // 'a
3  x = Box::new(i);
4  z = &x; // 'a
}

println!("{}", z); // 'a
```

清单1-9：生命周期可以有空洞。

- 生命周期从1开始，当我们对x进行引用时。然后在3处移出x，结束了生命周期'a，因为它不再有效。借用检查器通过将'a结束于2处来接受此移动，这样在3处就没有冲突的流动了。然后，我们通过在4处更新z的引用来重新启动生命周期。无论代码现在是否循环回到2处还是继续到最后的打印语句，这两个使用现在都有一个有效的值可以流动，而且没有冲突的流动，因此借用检查器接受了这段代码！
- 再次强调，这与我们之前讨论的内存数据流模型完全一致。当x被移动时，z停止存在。当我们稍后重新分配z时，我们创建了一个完全新的变量，它只存在于那一点之后。恰好这个新变量也被命名为z。在这个模型中，这个例子并不奇怪。
**注意** 借用检查器是保守的，也必须是保守的。如果它不确定一个借用是否有效，它会拒绝它，因为允许一个无效的借用的后果可能是灾难性的。借用检查器不断变得更加智能，但有时它需要帮助来理解为什么一个借用是合法的。这就是为什么我们有不安全的Rust的一部分原因。
**泛型生命周期**
有时您需要在自己的类型中存储引用。这些引用需要有一个生命周期，以便在类型的各种方法中使用时，借用检查器可以检查它们的有效性。特别是，如果您希望类型的某个方法返回一个超出self引用的引用。
- Rust允许您使类型定义在一个或多个生命周期上泛型化，就像允许您使其在类型上泛型化一样。Steve Klabnik和Carol Nichols的《Rust编程语言》（No Starch Press，2018）对这个主题进行了详细介绍，所以我不会在这里重复基础知识。但是，当您编写更复杂的此类类型时，有两个关于此类类型和生命周期之间交互的细微差别需要注意。
- 首先，如果您的类型还实现了Drop，则删除您的类型将计为您的类型泛型生命周期或类型的使用。实际上，当删除您的类型的实例时，借用检查器将在删除之前检查您的类型的任何泛型生命周期是否仍然可以使用。这是必要的，以防您的删除代码确实使用了其中的任何引用。如果您的类型没有实现Drop，则删除该类型不计为使用，用户可以自由地忽略存储在该类型中的任何引用，只要他们不再使用它，就像我们在清单1-7中看到的那样。我们将在第9章中更详细地讨论有关删除的规则。
- 其次，虽然一个类型可以在多个生命周期上泛型化，但这样做通常只会使类型签名变得不必要复杂。通常情况下，一个类型在单个生命周期上泛型化就足够了，编译器将使用较短的生命周期作为插入到类型中的任何引用的生命周期。只有当您有一个包含多个引用的类型，并且其方法返回应与这些引用之一的生命周期相关联的引用时，才真正需要使用多个泛型生命周期参数。
- 考虑清单1-10中的类型，它为您提供了一个按特定其他字符串分隔的字符串的部分的迭代器。

```rust
struct StrSplit<'s, 'p> {
  delimiter: &'p str,
  document: &'s str,
}
impl<'s, 'p> Iterator for StrSplit<'s, 'p> {
  type Item = &'s str;
  fn next(&self) -> Option<Self::Item> {
     todo!()
  }
}
fn str_before(s: &str, c: char) -> Option<&str> {
   StrSplit { document: s, delimiter: &c.to_string() }.next()
}
```

清单1-10：需要在多个生命周期上泛型化的类型

- 当构造此类型时，您必须提供要搜索的分隔符和文档，两者都是对字符串值的引用。当您请求下一个字符串时，您会得到对文档的引用。考虑一下，如果在此类型中使用单个生命周期会发生什么。迭代器产生的值将与文档和分隔符的生命周期相关联。这将使得无法编写str_before函数：返回类型将具有与函数局部变量（通过to_string生成的String）相关联的生命周期，而借用检查器将拒绝该代码。

**生命周期的变异性**
变异性是程序员经常接触但很少知道名称的概念，因为它大多是不可见的。乍一看，变异性描述了哪些类型是其他类型的子类型，以及何时可以将子类型用于超类型（反之亦然）。广义上讲，如果类型A至少与类型B一样有用，则类型A是类型B的子类型。变异性是为什么在Java中，您可以将Turtle传递给接受Animal的函数，如果Turtle是Animal的子类型，或者为什么在Rust中，您可以将'静态str传递给接受' a str的函数。

- 虽然变异性通常隐藏在视线之外，但它经常出现，我们需要对其有一定的了解。`Turtle是Animal的子类型`，因为T`urtle比某个未指定的Animal`更“有用”-Turtle可以做任何Animal可以做的事情，可能还有更多。同样，'静态是'a的子类型，因为'静态的生命周期至少与任何'a一样长，因此更有用。或者更一般地说，如果'b：'a（'b的生命周期超过'a），那么'b是'a的子类型。这显然不是正式定义，但足够接近以实际使用。
- 所有类型都有一个变异性，它定义了可以在该类型的位置使用哪些其他类似类型。变异性有三种类型：协变、不变和逆变。如果您可以在类型的位置上使用子类型，那么该类型是协变的。例如，如果一个变量的类型是' a T，您可以为其提供类型为'静态T的值，因为' a T在'a上是协变的。' a T在T上也是协变的，因此您可以将`&Vec<&'静态str>传递给接受&Vec<&'a str>的函数`。
- 有些类型是不变的，这意味着您必须提供完全相同的类型。&mut T就是一个例子-如果一个函数接受&mut Vec<&'a str>，您不能将&mut `Vec<&'静态str>传递给它`。也就是说，&mut T在T上是不变的。如果可以，函数可以将一个短命的字符串放入Vec中，而调用者则会继续使用它，`认为它是Vec<&'静态str>`，因此包含的字符串是'静态的！任何提供可变性的类型通常都是不变的，原因是相同的-例如，`Cell<T>` 在T上是不变的。
- 最后一类逆变性出现在函数参数中。如果函数的参数越不“有用”，函数类型就越有用。如果将参数类型的变异性与其作为函数参数时的变异性进行对比，这一点就更清楚了：

```rust
let x: &'static str; // more useful, lives longer
let x: &'a str; // less useful, lives shorter
fn take_func1(&'static str) // stricter, so less useful
fn take_func2(&'a str) // less strict, more useful
```

- 这种翻转的关系表明Fn(T)在T上是逆变的。
- 那么为什么在涉及生命周期时需要学习变异性呢？
当您考虑泛型生命周期参数与借用检查器的交互时，变异性变得相关。考虑一个像清单1-11中所示的类型，它在单个字段中使用了多个生命周期。

```rust
struct MutStr<'a, 'b> {
s: &'a mut &'b str
}
let mut s = "hello";
1 *MutStr { s: &mut s }.s = "world";
println!("{}", s);
```

清单1-11：需要在多个生命周期上泛型化的类型

- 乍一看，这里使用两个生命周期似乎是不必要的——我们没有需要区分结构不同部分借用的方法，就像清单1-10中的StrSplit一样。但是，如果将这里的两个生命周期替换为单个'a，代码将无法编译！这完全是因为变异性。

**注意** 1处的语法可能看起来陌生。它等同于定义一个持有MutStr的变量x，然后写入*x.s = "world"，只是没有变量，因此MutStr立即被丢弃。

- 在1处，编译器必须确定生命周期参数（s）应设置为什么生命周期。如果有两个生命周期，'a 将设置为对s的借用的待确定生命周期，'b 将设置为'static，因为这是提供的字符串"hello"的生命周期。如果只有一个生命周期'a，编译器推断该生命周期必须是'static。
- 当我们稍后尝试通过共享引用访问字符串引用s以打印它时，编译器尝试缩短MutStr使用的s的可变借用，以允许对s的共享借用。
- 在双生命周期的情况下，'a 在println之前简单地结束，'b 保持不变。另一方面，在单生命周期的情况下，我们遇到了问题。编译器希望缩短对s的借用，但要做到这一点，它还必须缩短对str的借用。尽管 'static str 通常可以缩短为任何'a str（'a T在'a 上是协变的），但在这里它在&mut T后面，而&mut T在T上是不变的。不变性要求相关类型永远不会被替换为子类型或超类型，因此编译器缩短借用的尝试失败，并报告列表仍然被可变借用。糟糕！
- 由于不变性所施加的灵活性降低，您希望确保您的类型在尽可能多的泛型参数上保持协变（或在适当的情况下是逆变的）。如果这需要引入额外的生命周期参数，您需要仔细权衡添加另一个参数的认知成本与不变性的人体工程学成本。

#### 总结

本章的目标是建立一个坚实的共享基础，以便在接下来的章节中进行构建。到目前为止，我希望您感到对Rust的内存和所有权模型有了牢固的掌握，并且您可能从借用检查器中获得的错误似乎不再神秘。您可能已经知道我们在这里介绍的一些内容，但希望本章能给您一个更全面的整体形象，了解它们如何相互配合。在下一章中，我们将对类型进行类似的操作。我们将介绍类型在内存中的表示方式，了解泛型和特性如何生成运行代码，并查看Rust为更高级用例提供的一些特殊类型和特性构造。

### 2 类型

现在基础知识已经介绍完毕，我们将看一下Rust的类型系统。我们将跳过《Rust编程语言》中介绍的基础知识，而是直接深入探讨不同类型在内存中的布局、特性和特性限制、存在类型以及在跨crate边界使用类型的规则。

#### 类型在内存中的布局

每个Rust值都有一个类型。类型在Rust中有很多用途，正如我们在本章中所看到的，但其中最基本的作用之一是告诉您如何解释内存中的位。例如，位序列0b10111101（用十六进制表示为0xBD）本身并没有任何意义，直到您为其分配一个类型。当在类型u8下解释时，该位序列表示数字189。当在类型i8下解释时，它表示-67。当您定义自己的类型时，编译器的工作是确定定义类型的每个部分在内存中的表示位置。您的结构体的每个字段在位序列中的位置是什么？您的枚举的鉴别器存储在哪里？在编写更高级的Rust代码时，了解这个过程的工作原理非常重要，因为这些细节会影响代码的正确性和性能。

##### 对齐

- 在我们讨论如何确定类型的内存表示之前，我们首先需要讨论对齐的概念，它决定了类型的字节可以存储在哪里。一旦确定了类型的表示，您可能会认为可以将任意内存位置视为该类型并解释存储在那里的字节。虽然从理论上讲这是正确的，但在实践中，硬件也限制了给定类型可以放置的位置。最明显的例子是指针指向字节而不是位。如果您将类型T的值放置在计算机内存的第4位开始的位置，您将无法引用其位置；您只能创建一个指向字节0或字节1（第8位）的指针。因此，无论其类型如何，所有值都必须从字节边界开始。我们说所有值至少需要字节对齐-它们必须放置在8位的倍数地址上。
- 有些值的对齐规则比仅仅字节对齐更严格。在CPU和内存系统中，内存通常以大于单个字节的块进行访问。例如，在64位CPU上，大多数值以8字节（64位）的块进行访问，每个操作都从8字节对齐的地址开始。这被称为CPU的字长。然后，CPU使用一些巧妙的方法来处理读取和写入较小的值，或者跨越这些块边界的值。
- 在可能的情况下，您希望确保硬件可以在其“本机”对齐方式下运行。为了看到原因，考虑一下如果尝试读取从8字节块的中间开始的i64会发生什么（即，指向它的指针不是8字节对齐）。硬件将不得不进行两次读取-一次从第一个块的后半部分到达i64的开头，一次从第二个块的前半部分读取剩余的i64-然后将结果拼接在一起。这是非常低效的。由于操作分布在对底层内存的多次访问中，如果您从不同的线程同时写入的内存上读取，可能会得到奇怪的结果。您可能在另一个线程的写入发生之前读取前4个字节，然后在写入之后读取后4个字节，导致值损坏。
- 对于未对齐的数据进行操作被称为未对齐访问，可能导致性能下降和并发问题。因此，许多CPU操作要求或强烈建议其参数是自然对齐的。自然对齐的值是其对齐与其大小相匹配的值。因此，例如，对于8字节的加载，提供的地址必须是8字节对齐的。
- 由于对齐访问通常更快并提供更强的一致性语义，编译器会尽可能利用它们。它通过为每个类型分配一个根据其包含的类型计算出的对齐方式来实现这一点。内置值通常对齐到其大小，因此u8是字节对齐的，u16是2字节对齐的，u32是4字节对齐的，u64是8字节对齐的。复杂类型-包含其他类型的类型-通常被分配为其包含的任何类型的最大对齐方式。例如，包含u8、u16和u32的类型将是4字节对齐的，因为有u32的存在。

##### 布局

现在您了解了对齐，我们可以探讨编译器如何确定类型的内存表示，即布局。默认情况下，正如您很快将看到的，Rust编译器对于如何布局类型几乎没有提供任何保证，这对于理解底层原理来说是一个很差的起点。幸运的是，Rust提供了一个repr属性，您可以将其添加到类型定义中，以请求该类型的特定内存表示。您最常见的可能是repr(C)。顾名思义，它以与C或C++编译器布局相兼容的方式布局类型。这在使用外部函数接口与其他语言进行交互的Rust代码中非常有用，我们将在第11章中讨论外部函数接口时详细介绍，因为Rust将生成与其他语言编译器期望的布局相匹配的布局。由于C布局是可预测的且不会更改，repr(C)在不安全的上下文中也非常有用，如果您使用原始指针与该类型一起工作，或者如果您需要在两个已知具有相同字段的不同类型之间进行类型转换。当然，它也非常适合进入布局算法的第一步。

**注意** 另一个有用的表示是repr(transparent)，它只能用于具有单个字段的类型，并保证外部类型的布局与内部类型完全相同。这在与“newtype”模式结合使用时非常方便，其中您可能希望按照内存表示操作某些struct A和struct NewA(A)。

- 那么，让我们看看编译器如何在内存中布局repr(C)中的特定类型：清单2-1中的Foo类型。您认为编译器会如何在内存中布局它？

```rust
#[repr(C)]
struct Foo {
  tiny: bool,
  normal: u32,
  small: u8,
  long: u64,
  short: u16,
}
 ```

清单2-1：对齐影响布局。

- 首先，编译器看到了字段tiny，其逻辑大小为1位（true或false）。但是由于CPU和内存是以字节为单位操作的，tiny在内存表示中被分配了1字节。接下来，normal是一个4字节的类型，所以我们希望它是4字节对齐的。但是，即使Foo是对齐的，我们分配给tiny的1字节也会使normal错过它的对齐。为了纠正这个问题，编译器在tiny和normal之间的内存表示中插入了3字节的填充-这些字节具有不确定的值，在用户代码中被忽略-。填充中没有值，但它占用了空间。
- 对于下一个字段small，对齐很简单：它是一个1字节的值，并且当前结构体中的字节偏移量为1 + 3 + 4 = 8。这已经是字节对齐的，所以small可以紧跟在normal之后。但是，对于long，我们又遇到了问题。现在，我们已经进入了9字节的Foo。如果Foo是对齐的，那么long不是我们希望的8字节对齐，所以我们必须再插入另外7字节的填充来重新对齐long。这也方便地确保了我们需要的2字节对齐，以容纳最后一个字段short，总共为26字节。现在，我们已经遍历了所有字段，还需要确定Foo本身的对齐方式。规则是使用Foo的任何字段的最大对齐方式，这将是8字节，因为有long。因此，为了确保如果将Foo放置在数组中（例如）时Foo保持对齐，编译器添加了最后的6字节填充，使Foo的大小成为其对齐的倍数，即32字节。
- 现在我们准备摆脱C的遗留问题，并考虑一下如果我们在清单2-1中没有使用repr(C)会发生什么。C表示的主要限制之一是它要求我们按照原始结构定义中的顺序放置所有字段。默认的Rust表示repr(Rust)消除了这个限制，以及其他一些较小的限制，例如对于具有相同字段的类型的确定性字段排序。也就是说，即使两个不同的类型共享所有相同的字段，类型相同，顺序相同，也不能保证在使用默认的Rust布局时它们的布局相同！
- 由于我们现在可以重新排序字段，我们可以按照大小递减的顺序放置它们。这意味着我们不再需要Foo字段之间的填充；字段本身用于实现所需的对齐！现在，Foo只是其字段的大小：只有16字节。这是Rust默认情况下不会给出关于类型在内存中布局的许多保证的原因之一：通过给编译器更多的自由重新排列事物，我们可以生成更高效的代码。
- 实际上，还有第三种布局类型的方式，那就是告诉编译器我们不希望在字段之间有任何填充。这样做意味着我们愿意承受使用未对齐访问的性能损失。最常见的用例是当每个额外的字节内存的影响都可以感知到时，例如如果您有大量类型的实例，如果您的内存非常有限，或者如果您正在通过像网络连接这样的低带宽介质发送内存表示。要选择此行为，您可以使用#[repr(packed)]注解您的类型。请记住，这可能导致更慢的代码，并且在极端情况下，如果尝试执行CPU仅对齐参数支持的操作，可能会导致程序崩溃。
- 有时，您希望为特定字段或类型提供比其技术要求更大的对齐方式。您可以使用属性#[repr(align(n))]来实现这一点。这样做的一个常见用例是确保存储在内存中连续的不同值（例如在数组中）最终位于CPU的不同缓存行中。这样，您可以避免伪共享，伪共享可能导致并发程序的性能严重下降。伪共享发生在两个不同的CPU访问共享缓存行的不同值时；虽然它们理论上可以并行操作，但它们最终都会争夺更新缓存中的同一条目。我们将在第10章中更详细地讨论并发性。

##### 复杂类型

- 您可能会好奇编译器如何在内存中表示其他Rust类型。这里是一个快速参考：

  - **元组** 以与元组值相同顺序的相同类型字段的结构体表示。
  - **数组** 作为包含类型的连续序列表示，元素之间没有填充。
  - **联合** 布局独立选择每个变体。对齐方式是所有变体中的最大值。
  - **枚举** 与联合相同，但有一个额外的隐藏共享字段，用于存储枚举变体的鉴别器。鉴别器是代码用来确定给定值包含的枚举变体的值。鉴别器字段的大小取决于变体的数量。
  
##### 动态大小类型和宽指针

您可能在Rust文档的各个奇怪角落和错误消息中遇到过标记特征Sized。通常，它会出现，因为编译器希望您提供一个Sized类型，但您（显然）没有提供。在Rust中，大多数类型都会自动实现Sized，也就是说，它们在编译时具有已知的大小，但有两种常见类型不是：特征对象和切片。如果您有一个dyn Iterator或[u8]，它们没有明确定义的大小。它们的大小取决于程序运行时才能知道的一些信息，而不是在编译时，这就是为什么它们被称为动态大小类型（DST）。没有人事先知道您的函数接收到的dyn Iterator是这个200字节的结构还是那个8字节的结构。这带来了一个问题：通常，编译器必须知道某个东西的大小才能生成有效的代码，例如对于类型为(i32，dyn Iterator，[u8]，i32)的元组分配多少空间，或者如果您的代码尝试访问第四个字段时要使用的偏移量。但是，如果类型不是Sized，那么这些信息是不可用的。

编译器几乎在任何地方都要求类型是Sized的。结构字段、函数参数、返回值、变量类型和数组类型都必须是Sized的。这个限制是如此常见，以至于您编写的每个类型约束都包括T: Sized，除非您使用T: ?Sized明确地选择退出（?表示“可能不是”）。但是，如果您有一个DST并且希望对其执行某些操作，这样做并不是很有帮助，例如，如果您真的希望函数接受特征对象或切片作为参数。
解决无大小和有大小类型之间差距的方法是将无大小类型放在宽指针（也称为胖指针）后面。宽指针与普通指针非常相似，但它包含一个额外的字大小字段，该字段提供了编译器生成与指针一起工作的合理代码所需的附加信息。当您对DST引用时，编译器会自动为您构造一个宽指针。对于切片，额外的信息只是切片的长度。对于特征对象-好吧，我们稍后再说。关键是，宽指针是Sized的。具体而言，它是usize的两倍大小（usize是目标平台上一个字的大小）：一个usize用于保存指针，一个usize用于保存“完成”类型所需的额外信息。
**注意** Box和Arc也支持存储宽指针，这就是为什么它们都支持T: ?Sized的原因。

#### 特质和特质约束

特质是Rust类型系统的关键部分，它们是使类型能够在定义时互操作的粘合剂。《Rust编程语言》对如何定义和使用特质进行了很好的介绍，所以我不会在这里详细介绍。相反，我们将看一下特质的一些更技术性的方面：它们是如何实现的、您必须遵守的限制以及一些更奇特的特质用法。

##### 编译和分派

到目前为止，您可能已经在Rust中编写了相当数量的泛型代码。您在类型和方法上使用了泛型类型参数，甚至可能偶尔使用了一些特质约束。但是，您是否曾经想过在编译泛型代码时实际发生了什么，或者在对dyn Trait调用特质方法时会发生什么？

当您编写一个对T泛型的类型或函数时，实际上是在告诉编译器为每个类型T创建一个副本的类型或函数。当您构造一个Vec或HashMap<String, bool>时，编译器实际上是将泛型类型及其所有实现块复制粘贴，并将每个泛型参数的所有实例替换为您提供的具体类型。它为每个T替换为i32创建了一个完整的Vec类型的副本，并为每个K替换为String和每个V替换为bool创建了一个完整的HashMap类型的副本。
**注意** 实际上，编译器并不真正进行完整的复制粘贴。它只复制您使用的代码的部分，因此如果您从未在Vec上调用find，那么find的代码将不会被复制和编译。

泛型函数也是如此。考虑清单2-2中的代码，它展示了一个泛型方法。

```rust
impl String {
pub fn contains(&self, p: impl Pattern) -> bool {
     p.is_contained_in(self)
  }
}
```

清单2-2：使用静态分派的泛型方法

- 对于每种不同的模式类型，都会创建该方法的副本（回想一下，impl Trait是<T: Trait>的简写）。我们需要为每种impl Pattern类型创建一个不同的函数体副本，因为我们需要知道is_contained_in函数的地址以便调用它。CPU需要被告知跳转到哪里并继续执行。对于任何给定的模式，编译器知道该地址是该模式类型实现该特质方法的地方的地址。但是，我们无法为任何类型使用一个地址，因此我们需要为每种类型创建一个副本，每个副本都有自己的地址跳转。这被称为静态分派，因为对于方法的任何给定副本，我们“分派到”的地址是已知的静态地址。

**注意** 您可能已经注意到，在这个上下文中，“静态”一词有点多义。静态通常用于指代在编译时已知的任何内容，或者可以被视为已知的内容，因为它可以写入静态内存，正如我们在第1章中讨论的那样。

- 从泛型类型到许多非泛型类型的过程称为单态化，这也是泛型Rust代码通常与非泛型代码一样高效的原因之一。当编译器开始优化代码时，它就好像没有泛型存在一样！每个实例都是单独优化的，并且所有类型都是已知的。因此，代码的效率与直接调用传入的模式的is_contained_in方法一样高效，而不需要任何特质。编译器对所涉及的类型有完全的了解，甚至可以选择内联is_contained_in的实现。
- 单态化也有代价：所有这些类型的实例化需要单独编译，如果编译器无法优化它们，这可能会增加编译时间。每个单态化函数也会产生自己的机器代码块，这可能会使程序变得更大。由于不同实例化的泛型类型方法之间不共享指令，CPU的指令缓存也不那么有效，因为现在需要保存多个实际上相同指令的副本。
  
  **非泛型内部函数**

- 通常，泛型方法中的大部分代码与类型无关。例如，考虑HashMap::insert的实现。计算提供的键的哈希值的代码取决于映射的键类型，但遍历映射的桶以找到插入点的代码可能与类型无关。在这种情况下，共享生成的非泛型方法的机器代码将更高效，只有在实际需要时才生成不同的副本。

- 您可以使用以下模式来处理这种情况：在泛型方法内部声明一个非泛型的辅助函数，执行共享操作。这样，编译器只需为您复制粘贴与类型相关的代码，而辅助函数可以共享使用。
- 将函数设置为内部函数的好处是，您不会在模块中污染一个单一目的的函数。相反，您可以在方法之外声明这样的辅助函数；只需注意不要将其设置为泛型impl块下的方法，否则它仍将被单态化。

- 静态分派的替代方案是动态分派，它使代码能够在不知道泛型类型的情况下调用特质方法。我之前说过，在清单2-2中需要多个方法实例的原因是，否则您的程序将不知道调用给定模式上的特质方法is_contained_in的地址。好吧，使用动态分派，调用方只需告诉您。如果您将impl Pattern替换为&dyn Pattern，您告诉调用方他们必须为此参数提供两个信息：模式的地址和is_contained_in方法的地址。在实践中，调用方给我们提供了一个指向称为虚方法表或vtable的内存块的指针，该内存块保存了有关该类型的所有特质方法的实现的地址，其中之一是is_contained_in。当方法内部的代码想要在提供的模式上调用特质方法时，它会在vtable中查找该模式的is_contained_in实现的地址，然后调用该地址处的函数。这使我们能够在不管调用方想要使用什么类型的情况下使用相同的函数体。
**注意** 每个vtable还包含有关具体类型的布局和对齐方式的信息，因为始终需要这些信息来处理类型。如果您想要一个显式vtable的示例，请查看std::task::RawWakerVTable类型。
- 当我们使用dyn关键字选择动态分派时，您会注意到我们必须在其前面加上&符号。原因是我们不再在编译时知道调用方传递的模式类型的大小，因此我们不知道为其分配多少空间。换句话说，dyn Trait是!Sized，其中!表示不。为了使其Sized，以便我们可以将其作为参数接受，我们将其放在指针后面（我们知道其大小）。由于我们还需要传递方法地址表，因此该指针变成了宽指针，其中额外的字保存了指向vtable的指针。您可以使用任何能够保存宽指针的类型进行动态分派，例如&mut、Box和Arc。清单2-3显示了清单2-2的动态分派等效形式。

```rust
impl String {
  pub fn contains(&self, p: &dyn Pattern) -> bool {
    p.is_contained_in(&*self)
  }
}
```

清单2-3：使用动态分派的通用方法

- 实现特质的类型和其vtable的组合被称为特质对象。大多数特质都可以转换为特质对象，但并非所有特质都可以。例如，Clone特质的clone方法返回Self，因此无法转换为特质对象。如果我们接受一个dyn Clone特质对象，然后调用clone方法，编译器将不知道返回的类型是什么。或者，考虑标准库中的Extend特质，它的extend方法对提供的迭代器的类型是泛型的（因此可能有多个实例）。如果您调用一个接受dyn Extend的方法，那么在特质对象的vtable中就没有一个单一的地址可以放置extend；必须为extend可能被调用的每种类型都有一个条目。这些是不可对象安全的特质的示例，因此可能无法转换为特质对象。为了是对象安全的，特质的所有方法都不能是泛型的，也不能使用Self类型。此外，特质不能有任何静态方法（即，第一个参数不解引用为Self的方法），因为无法知道要调用的方法的哪个实例。例如，不清楚FromIterator::from_iter(&[0])应该执行哪段代码。
- 在阅读有关特质对象的文档时，您可能会看到对特质绑定Self: Sized的提及。这样的绑定意味着Self没有通过特质对象使用（因为它将是!Sized）。您可以将该绑定放在特质上，以要求该特质永远不使用动态分派，或者可以将其放在特定方法上，以使该方法在通过特质对象访问特质时不可用。具有where Self: Sized绑定的方法在检查特质是否对象安全时被豁免。
- 动态分派可以减少编译时间，因为不再需要编译类型和方法的多个副本，并且可以提高CPU指令缓存的效率。然而，它也阻止编译器针对特定使用的类型进行优化。使用动态分派，清单2-2中的find方法只能通过vtable插入对函数的调用-编译器无法执行任何额外的优化，因为它不知道该函数调用的另一侧的代码是什么。此外，对特质对象的每个方法调用都需要在vtable中进行查找，这会增加一小部分开销，而不是直接调用方法。

- 当你在静态分派和动态分派之间做选择时，很少有明确的正确答案。总的来说，在库中使用静态分派，在二进制文件中使用动态分派是比较合适的。在库中，你希望允许用户根据他们的需求选择最适合他们的分派方式。如果你使用动态分派，他们也被迫这样做，而如果你使用静态分派，他们可以选择是否使用动态分派。另一方面，在二进制文件中，你编写的是最终代码，所以除了你编写的代码之外，没有其他需求需要考虑。动态分派通常允许你编写更清晰的代码，省略了泛型参数，并且编译速度更快，尽管性能上可能会有一些损失，所以通常对于二进制文件来说是更好的选择。

##### 泛型特质

Rust的特质可以通过两种方式进行泛型化：使用泛型类型参数，例如`trait Foo<T>`，或者使用关联类型，例如trait Foo { type Bar; }。这两者之间的区别并不立即显而易见，但幸运的是，有一个简单的经验法则：如果您期望给定类型的特质只有一个实现，请使用关联类型；否则，请使用泛型类型参数。

- 这样做的原因是，关联类型通常更容易使用，但不允许多个实现。因此，简而言之，建议您尽可能使用关联类型。
- 对于泛型特质，用户必须始终指定所有泛型参数并重复任何参数的约束。这可能很快变得混乱且难以维护。如果您向特质添加了一个泛型参数，那么该特质的所有用户也必须进行更新以反映这些更改。由于对于给定类型可能存在多个特质的实现，编译器可能很难确定您要使用的特质实例，从而导致糟糕的消除歧义的函数调用，例如`FromIterator::<u32>::from_iter`。但好处是，您可以为同一类型多次实现特质，例如，您可以为您的类型实现多个右侧类型的PartialEq，或者您可以同时为`FromIterator<T>`和FromIterator<&T>实现，其中T: Clone，这正是泛型特质提供的灵活性。
- 另一方面，对于关联类型，编译器只需要知道实现特质的类型，然后跟随所有关联类型（因为只有一个实现）。这意味着所有约束都可以存在于特质本身中，不需要在使用时重复。反过来，这允许特质添加进一步的关联类型而不影响其用户。由于类型决定了特质的所有关联类型，您永远不必使用前面段落中显示的统一函数调用语法进行消除歧义。然而，您不能为多个Target类型实现Deref，也不能为多个不同的Item类型实现Iterator。

##### 一致性和孤儿规则

Rust对于可以在哪里实现特质以及可以在哪些类型上实现它们有一些相当严格的规则。这些规则存在的目的是保持一致性属性：对于任何给定的类型和方法，对于该类型使用的方法的实现只有一个正确的选择。为了看到这一点的重要性，考虑一下如果我可以为标准库中的bool类型编写自己的Display特质实现会发生什么。现在，对于任何试图打印bool值并包含我的crate的代码，编译器将不知道是选择我编写的实现还是标准库中的实现。两种选择都不正确，也没有比另一种更好，编译器显然无法随机选择。如果没有涉及标准库，而是我们有两个相互依赖的crate，并且它们都为某个共享类型实现了一个特质，那么同样的问题也会发生。一致性属性确保编译器永远不会陷入这些情况，也永远不必做出这些选择：总是只有一个明显的选择。

- 维护一致性的一种简单方法是确保只有定义特质的crate才能为该特质编写实现；如果其他人无法实现该特质，那么就不会在其他地方出现冲突的实现。然而，在实践中，这太过限制性，会使特质变得无用，因为除非您将自己的类型包含在定义crate中，否则无法为自己的类型实现诸如std::fmt::Debug和serde::Serialize之类的特质。相反的极端观点是，只能为自己的类型实现特质，解决了这个问题，但引入了另一个问题：定义特质的crate现在无法为标准库或其他流行crate中的类型提供该特质的实现！理想情况下，我们希望找到一些规则，以在希望下游crate为其自己的类型实现上游特质之间取得平衡，同时又希望上游crate能够添加自己特质的实现而不会破坏下游代码。

**注意** 上游是指您的代码依赖的内容，下游是指依赖于您的代码的内容。通常，这些术语在直接的crate依赖关系意义上使用，但它们也可以用于指代代码库的官方分支的权威分支 - 如果您对Rust编译器进行分支，官方Rust编译器就是您的“上游”。

- 在Rust中，确立这种平衡的规则是孤儿规则。简单地说，孤儿规则规定，只有当特质或类型是本地crate的时候，才能为类型实现特质。因此，您可以为自己的类型实现Debug，也可以为bool实现MyNeatTrait，但不能为bool实现Debug。如果尝试这样做，您的代码将无法编译，并且编译器将告诉您存在冲突的实现。
- 这可以让您走得很远；它允许您为第三方类型实现自己的特质，并为自己的类型实现第三方特质。然而，孤儿规则并不是故事的终点。还有一些额外的含义、注意事项和例外情况需要注意。

**全局实现**
孤儿规则允许您使用类似impl`<T> MyTrait for T where T:`的代码来为一系列类型实现特质。这是一种全局实现 - 它不仅限于特定的类型，而是适用于广泛的类型范围。只有定义特质的crate才允许编写全局实现，并且向现有特质添加全局实现被认为是一种破坏性的更改。如果不是这样，包含impl MyTrait for Foo的下游crate可能会因为您更新定义MyTrait的crate而突然停止编译，并出现关于冲突实现的错误。
**基本类型**
某些类型非常重要，以至于必须允许任何人在其上实现特质，即使这似乎违反了孤儿规则。这些类型使用#[fundamental]属性进行标记，目前包括&、&mut和Box。对于孤儿规则而言，基本类型可以说不存在 - 在检查孤儿规则之前，它们实际上被擦除了，以便您可以为&MyType实现IntoIterator，例如。只有孤儿规则的话，这个实现是不允许的，因为它为外部类型实现了外部特质 - IntoIterator和&都来自标准库。添加基本类型的全局实现也被认为是一种破坏性的更改。
**覆盖实现**
有一些有限的情况下，我们希望允许为外部类型实现外部特质，而孤儿规则通常不允许这样做。这种情况的最简单示例是当您想要编写类似`impl From<MyType> for Vec<i32>`的代码时。在这里，From特质是外部的，Vec类型也是外部的，但没有违反一致性的危险。这是因为只有通过标准库中的全局实现（标准库无法命名MyType）才能添加冲突的实现，而这本身就是一种破坏性的更改。
为了允许这些类型的实现，孤儿规则包含了一个狭窄的例外，只允许在非常特定的情况下为外部类型实现外部特质。具体而言，只有当至少一个Ti是本地类型，并且在第一个这样的Ti之前的所有T都不是泛型类型P1..=Pn时，才允许给定的impl<P1..=Pn> ForeignTrait<T1..=Tn> for T0。泛型类型参数（Ps）允许出现在T0..Ti中，只要它们被某些中间类型覆盖即可。如果T作为其他类型的类型参数出现（例如`Vec<T>`），则T被视为已覆盖，但如果T独立存在（只是T）或仅出现在基本类型&后面，那么T就不被视为已覆盖。因此，Listing 2-4中的所有实现都是有效的。

```rust
impl<T> From<T> for MyType
impl<T> From<T> for MyType<T>
impl<T> From<MyType> for Vec<T>
impl<T> ForeignTrait<MyType, T> for Vec<T>

```

清单2-4：为外部类型实现外部特质的有效实现

然而，清单2-5中的实现是无效的。

```rust
impl<T> ForeignTrait for T
impl<T> From<T> for T
impl<T> From<Vec<T>> for T
impl<T> From<MyType<T>> for T
impl<T> From<T> for Vec<T>
impl<T> ForeignTrait<T, MyType> for Vec<T>
```

清单2-5：为外部类型实现外部特质的无效实现

- 孤儿规则的这种放宽使得在为现有特质添加新实现时，什么构成破坏性更改变得复杂。特别是，只有当新实现包含至少一个新的本地类型，并且该新的本地类型满足前面描述的例外规则时，才不会产生破坏性更改。添加任何其他新实现都是破坏性更改。

**注意** 注意，`impl<T> ForeignTrait<LocalType, T> for ForeignType`是有效的，但`impl<T> ForeignTrait<T, LocalType> for ForeignType`是无效的！这可能看起来是武断的，但如果没有这个规则，您可以编写impl`<T> ForeignTrait<T, LocalType> for ForeignType`，而另一个crate可以编写`impl<T> ForeignTrait<TheirType, T> for ForeignType`，只有当两个crate合并在一起时才会出现冲突。孤儿规则要求您的本地类型出现在类型参数之前，从而打破了这种关系，并确保如果两个crate在隔离环境中遵守一致性，它们在合并时也会遵守一致性。

##### 特质约束

标准库中充斥着特质约束，无论是HashMap中的键必须实现Hash + Eq，还是传递给thread::spawn的函数必须是FnOnce + Send + 'static。当您自己编写通用代码时，它几乎肯定会包含特质约束，否则您的代码对其泛型类型无法做太多事情。随着您编写更复杂的通用实现，您会发现自己对特质约束需要更多的准确性，因此让我们看一下一些实现这一目标的方法。

- 首先，特质约束不必采用T: Trait的形式，其中T是您的实现或类型泛型的某种类型。约束可以是任意类型限制，甚至不需要包括泛型参数、参数类型或本地类型。您可以编写像where String: Clone这样的特质约束，即使String: Clone始终为真且不包含本地类型。您还可以编写像`where io::Error: From<MyError<T>>`这样的特质约束；您的泛型类型参数不仅需要出现在左侧，这不仅允许您表达更复杂的约束，还可以避免不必要地重复约束。例如，如果您的方法想要构造一个HashMap<K, V, S>，其中键是某个泛型类型T，值是usize，而不是像where T: Hash + Eq, S: BuildHasher + Default这样写出约束，您可以写出where HashMap<T, usize, S>: FromIterator。这样可以避免查找您最终使用的方法的确切约束要求，并更清楚地传达代码的“真实”要求。正如您所看到的，如果底层特质方法的约束较复杂，它还可以显著减少约束的复杂性。

  **派生特质**

  虽然#[derive(Trait)]非常方便，但在特质约束的上下文中，您应该注意一个细微之处，即它通常是如何实现的。许多#[derive(Trait)]展开成`impl Trait for Foo<T> where T: Trait`。这通常是您想要的，但并非总是如此。例如，考虑以这种方式为`Foo<T>派生Clone，而Foo包含Arc<T>`。无论T是否实现Clone，Arc都实现了Clone，但由于派生的约束，只有当T实现Clone时，Foo才会实现Clone！这通常不是太大的问题，但它确实增加了一个不需要的约束。如果我们将类型重命名为Shared，问题可能会变得更清晰一些。想象一下，当编译器告诉用户他们无法克隆`Shared<NotClone>`时，他们会多么困惑！在撰写本文时，标准库提供的#[derive(Clone)]就是这样工作的，尽管这可能会在将来发生变化

- 有时，您希望对您的泛型类型的关联类型进行约束。例如，考虑迭代器方法flatten，它接受一个产生本身实现Iterator的项的迭代器，并生成这些内部迭代器的项的迭代器。它生成的类型Flatten是泛型的，其中I是外部迭代器的类型。如果I实现了Iterator，并且I产生的项本身实现了IntoIterator，则Flatten实现了Iterator。为了使您能够编写这样的约束，Rust允许您使用语法Type::AssocType来引用类型的关联类型。例如，我们可以使用I::Item来引用I的Item类型。如果一个类型具有多个相同名称的关联类型，例如如果提供关联类型的特质本身是泛型的（因此有许多实现），则可以使用语法`<Type as Trait>::AssocType`进行消歧义。使用这种方式，您不仅可以为外部迭代器类型编写约束，还可以为该外部迭代器的项类型编写约束。

- 在广泛使用泛型的代码中，您可能会发现自己需要编写一个关于对类型的引用的约束。通常情况下，这是可以接受的，因为您往往还会有一个可以用作这些引用的生命周期参数。然而，在某些情况下，您希望约束说“对于任何生命周期，此引用都实现此特质。”这种类型的约束被称为高阶特质约束，它在与Fn特质结合使用时特别有用。例如，假设您希望对一个函数进行泛型化，该函数接受对T的引用并返回对T内部的引用。如果您编写F: Fn(&T) -> &U，您需要为这些引用提供一个生命周期，但您真正想要说的是“任何生命周期，只要输出与输入相同。”使用高阶生命周期，您可以编写F: for<'a> Fn(&'a T) -> &'a U，表示对于任何生命周期'a，约束必须成立。Rust编译器足够聪明，当您编写类似于Fn约束的引用时，它会自动添加for，这涵盖了此功能的大多数用例。在撰写本文时，标准库仅在三个地方使用了显式形式，但确实存在，因此值得了解。

- 将所有这些内容结合起来，考虑清单2-6中的代码，它可以用于为任何可以迭代且元素具有Debug特性的类型实现Debug。
  
```rust
impl Debug for AnyIterable
where for<'a> &'a Self: IntoIterator,
for<'a> <&'a Self as IntoIterator>::Item: Debug {
fn fmt(&self, f: &mut Formatter) -> Result<(), Error> {
f.debug_list().entries(self).finish()
}}

```

清单2-6：适用于任何可迭代集合的过度泛化的Debug实现

- 您可以将此实现复制粘贴到几乎任何集合类型中，它将“正常工作”。当然，您可能希望有一个更智能的调试实现，但这很好地说明了特质约束的强大之处。

##### 标记特质

通常，我们使用特质来表示多个类型可以支持的功能；可以通过调用hash对Hash类型进行哈希，可以通过调用clone对Clone类型进行克隆，可以通过调用fmt对Debug类型进行格式化以进行调试。但并非所有特质都以这种方式发挥功能。一些特质，称为标记特质，相反地指示实现类型的属性。标记特质没有方法或关联类型，只是告诉您特定类型可以或不能以某种方式使用。例如，如果一个类型实现了Send标记特质，那么在线程边界上发送它是安全的。如果它没有实现这个标记特质，那么发送它是不安全的。这个行为没有关联的方法；它只是关于类型的一个事实。标准库中有许多这样的特质，包括Send、Sync、Copy、Sized和Unpin，它们中的大多数（除了Copy）也是自动特质；编译器会自动为类型实现它们，除非类型包含了不实现标记特质的内容。

- 标记特质在Rust中起着重要的作用：它们允许您编写捕捉代码中未直接表达的语义要求的约束。在需要类型是Send的代码中没有调用send。相反，代码假设给定的类型可以在单独的线程中使用，没有标记特质，编译器无法检查这个假设。程序员需要记住这个假设并仔细阅读代码，这是我们都知道不希望依赖的事情。这条路上充满了数据竞争、段错误和其他运行时问题。
- 类似于标记特质的是标记类型。它们是不包含数据且没有方法的单元类型（例如struct MyMarker;）。标记类型在标记类型处于特定状态时非常有用。当您希望防止用户误用API时，它们非常方便。例如，考虑一个类型SshConnection，它可能已经进行了身份验证，也可能还没有进行身份验证。您可以为SshConnection添加一个泛型类型参数，然后创建两个标记类型：Unauthenticated和Authenticated。当用户首次连接时，他们会得到`SshConnection<Unauthenticated>`。在其impl块中，您只提供一个方法：connect。connect方法返回一个`SshConnection<Authenticated>`，只有在该impl块中，您才提供运行命令等其他方法。我们将在第3章进一步讨论这种模式。

#### 存在类型

在Rust中，您很少需要为函数体中声明的变量指定类型，也不需要为调用的泛型参数的类型指定类型。这是因为类型推断，编译器根据类型出现的代码所评估的类型来决定使用的类型。编译器通常只会对变量和闭包的参数（和返回类型）进行类型推断；顶层定义，如函数、类型、特质和特质实现块，都需要您明确命名所有类型。这样做的原因有几个，但主要原因是当您至少有一些已知的起点来开始推断时，类型推断会更容易。然而，并不总是容易，甚至不可能完全命名一个类型！例如，如果您从函数返回一个闭包，或者从特质方法返回一个异步块，它的类型没有一个您可以在代码中输入的名称。

- 为了处理这种情况，Rust支持存在类型。很有可能，您已经看到了存在类型的作用。所有标记为async fn或返回类型为impl Trait的函数都具有存在返回类型：签名不给出返回值的真实类型，只是给出函数返回某个实现了一组特质的类型的一些提示，调用者可以依赖于该返回类型实现这些特质，而不是其他任何东西。

**注意** 严格来说，调用者不仅依赖于返回类型，还依赖于其他内容。编译器还会通过impl Trait在返回位置传播自动特质，如Send和Sync。我们将在下一章中更详细地讨论这个问题。

- 这种行为赋予了存在类型其名称：我们断言存在某个与签名匹配的具体类型，并且我们将其寻找的任务交给编译器。编译器通常会通过对函数体应用类型推断来找到这个类型。

- 并非所有的impl Trait实例都使用存在类型。如果您在函数的参数位置使用impl Trait，它实际上只是该函数的未命名泛型参数的简写。例如，fn foo(s: impl ToString)在大多数情况下只是fn foo<S: ToString>(s: S)的语法糖。
- 存在类型在实现具有关联类型的特质时特别有用。例如，想象一下，您正在实现IntoIterator特质。它有一个关联类型IntoIter，它保存了可以将所讨论的类型转换为的迭代器的类型。使用存在类型，您不需要定义一个单独的迭代器类型来用于IntoIter。相反，您可以将关联类型指定为impl Iterator<Item = Self::Item>，并在fn into_iter(self)中编写一个求值为迭代器的表达式，例如通过对某个现有迭代器类型进行映射和过滤。

- 存在类型不仅提供了方便，还提供了一个超越方便的功能：它们允许您执行零成本的类型擦除。您可以使用存在类型隐藏底层具体类型，而不是仅仅因为它们出现在公共签名中而导出辅助类型 - 迭代器和futures是常见的例子。您的接口的用户只会看到相关类型实现的特质，而具体类型则作为实现细节留下。这不仅简化了接口，而且还使您可以随意更改该实现，而不会破坏未来的下游代码。

#### `总结`

本章对Rust类型系统进行了全面的回顾。我们既看了编译器如何在内存中显现类型，又看了它如何推理类型本身。这是后续章节中编写不安全代码、复杂应用程序接口和异步代码的重要背景材料。您还会发现，本章中的大部分类型推理都与您设计Rust代码接口的方式有关，我们将在下一章中介绍这一点。

### 3 设计接口

**每个项目，无论大小，都有一个API。实际上，通常有几个API。其中一些是面向用户的，比如HTTP端点或命令行界面，而另一些是面向开发者的，比如库的公共接口。除此之外，Rust的crate还有许多内部接口：每个类型、特质和模块边界都有自己的微型API，与代码的其余部分进行交互。随着代码库的规模和复杂性的增长，您会发现值得投入一些思考和关注，以设计良好的内部API，以使使用和维护代码的体验尽可能愉快。在本章中，我们将讨论一些在Rust中编写符合惯用法的接口的最重要的考虑因素，无论这些接口的用户是您自己的代码还是使用您的库的其他开发者。这些原则基本上可以归结为四个：接口应该是不令人惊讶的、灵活的、明显的和受限制的。我将依次讨论这些原则，为编写可靠和可用的接口提供一些指导。**

- 我强烈建议在阅读本章后查看Rust API指南[（https://rust-lang.github.io/api-guidelines/）]。其中有一个优秀的清单，您可以按照其中的每个建议进行详细的操作。本章中的许多建议也可以通过cargo clippy工具进行检查，如果您尚未运行该工具，请开始运行。我还鼓励您阅读Rust RFC 1105[（https://rust-lang.github.io/rfcs/1105-api-evolution.html）]和The Cargo Book中关于SemVer兼容性的章节[（https://doc.rust-lang.org/cargo/reference/semver.html）]，它们涵盖了在Rust中什么是破坏性更改和什么不是破坏性更改。

#### 不令人惊讶

最小惊讶原则，也被称为最小惊讶法则，在软件工程中经常出现，对于Rust接口也同样适用。在可能的情况下，您的接口应该足够直观，以至于用户在猜测时通常能够猜对。当然，并不是您的应用程序的所有内容都会立即以这种方式直观，但任何可以不令人惊讶的地方都应该是如此。核心思想是紧密围绕用户可能已经了解的事物，以便他们不必以与他们习惯不同的方式重新学习概念。这样，您可以为他们节省脑力来解决实际上与您的接口有关的问题。

- 有多种方法可以使您的接口可预测。在这里，我们将看看如何使用命名、常见特质和人性化特质技巧来帮助用户。

##### 命名规范

用户通过名称首次接触到您的接口；他们会立即从类型、方法、变量、字段和库的名称中推断出一些信息。如果您的接口重用了其他（可能是常见的）接口中的方法和类型的名称，用户将知道他们可以对您的方法和类型做出某些假设。一个名为iter的方法可能会接受&self，并且可能会返回一个迭代器。一个名为into_inner的方法可能会接受self，并且可能会返回某种包装类型。一个名为SomethingError的类型可能会实现std::error::Error，并出现在各种结果中。通过为相同的目的重用常见名称，您使用户更容易猜测事物的功能，并使他们更容易理解您的接口中与其他接口不同的地方。

- 与此相关的是，具有相同名称的事物实际上应该以相同的方式工作。否则，例如，如果您的iter方法接受self，或者您的SomethingError类型没有实现Error，用户可能会根据他们对接口的期望编写错误的代码。他们会感到惊讶和沮丧，并且不得不花时间研究您的接口与他们的期望有何不同。当我们可以避免给用户带来这种摩擦时，我们应该这样做。

##### 类型的常见特质

在Rust中，用户通常会假设接口中的一切“只是工作的”。他们希望能够使用{:?}打印任何类型，并将任何东西发送到另一个线程，他们期望每个类型都是可克隆的。在可能的情况下，我们应该避免让用户感到惊讶，并且尽早实现大多数标准特质，即使我们目前并不需要它们。

- 由于第2章讨论的一致性规则，编译器不允许用户在需要时实现这些特质。用户不能为您接口中的外部类型实现外部特质（如Clone）。相反，他们需要在自己的类型中包装您的接口类型，即使这样，如果没有访问类型的内部，编写合理的实现可能也很困难。
- 首先，这些标准特质中的第一个是Debug特质。几乎每种类型都可以实现Debug特质，即使只是打印类型的名称也可以。在接口中实现Debug特质的最佳方式通常是使用#[derive(Debug)]，但请记住，所有派生的特质都会自动为任何泛型参数添加相同的约束。您也可以通过利用fmt::Formatter上的各种debug_辅助函数来编写自己的实现。
- 紧随其后的是 Rust 的自动 trait Send 和 Sync（以及在较小程度上是 Unpin）。如果一个类型没有实现这些 trait，那应该有一个非常好的理由。一个没有实现 Send 的类型不能放置在 Mutex 中，也不能在包含线程池的应用程序中进行传递使用。一个没有实现 Sync 的类型不能通过 Arc 进行共享，也不能放置在静态变量中。用户已经习惯了在这些上下文中类型可以正常工作，特别是在几乎所有东西都在线程池上运行的异步世界中，如果您不确保您的类型实现了这些 trait，用户会感到沮丧。如果您的类型无法实现这些 trait，请确保将此事实以及原因在文档中进行充分说明！
- 接下来，您应该实现的几乎通用的特质是Clone和Default。这些特质可以很容易地派生或实现，并且对大多数类型来说都是有意义的。如果您的类型无法实现这些特质，请确保在文档中明确说明，因为用户通常希望能够轻松创建更多（和新的）类型实例。如果他们不能，他们会感到惊讶。
- 在期望特质的层次结构中，比较特质：PartialEq、PartialOrd、Hash、Eq和Ord更进一步。特别值得注意的是PartialEq特质，因为用户最终会有两个希望使用==或assert_eq!来比较的实例。即使对于同一类型的相同实例，您的类型可能会比较相等，实现PartialEq也是值得的，以便让用户使用assert_eq!。
- PartialOrd和Hash更为特殊，可能不适用于所有情况，但在可能的情况下，您也应该实现它们。对于用户可能将其用作映射中的键或使用std::collection集合类型进行去重的类型，这一点尤其重要，因为它们往往需要这些约束。Eq和Ord对于实现类型的比较操作有额外的语义要求，超出了PartialEq和PartialOrd的要求。这些要求在这些特质的文档中有详细说明，只有在您确信这些语义实际适用于您的类型时才应该实现它们。
- 对于大多数类型来说，实现serde crate的Serialize和Deserialize特质是有意义的。这些特质可以很容易地派生，serde_derive crate甚至提供了覆盖仅适用于一个字段或枚举变体的序列化的机制。由于serde是一个第三方crate，您可能不希望添加对它的必需依赖。因此，大多数库选择提供一个serde功能，只有在用户选择时才添加对serde的支持。
- 你可能想知道为什么我没有在这一节中包括可派生的 trait Copy。有两个因素使 Copy 与其他提到的 trait 有所不同。首先，用户通常不希望类型是 Copy；相反，他们倾向于认为如果他们想要两个副本，他们必须调用 clone。Copy 改变了移动给定类型的值的语义，这可能会让用户感到惊讶。这与第二个观察结果相关：一个类型很容易停止是 Copy，因为 Copy 类型受到严格限制。一个最初简单的类型很容易最终需要持有一个 String 或其他非 Copy 类型。如果发生这种情况，并且您必须删除 Copy 实现，那将是一个不兼容的变更。相比之下，您很少需要删除 Clone 实现，因此这是一个较轻的承诺。

##### 人性化的特质实现

Rust 不会自动为实现了特质的类型的引用实现特质。换句话说，您通常不能使用 &Bar 调用 fn foo<T: Trait>(t: T)，即使 Bar: Trait。这是因为 Trait 可能包含需要 &mut self 或 self 的方法，显然无法在 &Bar 上调用。然而，对于看到 Trait 只有 &self 方法的用户来说，这种行为可能非常令人惊讶！

- 因此，当您定义一个新的特质时，通常会根据需要为&T where T: Trait，&mut T where T: Trait和Box<T> where T: Trait提供全局实现。根据Trait的方法接收者，您可能只能实现其中一些。标准库中的许多特质具有类似的实现，这样可以减少用户的惊讶。
- 迭代器是另一种情况，您通常会希望在对类型的引用上专门添加特质实现。对于任何可迭代的类型，考虑在适用的情况下为&MyType和&mut MyType实现IntoIterator。这使得使用借用实例的循环也能够正常工作，就像用户期望的那样。

##### 包装类型

Rust在经典意义上没有对象继承。然而，Deref特质及其类似物AsRef提供了一种类似继承的机制。这些特质允许您拥有一个类型为T的值，并通过直接在T类型的值上调用它们来调用类型为U的一些方法，前提是T: Deref<Target = U>。这对用户来说感觉像是魔法，通常非常好用。

- 如果您提供了一个相对透明的包装类型（比如Arc），很有可能您希望实现Deref，以便用户可以通过使用点运算符在内部类型上直接调用方法。如果访问内部类型不需要任何复杂或潜在缓慢的逻辑，您还应该考虑实现AsRef，这样用户就可以轻松地将&WrapperType用作&InnerType。对于大多数包装类型，您还应该在可能的情况下实现`From<InnerType>`和`Into<InnerType>`，以便用户可以轻松地添加或删除您的包装。
- 您可能也遇到过Borrow特质，它与Deref和AsRef非常相似，但实际上是一个稍微不同的东西。具体而言，Borrow专为一个更狭窄的用例而设计：允许调用者提供同一类型的多个本质上相同的变体中的任何一个。它可能本可以被称为Equivalent。例如，对于一个`HashSet<String>`，Borrow允许调用者提供&str或&String。虽然使用AsRef也可以实现相同的效果，但如果没有Borrow的额外要求，即目标类型与实现类型完全相同地实现了Hash、Eq和Ord，那将是不安全的。Borrow还为T、&T和&mut T提供了`Borrow<T>`的全局实现，这使得在trait约束中方便地接受给定类型的拥有或引用值。一般来说，Borrow仅适用于当您的类型与另一个类型本质上等效时，而Deref和AsRef则适用于更广泛地实现您的类型可以“扮演”的任何内容。

  **解引用和固有方法**

- 当存在在T上接受self的方法时，点运算符和Deref的魔力可能会变得令人困惑和意外。例如，给定一个值t: T，不清楚t.frobnicate()是对T还是底层的U进行frobnicate！
- 因此，允许您在某个事先未知的内部类型上透明调用方法的类型应避免使用固有方法。对于Vec来说，它有一个push方法是可以的，即使它解引用为一个slice，因为您知道slice不会很快获得push方法。但是，如果您的类型解引用为一个用户可控制的类型，您添加的任何固有方法也可能存在于该用户可控制的类型上，从而引发问题。在这些情况下，更倾向于使用形式为fn frobnicate(t: T)的静态方法。这样，t.frobnicate()总是调用U::frobnicate，而T::frobnicate(t)可以用于对T本身进行frobnicate。
  
#### 灵活性

您编写的每段代码都包含一个合同，无论是隐式还是显式。合同由一组要求和一组承诺组成。要求是对代码使用方式的限制，而承诺是关于代码使用方式的保证。在设计新接口时，您需要仔细考虑这个合同。一个好的经验法则是避免施加不必要的限制，并只做出您能够遵守的承诺。添加限制或删除承诺通常需要进行重大的语义版本更改，并可能会破坏其他地方的代码。另一方面，放宽限制或提供额外的承诺通常是向后兼容的。

- 在Rust中，限制通常以特质约束和参数类型的形式出现，而承诺通常以特质实现和返回类型的形式出现。例如，比较列表3-1中的三个函数签名。

```rust
fn frobnicate1(s: String) -> String
fn frobnicate2(s: &str) -> Cow<'_, str>
fn frobnicate3(s: impl AsRef<str>) -> impl AsRef<str>

```

列表3-1：具有不同合同的相似函数签名

- 这三个函数签名都接受一个字符串并返回一个字符串，但它们在合同上有很大的区别。
- 第一个函数要求调用者拥有字符串，以String类型的形式，它承诺将返回一个拥有的String。由于合同要求调用者分配并要求我们返回一个拥有的String，我们无法以向后兼容的方式使这个函数无需分配。
- 第二个函数放宽了合同：调用者可以提供任何字符串的引用，因此用户不再需要分配或放弃对String的所有权。它还承诺返回一个std::borrow::Cow，这意味着它可以返回一个字符串引用或一个拥有的String，具体取决于它是否需要拥有该字符串。这里的承诺是函数将始终返回一个Cow，这意味着我们不能将其更改为使用其他优化的字符串表示。调用者还必须明确提供一个&str，因此如果他们有一个预先存在的自己的String，他们必须将其解引用为&str来调用我们的函数。
- 第三个函数放宽了这些限制。它只要求用户传入一个可以产生字符串引用的类型，并且只承诺返回值可以产生字符串引用。
- 这些函数签名中没有一个比其他函数更好。如果您需要在函数中拥有一个字符串的所有权，您可以使用第一个参数类型来避免额外的字符串复制。如果您希望允许调用者利用分配和返回的拥有字符串的情况，具有返回类型为Cow的第二个函数可能是一个不错的选择。然而，我希望您从中得出的结论是，您应该仔细考虑您的接口绑定给您的合同，因为事后更改可能会带来破坏性的影响。
- 在本节的其余部分，我将给出一些常见的接口设计决策示例，以及它们对您的接口合同的影响。

##### 泛型参数

您的接口对用户的一个明显要求是他们必须为您的代码提供的类型。如果您的函数明确接受一个Foo，用户必须拥有并提供一个Foo。没有其他办法。在大多数情况下，使用泛型而不是具体类型会更有回报，允许调用者传递符合函数实际需求的任何类型，而不仅仅是特定类型。将列表3-1中的&str更改为`impl AsRef<str>`就是这种放松要求的示例。通过这种方式放宽要求的一种方法是从参数完全泛型且没有约束开始，然后只需按照编译器错误提示添加所需的约束。

- 然而，如果过度使用这种方法，将使每个函数的每个参数都成为自己的泛型类型，这既难以阅读又难以理解。没有确切的硬性规则来确定何时应该或不应该使给定的参数成为泛型类型，所以请根据自己的判断。一个好的经验法则是，如果您可以想到其他类型的用户可能合理且经常地想要使用而不是您最初使用的具体类型，则将参数设置为泛型。
- 你可能还记得第2章中提到的泛型代码通过单态化为每个使用的类型组合而复制。考虑到这一点，让许多参数成为泛型可能会让你担心过度增大你的二进制文件。在第2章中，我们还讨论了如何使用动态分发来减轻这个问题，通常只会带来可忽略的性能损失，在这里也适用。对于你已经以引用方式接受的参数（记住dyn Trait不是Sized，并且你需要一个宽指针来使用它们），你可以轻松地用使用动态分发的参数替换你的泛型参数。例如，你可以使用`&dyn AsRef<str>`来取代`impl AsRef<str>`。
- 在你开始这样做之前，有几件事情你应该考虑。首先，你代表你的用户做出了这个选择，他们无法选择不使用动态分发。如果你知道你应用动态分发的代码永远不会对性能敏感，那可能没问题。但是如果有一个用户想在他们的高性能应用程序中使用你的库，那么在一个热循环中调用的函数中使用动态分发可能会成为一个绊脚石。其次，在撰写本文时，只有当你有一个简单的特质约束，比如`T: AsRef<str>` 或 `impl AsRef<str>`时，使用动态分发才能工作。对于更复杂的约束，Rust 不知道如何构建动态分发的虚函数表，所以你不能使用 &dyn Hash + Eq 这样的约束。最后，请记住，对于泛型，调用者总是可以通过传递一个特质对象来选择动态分发。反之则不成立：如果你接受一个特质对象，那就是调用者必须提供的。
- 可能会诱人的是从具体类型开始定义接口，然后逐渐将其转换为泛型。这样做是可行的，但请记住，这样的更改不一定向后兼容。为了理解原因，想象一下将函数从`fn foo(v: &Vec<usize>)`更改为fn foo(v: impl AsRef<[usize]>). 虽然每个`&Vec<usize>`都实现了AsRef<[usize]>，但类型推断仍然可能对用户造成问题。考虑一下如果调用者使用foo(&iter.collect())调用foo会发生什么。在原始版本中，编译器可以确定它应该收集到一个Vec中，但现在它只知道它需要收集到某种实现了AsRef<[usize]>的类型中。而且可能有多个这样的类型，所以随着这个更改，调用者的代码将无法编译！

##### 对象安全性

当您定义一个新的trait时，无论该trait是否是对象安全的（请参见第2章“编译和调度”的末尾），都是该trait合同的一个未写明的部分。如果该trait是对象安全的，用户可以使用dyn Trait将实现该trait的不同类型视为单个公共类型。如果不是，编译器将禁止对该trait使用dyn Trait。即使这会稍微降低使用它们的人性化程度（例如，使用impl AsRef而不是&str），您也应该更喜欢使您的trait成为对象安全的，因为对象安全使您的trait能够以新的方式使用。如果您的trait必须具有泛型方法，请考虑将其泛型参数放在trait本身上，或者如果其泛型参数也可以使用动态分发来保持trait的对象安全性。或者，您可以为该方法添加一个where Self: Sized的trait约束，这样只能使用trait的具体实例来调用该方法（而不是通过dyn Trait）。您可以在Iterator和Read traits中看到这种模式的示例，它们是对象安全的，但在具体实例上提供了一些额外的便利方法。

- 对于如何牺牲多少来保持对象安全性的问题，没有一个单一的答案。我的建议是考虑您的trait将如何使用，以及用户是否希望将其用作trait对象。如果您认为用户可能希望同时使用许多不同的trait实例，那么您应该更加努力提供对象安全性，而如果您认为这种用例并不太有意义，那么就不需要那么努力。例如，对于FromIterator trait来说，动态分发并不有用，因为它的一个方法不接受self，所以您根本无法构造一个trait对象。同样地，std::io::Seek作为一个单独的trait对象几乎没有用处，因为您只能对这样的trait对象进行seek操作，而无法进行读取或写入。
**DROP TRAIT OBJECTS**
您可能认为Drop特质作为特质对象也是无用的，因为您只能对其进行丢弃操作。但事实证明，有一些库只想能够丢弃任意类型。例如，一个提供延迟丢弃值的库，例如用于并发垃圾回收或延迟清理，只关心值是否可以被丢弃，而不关心其他任何内容。有趣的是，Drop的故事并不止于此；由于Rust需要能够丢弃特质对象，每个虚函数表都包含了drop方法。实际上，每个dyn Trait也是一个dyn Drop。请记住，对象安全性是您的公共接口的一部分！
如果以与向后兼容的方式修改特质，例如通过添加具有默认实现的方法，但这使得特质不再是对象安全的，您需要提升主要语义版本号。

##### 借用 vs 拥有

对于您在Rust中定义的几乎每个函数、特质和类型，您都必须决定它是拥有数据还是仅持有对数据的引用。您所做的决定将对接口的人性化和性能产生深远的影响。
幸运的是，这些决策往往是自然而然的。
如果您编写的代码需要拥有数据的所有权，例如调用需要self的方法或将数据移动到另一个线程中，它必须存储拥有的数据。当您的代码必须拥有数据时，通常也应该要求调用者提供拥有的数据，而不是通过引用并进行克隆。这样做可以让调用者控制分配，并明确了使用相关接口的成本。

- 另一方面，如果您的代码不需要拥有数据，应该使用引用进行操作。一个常见的例外是对于像i32、bool或f64这样的小类型，直接存储和复制与通过引用存储一样廉价。但是要小心假设这对所有Copy类型都成立；[u8; 8192]是Copy的，但是在各个地方存储和复制它会很昂贵。
- 当然，在现实世界中，事情往往没有那么清晰明了。
有时候，您事先不知道您的代码是否需要拥有数据。例如，String::from_utf8_lossy只有在传递给它的字节序列包含无效的UTF-8序列时才需要拥有该字节序列的所有权。在这种情况下，Cow类型是您的朋友：它允许您在数据允许的情况下操作引用，并在必要时生成拥有的值。
- 有时候，引用的生命周期会使接口变得非常复杂，以至于使用起来非常麻烦。如果您的用户在使用您的接口时遇到编译问题，那就意味着您可能需要（即使是不必要地）获取某些数据的所有权。如果这样做，首先从便宜的可克隆数据或不涉及性能敏感的数据开始，然后再决定是否需要堆分配可能是一个巨大的字节块。

##### 可失败和阻塞的析构函数

以I/O为中心的类型在被丢弃时通常需要执行清理操作。这可能包括将写入刷新到磁盘、关闭文件或优雅地终止与远程主机的连接。执行这些清理操作的自然位置是类型的Drop实现。不幸的是，一旦一个值被丢弃，我们就没有办法向用户传达错误，除非通过panic。在异步代码中也会出现类似的问题，我们希望在有待处理的工作时完成。当调用drop时，执行器可能正在关闭，我们无法再做更多的工作。我们可以尝试启动另一个执行器，但这会带来一系列问题，比如在异步代码中阻塞，正如我们将在第8章中看到的那样。

- 对于这些问题，没有完美的解决方案，无论我们做什么，一些应用程序都不可避免地会回退到我们的Drop实现。因此，我们需要通过Drop提供尽力而为的清理。如果清理出现错误，至少我们尝试过-我们会忽略错误并继续执行。如果执行器仍然可用，我们可能会生成一个未来任务来进行清理，但如果它从未运行，我们已经尽力而为了。
- 然而，我们应该为希望不留下任何悬空线程的用户提供更好的选择。我们可以通过提供显式的析构函数来实现这一点。通常，这采用一个接受 self 所有权并公开与销毁相关的任何错误（使用 -> Result<_,_>) 或异步性（使用 async fn）的方法的形式。然后，细心的用户可以使用该方法来优雅地关闭任何相关资源。
**注意** 确保在文档中突出显示显式析构函数！
- 像往常一样，存在权衡。一旦添加了显式析构函数，您将遇到两个问题。首先，由于您的类型实现了Drop，您无法在析构函数中移动该类型的任何字段。这是因为在显式析构函数运行之后，仍然会调用Drop::drop，并且它需要&mut self，这要求self的任何部分都没有被移动。其次，drop接受的是&mut self，而不是self，因此您的Drop实现不能简单地调用显式析构函数并忽略其结果（因为它不拥有self）。有几种方法可以解决这些问题，但没有一种是完美的。
- 第一种方法是将顶层类型作为一个新类型包装器，包装一个Option，而Option又持有一些内部类型，该内部类型持有所有字段。然后，在两个析构函数中都可以使用Option::take，并且仅在内部类型尚未被取走时调用内部类型的显式析构函数。由于内部类型没有实现Drop，因此您可以拥有所有字段的所有权。这种方法的缺点是，您希望在顶层类型上提供的所有方法现在都必须包含代码，以通过Option（您知道始终是Some，因为尚未调用drop）访问内部类型的字段。

- 第二种解决方法是使每个字段可取。您可以通过将其替换为None（这就是Option::take的作用）来“取走”一个Option，但您也可以对许多其他类型执行此操作。例如，您可以通过将它们替换为廉价构造的默认值（std::mem::take在这里很有用）来取走一个Vec或HashMap。如果您的类型具有合理的“空”值，这种方法非常有效，但如果您必须将几乎每个字段都包装在Option中，然后修改对这些字段的每次访问以匹配unwrap，这将变得乏味。
- 第三种选择是将数据保存在ManuallyDrop类型中，该类型解引用为内部类型，因此不需要解包。您还可以在析构时使用ManuallyDrop::take来获取所有权。这种方法的主要缺点是ManuallyDrop::take是不安全的。没有安全机制来确保您在调用take之后不会尝试使用ManuallyDrop内部的值，或者不会多次调用take。如果这样做，您的程序将悄无声息地表现出未定义的行为，并发生糟糕的事情。
- 最终，您应该选择最适合您应用程序的方法。我倾向于选择第二个选项，并只在您发现自己陷入Option的困境时切换到其他选项。如果代码足够简单，您可以轻松检查代码的安全性，并且对自己的能力有信心，那么ManuallyDrop解决方案是非常好的选择。

#### 显而易见的

虽然一些用户可能熟悉支撑接口的实现的某些方面，但他们不太可能了解其所有规则和限制。他们不会知道在调用bar之后调用foo是不允许的，或者只有在月亮处于47度角且过去18秒内没有人打喷嚏时才能安全地调用不安全的方法baz。只有当接口明确表明发生了奇怪的事情时，他们才会查阅文档或仔细阅读类型签名。因此，对于用户来说，理解您的接口尽可能容易，尽可能难以错误使用是至关重要的。为此，您可以利用文档和类型系统这两种主要技术。让我们依次看看每种技术。
**注意** 您还可以利用命名来向用户暗示接口背后可能有更多内容。如果用户看到一个名为dangerous的方法，他们很可能会阅读其文档。

##### 文档

使接口透明的第一步是编写良好的文档。
我可以写一本专门讲如何编写文档的书，
但让我们在这里专注于与Rust相关的建议。

- 首先，清楚地记录您的代码可能会做一些意外的事情的情况，
或者它依赖于用户做一些超出类型签名规定的事情。Panic是这两种情况的一个很好的例子：
如果您的代码可能会panic，请记录这一事实，以及它可能在哪些情况下会panic。
同样，如果您的代码可能返回错误，请记录它返回错误的情况。
对于不安全的函数，请记录调用者必须保证的条件，以使调用安全。
- 其次，在crate和模块级别包含端到端的使用示例。这些示例比特定类型或方法的示例更重要，
因为它们让用户了解整个接口的结构。通过对接口的高层次理解，
开发人员可能很快意识到特定方法和类型的作用以及它们应该在哪里使用。
端到端的示例还为用户提供了定制使用的起点，他们可以复制粘贴示例，然后根据自己的需求进行修改。
这种“边做边学”的方式往往比让他们从组件中拼凑出来更有效。
**注意** 非常特定于方法的示例，显示len方法确实返回长度的示例，
不太可能告诉用户有关您的代码的新信息。

- 第三，组织您的文档。将所有类型、特质和函数放在单个顶层模块中，会让用户很难找到开始的地方。利用模块将语义相关的项分组在一起。然后，使用内部文档链接将这些项相互链接起来。如果类型A的文档中提到了特质B，那么应该在那里链接到该特质。如果让用户能够轻松探索您的接口，他们就不太可能错过重要的关联或依赖关系。还要考虑使用#[doc(hidden)]标记那些不打算公开但出于遗留原因仍然需要的接口部分，以避免在文档中混乱。
- 最后，在可能的情况下丰富您的文档。链接到解释概念、数据结构、算法或其他方面的外部资源，这些资源可能在其他地方有很好的解释。如果有相关的RFC、博客文章和白皮书，那就很好。使用#[doc(cfg(..))]来突出显示仅在某些配置下可用的项，这样用户就能快速意识到为什么文档中列出的某个方法不可用。使用#[doc(alias = "...")]使类型和方法可以通过其他名称进行搜索和发现。在顶层文档中，指导用户常用的模块、特性、类型、特质和方法。

##### 类型系统泛型

类型系统是确保接口明显、自说明和抗误用的优秀工具。您有几种技术可以使接口很难被误用，从而更有可能被正确使用。
其中之一是语义类型化，您可以添加类型来表示值的含义，而不仅仅是其原始类型。这里的经典示例是布尔值：如果您的函数接受三个布尔参数，很有可能某个用户会搞错值的顺序，并在出现严重问题后才意识到。然而，如果它接受三个不同的两变量枚举类型的参数，用户在没有编译器警告的情况下无法搞错顺序：如果他们试图将DryRun::Yes传递给overwrite参数，那将不起作用，将Overwrite::No传递给dry_run参数也是如此。您还可以将语义类型化应用于布尔值以外的情况。例如，围绕数值类型的新类型可以为包含的值提供单位，或者可以将原始指针参数限制为仅限于由另一个方法返回的指针。

- 一个密切相关的技术是使用零大小的类型来表示关于类型实例的特定事实。例如，考虑一个名为Rocket的类型，它表示真实火箭的状态。Rocket上的一些操作（方法）应该在任何状态下都可用，但有些操作只在特定情况下有意义。例如，如果火箭已经发射，就不可能再次发射。同样，如果火箭尚未发射，可能不应该能够分离燃料箱。我们可以将它们建模为枚举变体，但这样所有的方法都会在每个阶段都可用，并且我们需要引入可能的panic。
- 相反，如图3-2所示，我们可以在Rocket上引入一个泛型参数Stage，并使用它来限制在何时可用哪些方法。

```rust
1 struct Grounded;
struct Launched;
// and so on
struct Rocket<Stage = Grounded> {
2 stage: std::marker::PhantomData<Stage>,
}
3 impl Default for Rocket<Grounded> {}
impl Rocket<Grounded> {
pub fn launch(self) -> Rocket<Launched> { }
}
4 impl Rocket<Launched> {
pub fn accelerate(&mut self) { }
pub fn decelerate(&mut self) { }
}
5 impl<Stage> Rocket<Stage> {
pub fn color(&self) -> Color { }
pub fn weight(&self) -> Kilograms { }
}
```

清单3-2：使用标记类型限制实现

- 我们引入了单元类型来表示火箭的每个阶段1。实际上，我们不需要存储阶段-只需要存储它提供的元信息-因此我们使用PhantomData 2将其存储在后面，以确保它在编译时被消除。然后，我们只在Rocket持有特定类型参数时编写实现块。您只能在地面上（目前）构建火箭，并且只能从地面上发射它3。只有在火箭发射后，您才能控制其速度4。无论火箭处于什么状态，您总是可以执行某些操作，这些操作我们放在一个通用的实现块中5。您会注意到，以这种方式设计接口，用户根本无法在错误的时间调用方法-我们已经将使用规则编码到类型本身中，并使非法状态无法表示。

- 这个概念也适用于许多其他领域；如果您的函数忽略一个指针参数，除非给定的布尔参数为true，最好将这两个参数合并在一起。使用一个枚举类型，其中一个变体表示false（没有指针），另一个变体表示true并持有一个指针，既不会让调用者也不会让实现者误解两者之间的关系。这是一个强大的想法，我强烈鼓励您使用它。
- 在使接口明显的过程中，另一个小而有用的工具是#[must_use]注解。将其添加到任何类型、特质或函数上，如果用户的代码接收到该类型或特质的元素，或者调用该函数，并且没有明确处理它，编译器将发出警告。您可能已经在Result的上下文中见过这个注解：如果一个函数返回一个Result，而您没有在某个地方分配其返回值，您将得到一个编译器警告。但要小心不要过度使用这个注解-只有在用户在不使用返回值时很可能犯错误时才添加它。

#### 受限制的

随着时间的推移，一些用户将依赖于您接口的每个属性，无论是错误还是功能。这对于公开可用的库尤其如此，因为您无法控制用户。因此，在进行用户可见的更改之前，您应该仔细考虑。无论是添加新类型、字段、方法还是特质实现，还是更改现有的内容，您都希望确保更改不会破坏现有用户的代码，并且您计划保留该更改一段时间。频繁的不向后兼容的更改（在语义版本控制中的主要版本增加）肯定会引起用户的不满。

- 许多不向后兼容的更改是显而易见的，比如重命名公共类型或删除公共方法，但有些更改更加微妙，并且与Rust的工作方式紧密相关。在这里，我们将介绍一些棘手的微妙更改以及如何为其进行规划。您将看到，您需要在某些情况下权衡这些更改，以及您希望接口有多灵活-有时，必须做出一些让步。

##### 类型修改

删除或重命名公共类型几乎肯定会破坏某些用户的代码。为了应对这个问题，您应该尽可能利用Rust的可见性修饰符，如pub(crate)和pub(in path)。您拥有的公共类型越少，您就越有自由在以后更改事物而不破坏现有代码。
用户代码可以以更多方式依赖于您的类型，而不仅仅是名称。考虑清单3-3中的公共类型以及给定的代码使用。

```rust
// in your interface
pub struct Unit;
// in user code
let u = lib::Unit;
```

清单3-3：一个看似无害的公共类型
现在考虑一下如果给Unit添加一个私有字段会发生什么。即使您添加的字段是私有的，这个更改仍然会破坏用户的代码，因为他们依赖的构造函数已经消失了。类似地，考虑清单3-4中的代码和用法。

```rust
// in your interface
pub struct Unit { pub field: bool };
// in user code
fn is_true(u: lib::Unit) -> bool {
matches!(u, Unit { field: true })
}
```

清单3-4：访问单个公共字段的用户代码
同样，如果给Unit添加一个私有字段，这将破坏用户的代码，因为Rust的完整模式匹配检查逻辑能够看到用户无法看到的接口的部分。它识别出还有更多的字段，即使用户代码无法访问它们，并拒绝用户的模式作为不完整的。如果我们将元组结构体转换为具有命名字段的常规结构体，也会出现类似的问题：即使字段本身完全相同，任何旧的模式对于新的类型定义将不再有效。
Rust提供了#[non_exhaustive]属性来帮助缓解这些问题。您可以将其添加到任何类型定义中，编译器将禁止在该类型上使用隐式构造函数（例如lib::Unit { field1: true }）和非穷尽模式匹配（即没有尾随, ..的模式）。如果您怀疑将来可能修改某个特定类型，这是一个很好的属性添加。但是，它会限制用户代码，例如剥夺用户依赖穷尽模式匹配的能力，因此如果您认为某个类型可能保持稳定，请避免添加该属性。

##### 特质实现

正如您在第2章中所记得的，Rust的一致性规则禁止为给定类型实现给定特质的多个实现。由于我们不知道下游代码可能添加了什么实现，因此为现有特质添加一个全局实现通常是一个破坏性的更改。对于为现有类型实现外部特质或为外部类型实现现有特质，情况也是如此-在这两种情况下，外部特质或类型的所有者可能同时添加一个冲突的实现，因此这必须是一个破坏性的更改。
删除特质实现是一个破坏性的更改，但为新类型实现特质从来不会成为问题，因为没有一个crate可以有与该类型冲突的实现。
也许令人意外的是，您还需要小心为现有类型实现任何特质。为了理解原因，请考虑清单3-5中的代码。

```rust
// crate1 1.0
pub struct Unit;
put trait Foo1 { fn foo(&self) }
// note that Foo1 is not implemented for Unit
// crate2; depends on crate1 1.0
use crate1::{Unit, Foo1};
trait Foo2 { fn foo(&self) }
impl Foo2 for Unit { .. }
fn main() {
Unit.foo();
}
```

清单3-5：为现有类型实现特质可能会引起问题。

- 如果您在crate1中添加了对Unit的impl Foo1的实现，而没有将其标记为破坏性更改，那么下游代码将突然停止编译，因为对foo的调用现在变得模糊不清。即使是对新的公共特质的实现，如果下游crate使用通配符导入（use crate1::*），这种情况也可能发生。如果您提供了一个预导模块，并指示用户使用通配符导入，那么您尤其需要牢记这一点。
- 对现有特质的大多数更改也是破坏性的更改，比如改变方法签名或添加新方法。改变方法签名会破坏所有实现，并且可能会破坏特质的许多使用方式，而添加新方法只是“仅仅”破坏所有实现。但是，添加具有默认实现的新方法是可以的，因为现有的实现将继续适用。
- 我在这里使用“通常”和“大多数”这样的词，因为作为接口作者，我们有一个工具可以帮助我们规避一些规则：封闭特质。封闭特质是一种只能被其他 crate 使用而不能被实现的特质。这立即使得一些破坏性更改变得非破坏性。例如，您可以向封闭特质添加一个新方法，因为您知道在当前 crate 之外没有其他实现需要考虑。同样地，您可以为新的外部类型实现封闭特质，因为您知道定义该类型的外部 crate 不可能添加冲突的实现。
- 封闭特质最常用于派生特质-为实现特定其他特质的类型提供全局实现的特质。只有在不合理的情况下，才应该封闭一个特质，使外部 crate 无法实现它；这严重限制了特质的实用性，因为下游 crate 将无法为其自己的类型实现该特质。您还可以使用封闭特质来限制可以用作类型参数的类型，例如将清单3-2中的 Rocket 示例中的 Stage 类型限制为仅限 Grounded 和 Launched 类型。清单3-6展示了如何封闭一个特质，以及如何在定义 crate 中为其添加实现。

```rust
pub trait CanUseCannotImplement: sealed::Sealed 1 { .. }
mod sealed {
pub trait Sealed {}
2 impl<T> Sealed for T where T: TraitBounds {}
}
impl<T> CanUseCannotImplement for T where T: TraitBounds {}
```

清单3-6：如何封闭一个特质并为其添加实现
诀窍是将一个私有的、空的特质作为要封闭的特质的超特质添加进去1。由于超特质在一个私有模块中，其他 crate 无法访问它，因此也无法实现它。封闭特质要求底层类型实现 Sealed，因此只有我们明确允许的类型2才能最终实现该特质。
**注意**：如果您以这种方式封闭一个特质，请确保记录下这个事实，以免用户尝试自己实现该特质而感到沮丧！

##### 隐藏的约定

有时，您对代码的某一部分所做的更改会以微妙的方式影响接口中的其他部分的约定。这种情况主要发生在重新导出和自动特质中。
**重新导出**

如果您的接口的任何部分暴露了外部类型，那么对其中一个外部类型的任何更改也是对您的接口的更改。例如，考虑一下如果您将依赖项的新主要版本作为迭代器类型暴露在您的接口中会发生什么。依赖于您接口的用户可能还直接依赖于该依赖项，并期望您接口提供的类型与该依赖项中同名的类型相同。但是，如果您更改了依赖项的主要版本，即使类型的名称相同，这也不再成立。

清单3-7展示了一个例子。

```rust
// your crate: bestiter
pub fn iter<T>() -> itercrate::Empty<T> { .. }
// their crate
struct EmptyIterator { it: itercrate::Empty<()> }

EmptyIterator { it: bestiter::iter() }
```

清单3-7：重新导出使外部 crate 成为接口契约的一部分。


- If your crate moves from itercrate 1.0 to itercrate 2.0 but otherwise does
not change, the code in this listing will no longer compile. Even though no
types have changed, the compiler believes (correctly) that itercrate1.0::Empty
and itercrate2.0::Empty are different types. Therefore, you cannot assign the
latter to the former, making this a breaking change in your interface.
- To mitigate issues like this, it’s often best to wrap foreign types using
the newtype pattern, and then expose only the parts of the foreign type
that you think are useful. In many cases, you can avoid the newtype wrapper
altogether by using impl Trait to provide only the very minimal contract
to the caller. By promising less, you make fewer changes breaking.
**THE SEMVER TRICK**
- - The itercrate example may have rubbed you the wrong way. If the Empty type
did not change, then why does the compiler not allow anything that uses it to
keep working, regardless of whether the code is using version 1.0 or 2.0 of it?
The answer is . . . complicated. It boils down to the fact that the Rust compiler
does not assume that just because two types have the same fields, they are the
same. To take a simple example of this, imagine that itercrate 2.0 added a
`#[derive(Copy)]` for Empty. Now, the type suddenly has different move semantics
depending on whether you are using 1.0 or 2.0! And code written with one
in mind won’t work with the other.
- - This problem tends to crop up in large, widely used libraries, where over
time, breaking changes are likely to have to happen somewhere in the crate.
Unfortunately, semantic versioning happens at the crate level, not the type level,
so a breaking change anywhere is a breaking change everywhere.
- - But all is not lost. A few years ago, David Tolnay (the author of serde,
among a vast number of other Rust contributions) came up with a neat trick to
handle exactly this kind of situation. He called it “the semver trick.” The idea
is simple: if some type T stays the same across a breaking change (from 1.0
to 2.0, say), then after releasing 2.0, you can release a new 1.0 minor version
that depends on 2.0 and replaces T with a re-export of T from 2.0.
- - By doing this, you’re ensuring that there is in fact only a single type T
across both major versions. This, in turn, means that any crate that depends on
1.0 will be able to use a T from 2.0, and vice versa. And because this happens
only for types you explicitly opt into with this trick, changes that were in fact
breaking will continue to be.
##### Auto-Traits
Rust has a handful of traits that are automatically implemented for every
type depending on what that type contains. The most relevant of these for
this discussion are Send and Sync, though the Unpin, Sized, and UnwindSafe
traits have similar issues. By their very nature, these add a hidden promise
made by nearly every type in your interface. These traits even propagate
through otherwise type-erased types like impl Trait.

- Implementations for these traits are (generally) automatically added by
the compiler, but that also means that they are not automatically added if
they no longer apply. So, if you have a public type A that contains a private
type B, and you change B so that it is no longer Send, then A is now also not
Send. That is a breaking change!
- These changes can be hard to keep track of and are often not discovered
until a user of your interface complains that their code no longer
works. To catch these cases before they happen, it’s good practice to include
some simple tests in your test suite that check that all your types implement
these traits the way you expect. Listing 3-8 gives an example of what such a
test might look like.

```rust
fn is_normal<T: Sized + Send + Sync + Unpin>() {}
#[test]
fn normal_types() {
is_normal::<MyType>();
}
```
Listing 3-8: Testing that a type implements a set of traits
- Notice that this test does not run any code, but simply tests that the
code compiles. If MyType no longer implements Sync, the test code will not
compile, and you will know that the change you just made broke the autotrait
implementation.
**HIDING ITEMS FROM DOCUMENTATION**
The #[doc(hidden)] attribute lets you hide a public item from your documentation
without making it inaccessible to code that happens to know it is there. This
is often used to expose methods and types that are needed by macros, but not
by user code. How such hidden items interact with your interface contract is a
matter of some debate. In general, items marked as #[doc(hidden)] are only
considered part of your contract insofar as their public effects; for example, if
user code may end up containing a hidden type, then whether that type is Send
or not is part of the contract, whereas its name is not. Hidden inherent methods
and hidden trait methods on sealed traits are not generally part of your interface
contract, though you should make sure to state this clearly in the documentation
for those methods. And yes, hidden items should still be documented!


56 Chapter 3
Summary
In this chapter we’ve explored the many facets of designing a Rust interface,
whether it’s intended for external use or just as an abstraction boundary
between the different modules within your crate. We covered a lot of
specific pitfalls and tricks, but ultimately, the high-level principles are what
should guide your thinking: your interfaces should be unsurprising, flexible,
obvious, and constrained. In the next chapter, we will dig into how to
represent and handle errors in Rust code.






### 4.ERROR HANDLING

For all but the simplest programs, you will
have methods that can fail. In this chapter,
we’ll look at different ways to represent,
handle, and propagate those failures and
the advantages and drawbacks of each. We’ll start by
exploring different ways to represent errors, including
enumeration and erasure, and then examine some
special error cases that require a different representation
technique. Next, we’ll look at various ways of handling
errors and the future of error handling.
It’s worth noting that best practices for error handling in Rust are still
an active topic of conversation, and at the time of writing, the ecosystem
has not yet settled on a single, unified approach. This chapter will therefore
focus on the underlying principles and techniques rather than recommending
specific crates or patterns.

#### Representing Errors

When you write code that can fail, the most important question to ask yourself
is how your users will interact with any errors returned. Will users need
to know exactly which error happened and the minutiae about what went
wrong, or will they simply log that an error occurred and move on as best
they can? To understand this, we have to look at whether the nature of the
error is likely to affect what the caller does upon receiving it. This in turn
will dictate how we represent different errors.
You have two main options for representing errors: enumeration and erasure.
That is, you can either have your error type enumerate the possible error
conditions so that the caller can distinguish them, or you can just provide the
caller with a single, opaque error. Let’s discuss these two options in turn.
Enumeration
For our example, we’ll use a library function that copies bytes from some
input stream into some output stream, much like std::io::copy. The user
provides you with two streams, one to read from and one to write to, and
you copy the bytes from one to the other. During this process, it’s entirely
possible for either stream to fail, at which point the copy has to stop and
return an error to the user. Here, the user will likely want to know whether
it was the input stream or the output stream that failed. For example, in a
web server, if an error occurs on the input stream while streaming a file to a
client, it might be because a disk was ejected, whereas if the output stream
errors, maybe the client just disconnected. The latter may be an error the
server should ignore, since copies to new connections can still complete,
whereas the former may require that the whole server be shut down!
This is a case where we want to enumerate the errors. The user needs
to be able to distinguish between the different error cases so that they can
respond appropriately, so we use an enum named CopyError, with each variant
representing a separate underlying cause for the error, like in Listing 4-1.
pub enum CopyError {
In(std::io::Error),
Out(std::io::Error),
}
Listing 4-1: An enumerated error type
Each variant also includes the error that was encountered to provide
the caller with as much information about went wrong as possible.
When making your own error type, you need to take a number of steps to
make the error type play nicely with the rest of the Rust ecosystem. First, your
error type should implement the std::error::Error trait, which provides callers
with common methods for introspecting error types. The main method of
interest is Error::source, which provides a mechanism to find the underlying
cause of an error. This is most commonly used to print a backtrace that displays
a trace all the way back to the error’s root cause. For our CopyError type,
Error Handling 59
the implementation of source is straightforward: we match on self and extract
and return the inner std::io::Error.
Second, your type should implement both Display and Debug so that callers
can meaningfully print your error. This is required if you implement the
Error trait. In general, your implementation of Display should give a one-line
description of what went wrong that can easily be folded into other error messages.
The display format should be lowercase and without trailing punctuation
so that it fits nicely into other, larger error reports. Debug should provide
a more descriptive error including auxiliary information that may be useful
in tracking down the cause of the error, such as port numbers, request identifiers,
filepaths, and the like, which #[derive(Debug)] is usually sufficient for.
NOTE In older Rust code, you may see references to the Error::description method, but this
has been deprecated in favor of Display.
Third, your type should, if possible, implement both Send and Sync so
that users are able to share the error across thread boundaries. If your error
type is not thread-safe, you will find that it’s almost impossible to use your
crate in a multithreaded context. Error types that implement Send and Sync
are also much easier to use with the very common std::io::Error type, which
is able to wrap errors that implement Error, Send, and Sync. Of course, not all
error types can reasonably be Send and Sync, such as if they’re tied to particular
thread-local resources, and that’s okay. You’re probably not sending those
errors across thread boundaries either. However, it’s something to be aware
of before you go placing Rc<String> and RefCell<bool> types in your errors.
Finally, where possible, your error type should be 'static. The most
immediate benefit of this is that it allows the caller to more easily propagate
your error up the call stack without running into lifetime issues. It also
enables your error type to be used more easily with type-erased error types,
as we’ll see shortly.
Opaque Errors
Now let’s consider a different example: an image decoding library. You
give the library a bunch of bytes to decode, and it gives you access to various
image manipulation methods. If the decoding fails, the user needs to
be able to figure out how to resolve the issue, and so must understand the
cause. But is it important whether the cause is the size field in the image
header being invalid, or the compression algorithm failing to decompress
a block? Probably not—the application can’t meaningfully recover from
either situation, even if it knows the exact cause. In cases like this, you as
the library author may instead want to provide a single, opaque error type.
This also makes your library a little nicer to use, because there is only one
error type in use everywhere. This error type should implement Send, Debug,
Display, and Error (including the source method where appropriate), but
beyond that, the caller doesn’t need to know anything more. You might
internally represent more fine-grained error states, but there is no need
to expose those to the users of the library. Doing so would only serve to
unnecessarily increase the size and complexity of your API.


60 Chapter 4
Exactly what your opaque error type should be is mostly up to you. It
could just be a type with all private fields that exposes only limited methods
for displaying and introspecting the error, or it could be a severely
type-erased error type like Box<dyn Error + Send + Sync + 'static>, which
reveals nothing more than the fact that it is an error and does not generally
let your users introspect at all. Deciding how opaque to make your
error types is mostly a matter of whether there is anything interesting
about the error beyond its description. With Box<dyn Error>, you leave your
users with little option but to bubble up your error. That might be fine if
it truly has no information of value to present to the user—for example, if
it’s just a dynamic error message or is one of a large number of unrelated
errors from deeper inside your program. But if the error has some interesting
facets to it, such as a line number or a status code, you may want to
expose that through a concrete but opaque type instead.
NOTE In general, the community consensus is that errors should be rare and therefore should
not add much cost to the “happy path.” For that reason, errors are often placed behind
a pointer type, such as a Box or Arc. This way, they’re unlikely to add much to the size
of the overall Result type they’re contained within.
One benefit of using type-erased errors is that it allows you to easily
combine errors from different sources without having to introduce additional
error types. That is, type-erased errors often compose nicely, and allow
you to express an open-ended set of errors. If you write a function whose
return type is Box<dyn Error + ...>, then you can use ? across different error
types inside that function, on all sorts of different errors, and they will all
be turned into that one common error type.
The 'static bound on Box<dyn Error + Send + Sync + 'static> is worth
spending a bit more time on in the context of erasure. I mentioned in the
previous section that it’s useful for letting the caller propagate the error
without worrying about the lifetime bounds of the method that failed, but
it serves an even bigger purpose: access to downcasting. Downcasting is the
process of taking an item of one type and casting it to a more specific type.
This is one of the few cases where Rust gives you access to type information at
runtime; it’s a limited case of the more general type reflection that dynamic
languages often provide. In the context of errors, downcasting allows a user to
turn a dyn Error into a concrete underlying error type when that dyn Error was
originally of that type. For example, the user may want to take a particular
action if the error they received was a std::io::Error of kind std::io::ErrorKind
::WouldBlock, but they would not take that same action in any other case. If the
user gets a dyn Error, they can use Error::downcast_ref to try to downcast the
error into a std::io::Error. The downcast_ref method returns an Option, which
tells the user whether or not the downcast succeeded. And here is the key
observation: downcast_ref works only if the argument is 'static. If we return an
opaque Error that’s not 'static, we take away the user’s ability to do this kind of
error introspection should they wish.
There’s some disagreement in the ecosystem about whether a library’s
type-erased errors (or more generally, its type-erased types) are part of

Error Handling 61
its public and stable API. That is, if the method foo in your library returns
lib::MyError as a Box<dyn Error>, would changing foo to return a different
error type be a breaking change? The type signature hasn’t changed, but
users may have written code that assumes that they can use downcast to
turn that error back into lib::MyError. My opinion on this matter is that
you chose to return Box<dyn Error> (and not lib::MyError) for a reason, and
unless explicitly documented, that does not guarantee anything in particular
about downcasting.
NOTE While Box<dyn Error + ...> is an attractive type-erased error type, it counterintuitively
does not itself implement Error. Therefore, consider adding your own
BoxError type for type erasure in libraries that does implement Error.
You may wonder how Error::downcast_ref can be safe. That is, how does
it know whether a provided dyn Error argument is indeed of the given type
T? The standard library even has a trait called Any that is implemented for
any type, and which implements downcast_ref for dyn Any—how can that
be okay? The answer lies in the compiler-supported type std::any::TypeId,
which allows you to get a unique identifier for any type. The Error trait has
a hidden provided method called type_id, whose default implementation is
to return TypeId::of::<Self>(). Similarly, Any has a blanket implementation
of impl Any for T, and in that implementation, its type_id returns the same.
In the context of these impl blocks, the concrete type of Self is known, so
this type_id is the type identifier of the real type. That provides all the information
downcast_ref needs. downcast_ref calls self.type_id, which forwards
through the vtable for dynamically sized types (see Chapter 2) to the implementation
for the underlying type and compares that to the type identifier
of the provided downcast type. If they match, then the type behind the dyn
Error or dyn Any really is T, and it is safe to cast from a reference to one to a
reference to the other.
Special Error Cases
Some functions are fallible but cannot return any meaningful error if they
fail. Conceptually, these functions have a return type of Result<T, ()>. In
some codebases, you may see this represented as Option<T> instead. While
both are legitimate choices for the return type for such a function, they
convey different semantic meanings, and you should usually avoid “simplifying”
a Result<T, ()> to Option<T>. An Err(()) indicates that an operation failed
and should be retried, reported, or otherwise handled exceptionally. None,
on the other hand, conveys only that the function has nothing to return;
it is usually not considered an exceptional case or something that should
be handled. You can see this in the #[must_use] annotation on the Result
type—when you get a Result, the language expects that it is important to
handle both cases, whereas with an Option, neither case actually needs to
be handled.

62 Chapter 4
NOTE You should also keep in mind that () does not implement the Error trait. This means
that it cannot be type-erased into Box<dyn Error> and can be a bit of a pain to use
with ?. For this reason, it is often better to define your own unit struct type, implement
Error for it, and use that as the error instead of () in these cases.
Some functions, like those that start a continuously running server
loop, only ever return errors; unless an error occurs, they run forever. Other
functions never error but need to return a Result nonetheless, for example,
to match a trait signature. For functions like these, Rust provides the never
type, written with the ! syntax. The never type represents a value that can
never be generated. You cannot construct an instance of this type yourself—
the only way to make one is by entering an infinite loop or panicking, or
through a handful of other special operations that the compiler knows never
return. With Result, when you have an Ok or Err that you know will never
be used, you can set it to the ! type. If you write a function that returns
Result<T, !>, you will be unable to ever return Err, since the only way to do
so is to enter code that will never return. Because the compiler knows that
any variant with a ! will never be produced, it can also optimize your code
with that in mind, such as by not generating the panic code for an unwrap on
Result<T, !>. And when you pattern match, the compiler knows that any variant
that contains a ! does not even need to be listed. Pretty neat!
One last curious error case is the error type std::thread::Result. Here’s
its definition:
type Result<T> = Result<T, Box<dyn Any + Send + 'static>>;
The error type is type-erased, but it’s not erased into a dyn Error as
we’ve seen so far. Instead, it is a dyn Any, which guarantees only that the
error is some type, and nothing more . . . which is not much of a guarantee
at all. The reason for this curious-looking error type is that the error variant
of std::thread::Result is produced only in response to a panic; specifically,
if you try to join a thread that has panicked. In that case, it’s not clear
that there’s much the joining thread can do other than either ignore the
error or panic itself using unwrap. In essence, the error type is “a panic” and
the value is “whatever argument was passed to panic!,” which can truly be
any type (even though it’s usually a formatted string).
Propagating Errors
Rust’s ? operator acts as a shorthand for unwrap or return early, for working
easily with errors. But it also has a few other tricks up its sleeve that are worth
knowing about. First, ? performs type conversion through the From trait. In
a function that returns Result<T, E>, you can use ? on any Result<T, X> where
E: From<X>. This is the feature that makes error erasure through Box<dyn Error>
so appealing; you can just use ? everywhere and not worry about the particular
error type, and it will usually “ just work.”

Error Handling 63
**FROM AND INTO**
The standard library has many conversion traits, but two of the core ones are
From and Into. It might strike you as odd to have two: if we have From, why
do we need Into, and vice versa? There are a couple of reasons, but let’s start
with the historical one: it wouldn’t have been possible to have just one in the
early days of Rust due to the coherence rules discussed in Chapter 2. Or, more
specifically, what the coherence rules used to be.
Suppose you want to implement two-way conversion between some
local type you have defined in your crate and some type in the standard
library. You can write impl<T> From<Vec<T>> for MyType<T> and impl<T>
Into<Vec<T>> for MyType<T> easily enough, but if you only had From or Into,
you would have to write impl<T> From<MyType<T>> for Vec<T> or impl<T>
Into<MyType<T>> for Vec<T>. However, the compiler used to reject those
implementations! Only since Rust 1.41.0, when the exception for covered types
was added to the coherence rules, are they legal. Before that change, it was
necessary to have both traits. And since much Rust code was written before
Rust 1.41.0, neither trait can be removed now.
Beyond that historical fact, however, there are also good ergonomic reasons
to have both of these traits, even if we could start from scratch today. It is
often significantly easier to use one or the other in different situations. For example,
if you’re writing a method that takes a type that can be turned into a Foo,
would you rather write fn(impl Into<Foo>) or fn<T>(T) where Foo: From<T>?
And conversely, to turn a string into a syntax identifier, would you rather write
Ident::from("foo") or <_ as Into<Ident>>::into("foo")? Both of these traits
have their uses, and we’re better off having them both.
Given that we do have both, you may wonder which you should use in
your code today. The answer, it turns out, is pretty simple: implement From, and
use Into in bounds. The reason is that Into has a blanket implementation for
any T that implements From, so regardless of whether a type explicitly implements
From or Into, it implements Into!
Of course, as simple things frequently go, the story doesn’t quite end there.
Since the compiler often has to “go through” the blanket implementation when
Into is used as a bound, the reasoning for whether a type implements Into
is more complicated than whether it implements From. And in some cases, the
compiler is not quite smart enough to figure that puzzle out. For this reason,
the ? operator at the time of writing uses From, not Into. Most of the time that
doesn’t make a difference, because most types implement From, but it does
mean that error types from old libraries that implement Into instead may not
work with ?. As the compiler gets smarter, ? will likely be “upgraded” to use
Into, at which point that problem will go away, but it's what we have for now.


64 Chapter 4
The second aspect of ? to be aware of is that this operator is really just
syntax sugar for a trait tentatively called Try. At the time of writing, the Try
trait has not yet been stabilized, but by the time you read this, it’s likely that
it, or something very similar, will have been settled on. Since the details
haven’t all been figured out yet, I’ll give you only an outline of how Try
works, rather than the full method signatures. At its heart, Try defines a
wrapper type whose state is either one where further computation is useful
(the happy path), or one where it is not. Some of you will correctly think
of monads, though we won’t explore that connection here. For example, in
the case of Result<T, E>, if you have an Ok(t), you can continue on the happy
path by unwrapping the t. If you have an Err(e), on the other hand, you
want to stop executing and produce the error value immediately, since further
computation is not possible as you don’t have the t.
What’s interesting about Try is that it applies to more types than just
Result. An Option<T>, for example, follows the same pattern—if you have a
Some(t), you can continue on the happy path, whereas if you have a None, you
want to yield None instead of continuing. This pattern extends to more complex
types, like Poll<Result<T, E>>, whose happy path type is Poll<T>, which
makes ? apply in far more cases than you might expect. When Try stabilizes,
we may see ? start to work with all sorts of types to make our happy path
code nicer.
The ? operator is already usable in fallible functions, in doctests, and in
fn main. To reach its full potential, though, we also need a way to scope this
error handling. For example, consider the function in Listing 4-2.
fn do_the_thing() -> Result<(), Error> {
let thing = Thing::setup()?;
// .. code that uses thing and ? ..
thing.cleanup();
Ok(())
}
Listing 4-2: A multi-step fallible function using the ? operator
This won’t quite work as expected. Any ? between setup and cleanup
will cause an early return from the entire function, which would skip the
cleanup code! This is the problem try blocks are intended to solve. A try block
acts pretty much like a single-iteration loop, where ? uses break instead of
return, and the final expression of the block has an implicit break. We can
now fix the code in Listing 4-2 to always do cleanup, as shown in Listing 4-3.
fn do_the_thing() -> Result<(), Error> {
let thing = Thing::setup()?;
let r = try {
// .. code that uses thing and ? ..
};
thing.cleanup();
r
}
Listing 4-3: A multi-step fallible function that always cleans up after itself

Error Handling 65
Try blocks are also not stable at the time of writing, but there is enough
of a consensus on their usefulness that they’re likely to land in a form similar
to that described here.
Summary
This chapter covered the two primary ways to construct error types in Rust:
enumeration and erasure. We looked at when you may want to use each one
and the advantages and drawbacks of each. We also took a look at some of
the behind-the-scenes aspects of the ? operator and considered how ? may
become even more useful going forward. In the next chapter, we’ll take a
step back from the code and look at how you structure a Rust project. We’ll
look at feature flags, dependency management, and versioning as well as
how to manage more complex crates using workspaces and subcrates. See
you on the next page!

### 5.PROJECT S TRUCTURE

This chapter provides some ideas for structuring
your Rust projects. For simple projects,
the structure set up by cargo new is likely
to be something you think little about. You
may add some modules to split up the code, and some
dependencies for additional functionality, but that’s about it. However, as a
project grows in size and complexity, you’ll find that you need to go beyond
that. Maybe the compilation time for your crate is getting out of hand, or
you need conditional dependencies, or you need a better strategy for continuous
integration. In this chapter, we will look at some of the tools that the
Rust language, and Cargo in particular, provide that make it easier to manage
such things.
Features
Features are Rust’s primary tool for customizing projects. At its core, a feature
is just a build flag that crates can pass to their dependencies in order
to add optional functionality. Features carry no semantic meaning in and of
themselves—instead, you choose what a feature means for your crate

68 Chapter 5
Generally, we use features in three ways: to enable optional dependencies,
to conditionally include additional components of a crate, and to augment the
behavior of the code. Note that all of these uses are additive; features can add
to the functionality of the crate, but they shouldn’t generally do things like
remove modules or replace types or function signatures. This stems from the
principle that if a developer makes a simple change to their Cargo.toml, such
as adding a new dependency or enabling a feature, that shouldn’t make their
crate stop compiling. If a crate has mutually exclusive features, that principle
quickly falls by the wayside—if crate A depends on one feature of crate C,
and crate B on another mutually exclusive feature of C, adding a dependency
on crate B would then break crate A! For that reason, we generally follow the
principle that if crate A compiles against crate C with some set of features, it
should also compile if all features are enabled on crate C.
Cargo leans into this principle quite hard. For example, if two crates
(A and B) both depend on crate C, but they each enable different features
on C, Cargo will compile crate C only once, with all the features that either
A or B requires. That is, it’ll take the union of the requested features for C
across A and B. Because of this, it’s generally hard to add mutually exclusive
features to Rust crates; chances are that some two dependents will depend
on the crate with different features, and if those features are mutually
exclusive, the downstream crate will fail to build.
NOTE I highly recommend that you configure your continuous integration infrastructure to
check that your crate compiles for any combination of its features. One tool that helps you
do this is cargo-hack, which you can find at https://github.com/taiki-e/cargo-hack/.
Defining and Including Features
Features are defined in Cargo.toml. Listing 5-1 shows an example of a crate
named foo with a simple feature that enables the optional dependency syn.
[package]
name = "foo"
...
[features]
derive = ["syn"]
[dependencies]
syn = { version = "1", optional = true }
Listing 5-1: A feature that enables an optional dependency
When Cargo compiles this crate, it will not compile the syn crate by default,
which reduces compile time (often significantly). The syn crate will be compiled
only if a downstream crate needs to use the APIs enabled by the derive
feature and explicitly opts in to it. Listing 5-2 shows how such a downstream
crate bar would enable the derive feature, and thus include the syn dependency.
[package]
name = "bar"
...
Project Structure 69
[dependencies]
foo = { version = "1", features = ["derive"] }
Listing 5-2: Enabling a feature of a dependency
Some features are used so frequently that it makes more sense to have a
crate opt out of them rather than in to them. To support this, Cargo allows
you to define a set of default features for a crate. And similarly, it allows you
to opt out of the default features of a dependency. Listing 5-3 shows how
foo can make its derive feature enabled by default, while also opting out of
some of syn’s default features and instead enabling only the ones it needs
for the derive feature.
[package]
name = "foo"
...
[features]
derive = ["syn"]
default = ["derive"]
[dependencies.syn]
version = "1"
default-features = false
features = ["derive", "parsing", "printing"]
optional = true
Listing 5-3: Adding and opting out of default features, and thus optional dependencies
Here, if a crate depends on foo and does not explicitly opt out of the
default features, it will also compile foo’s syn dependency. In turn, syn will
be built with only the three listed features, and no others. Opting out of
default features this way, and opting in to only what you need, is a great way
to cut down on your compile times!
OPTIONAL DEPENDENCIES AS FEATURES
When you define a feature, the list that follows the equal sign is itself a list of
features. This might, at first, sound a little odd—in Listing 5-3, syn is a dependency,
not a feature. It turns out that Cargo makes every optional dependency
a feature with the same name as the dependency. You’ll see this if you try to
add a feature with the same name as an optional dependency; Cargo won’t
allow it. Support for a different namespace for features and dependencies is
in the works in Cargo, but has not been stabilized at the time of writing. In the
meantime, if you want to have a feature named after a dependency, you can
rename the dependency using package = "" to avoid the name collision. The list
of features that a feature enables can also include features of dependencies.
For example, you can write derive = ["syn/derive"] to have your derive feature
enable the derive feature of the syn dependency.

70 Chapter 5
Using Features in Your Crate
When using features, you need to make sure your code uses a dependency
only if it is available. And if your feature enables a particular component,
you need to make sure that if the feature isn’t enabled, the component is
not included.
You achieve this using conditional compilation, which lets you use annotations
to give conditions under which a particular piece of code should or
should not be compiled. Conditional compilation is primarily expressed
using the #[cfg] attribute. There is also the closely related cfg! macro, which
lets you change runtime behavior based on similar conditions. You can do
all sorts of neat things with conditional compilation, as we’ll see later in this
chapter, but the most basic form is #[cfg(feature = "some-feature")], which
makes it so that the next “thing” in the source code is compiled only if the
some-feature feature is enabled. Similarly, if cfg!(feature = "some-feature")
is equivalent to if true only if the derive feature is enabled (and if false
otherwise).
The #[cfg] attribute is used more often than the cfg! macro, because
the macro modifies runtime behavior based on the feature, which can
make it difficult to ensure that features are additive. You can place #[cfg]
in front of certain Rust items—such as functions and type definitions, impl
blocks, modules, and use statements—as well as on certain other constructs
like struct fields, function arguments, and statements. The #[cfg] attribute
can’t go just anywhere, though; where it can appear is carefully restricted by
the Rust language team so that conditional compilation can’t cause situations
that are too strange and hard to debug.
Remember that modifying certain public parts of your API may inadvertently
make a feature nonadditive, which in turn may make it impossible
for some users to compile your crate. You can often use the rules for
backward compatible changes as a rule of thumb here—for example, if you
make an enum variant or a public struct field conditional upon a feature,
then that type must also be annotated with #[non_exhaustive]. Otherwise,
a dependent crate that does not have the feature enabled may no longer
compile if the feature is added due to some second crate in the dependency
tree.
NOTE If you’re writing a large crate where you expect that your users will need only a subset
of the functionality, you should consider making it so that larger components (usually
modules) are guarded by features. That way, users can opt in to, and pay the compilation
cost of, only the parts they really need.
Workspaces
Crates play many roles in Rust—they are the vertices in the dependency
graph, the boundaries for trait coherence, and the scopes for compilation
features. Because of this, each crate is managed as a single compilation

Project Structure 71
unit; the Rust compiler treats a crate more or less as one big source file
compiled as one chunk that is ultimately turned into a single binary output
(either a binary or a library).
While this simplifies many aspects of the compiler, it also means that
large crates can be painful to work with. If you change a unit test, a comment,
or a type in one part of your application, the compiler must re-evaluate
the
entire crate to determine what, if anything, changed. Internally, the compiler
implements a number of mechanisms to speed up this process, like incremental
recompilation and parallel code generation, but ultimately the size of your
crate is a big factor in how long your project takes to compile.
For this reason, as your project grows, you may want to split it into
multiple crates that internally depend on one another. Cargo has just the
feature you need to make this convenient: workspaces. A workspace is a collection
of crates (often called subcrates) that are tied together by a top-level
Cargo.toml file like the one shown in Listing 5-4.
[workspace]
members = [
"foo",
"bar/one",
"bar/two",
]
Listing 5-4: A workspace Cargo.toml
The members array is a list of directories that each contain a crate in
the workspace. Those crates all have their own Cargo.toml files in their own
subdirectories, but they share a single Cargo.lock file and a single output
directory. The crate names don’t need to match the entry in members. It is
common, but not required, that crates in a workspace share a name prefix,
usually chosen as the name of the “main” crate. For example, in the tokio
crate, the members are called tokio, tokio-test, tokio-macros, and so on.
Perhaps the most important feature of workspaces is that you can interact
with all of the workspace’s members by invoking cargo in the root of the
workspace. Want to check that they all compile? cargo check will check them
all. Want to run all your tests? cargo test will test them all. It’s not quite as
convenient as having everything in one crate, so don’t go splitting everything
into minuscule crates, but it’s a pretty good approximation.
NOTE Cargo commands will generally do the “right thing” in a workspace. If you ever need
to disambiguate, such as if two workspace crates both have a binary by the same
name, use the -p flag (for package). If you are in the subdirectory for a particular
workspace crate, you can pass --workspace to perform the command for the entire
workspace instead.
Once you have a workspace-level Cargo.toml with the array of workspace
members, you can set your crates to depend on one another using path
dependencies, as shown in Listing 5-5.


72 Chapter 5

# bar/two/Cargo.toml

[dependencies]
one = { path = "../one" }

# bar/one/Cargo.toml

[dependencies]
foo = { path = "../../foo" }
Listing 5-5: Intercrate dependencies among workspace crates
Now if you make a change to the crate in bar/two, then only that crate
is re-compiled, since foo and bar/one did not change. It may even be faster
to compile your project from scratch, since the compiler does not need to
evaluate your entire project source for optimization opportunities.
SPECIFYING INTRA-WORKSPACE DEPENDENCIES
The most obvious way to specify that one crate in a workspace depends on
another is to use the path specifier, as shown in Listing 5-5. However, if your
individual subcrates are intended for public consumption, you may want to use
version specifiers instead.
Say you have a crate that depends on a Git version of the one crate from
the bar workspace in Listing 5-5 with one = { git = ". . ." }, and a released
version of foo (also from bar) with foo = "1.0.0". Cargo will dutifully fetch
the one Git repository, which holds the entire bar workspace, and see that one
in turn depends on foo, located at ../../foo inside the workspace. But Cargo
doesn’t know that the released version foo = "1.0.0" and the foo in the Git
repository are the same crate! It considers them two separate dependencies
that just happen to have the same name.
You may already see where this is going. If you try to use any type from
foo (1.0.0) with an API from one that accepts a type from foo, the compiler will
reject the code. Even though the types have the same name, the compiler can’t
know that they are the same underlying type. And the user will be thoroughly
confused, since the compiler will say something like “expected foo::Type, got
foo::Type.”
The best way to mitigate this problem is to use path dependencies between
subcrates only if they depend on unpublished changes. As long as one works
with foo 1.0.0, it should list foo = "1.0.0" in its dependencies. Only if you
make a change to foo that one needs should you change one to use a path
dependency. And once you release a new version of foo that one can depend
on, you should remove the path dependency again.
This approach also has its shortcomings. Now if you change foo and then
run the tests for one, you’ll see that one will be tested using the old foo, which
may not be what you expected. You’ll probably want to configure your continuous
integration infrastructure to test each subcrate both with the latest released
versions of the other subcrates and with all of them configured to use path
dependencies.

Project Structure 73
Project Configuration
Running cargo new sets you up with a minimal Cargo.toml that has the crate’s
name, its version number, some author information, and an empty list of
dependencies. That will take you pretty far, but as your project matures, there
are a number of useful things you may want to add to your Cargo.toml.
Crate Metadata
The first and most obvious thing to add to your Cargo.toml is all the metadata
directives that Cargo supports. In addition to obvious fields like
description and homepage, it can be useful to include information such as
the path to a README for the crate (readme), the default binary to run
with cargo run (default-run), and additional keywords and categories to help
crates.io categorize your crate.
For crates with a more convoluted project layout, it’s also useful to set
the include and exclude metadata fields. These dictate which files should be
included and published in your package. By default, Cargo includes all files
in a crate’s directory except any listed in your .gitignore file, but this may not
be what you want if you also have large test fixtures, unrelated scripts, or
other auxiliary data in the same directory that you do want under version
control. As their names suggest, include and exclude allow you to include
only a specific set of files or exclude files matching a given set of patterns,
respectively.
NOTE If you have a crate that should never be published, or should be published only to certain
alternative registries (that is, not to crates.io), you can set the publish directive
to false or to a list of allowed registries.
The list of metadata directives you can use continues to grow, so make
sure to periodically check in on the Manifest Format page of the Cargo reference
(https://doc.rust-lang.org/cargo/reference/manifest.html).
Build Configuration
Cargo.toml can also give you control over how Cargo builds your crate.
The most obvious tool for this is the build parameter, which allows you to
write a completely custom build program for your crate (we’ll revisit this
in Chapter 11). However, Cargo also provides two smaller, but very useful,
mechanisms that we’ll explore here: patches and profiles.
[patch]
The [patch] section of Cargo.toml allows you to specify a different source for
a dependency that you can use temporarily, no matter where in your dependencies
the patched dependency appears. This is invaluable when you need
to compile your crate against a modified version of some transitive dependency
to test a bug fix, a performance improvement, or a new minor release
you’re about to publish. Listing 5-6 shows an example of how you might
temporarily use a variant of a set of dependencies.


74 Chapter 5
[patch.crates-io]

# use a local (presumably modified) source

regex = { path = "/home/jon/regex" }

# use a modification on a git branch

serde = { git = "https://github.com/serde-rs/serde.git", branch = "faster" }

# patch a git dependency

[patch.'https://github.com/jonhoo/project.git']
project = { path = "/home/jon/project" }
Listing 5-6: Overriding dependency sources in Cargo.toml using [patch]
Even if you patch a dependency, Cargo takes care to check the crate
versions so that you don’t accidentally end up patching the wrong major version
of a crate. If you for some reason transitively depend on multiple major
versions of the same crate, you can patch each one by giving them distinct
identifiers, as shown in Listing 5-7.
[patch.crates-io]
nom4 = { path = "/home/jon/nom4", package = "nom" }
nom5 = { path = "/home/jon/nom5", package = "nom" }
Listing 5-7: Overriding multiple versions of the same crate in Cargo.toml using [patch]
Cargo will look at the Cargo.toml inside each path, realize that /nom4
contains major version 4 and that /nom5 contains major version 5, and patch
the two versions appropriately. The package keyword tells Cargo to look for a
crate by the name nom in both cases instead of using the dependency identifiers
(the part on the left) as it does by default. You can use package this way
in your regular dependencies as well to rename a dependency!
Keep in mind that patches are not taken into account in the package
that’s uploaded when you publish a crate. A crate that depends on your
crate will use only its own [patch] section (which may be empty), not that of
your crate!
CRATES VS. PACKAGES
You may wonder what the difference between a package and a crate is.
These two terms are often used interchangeably in informal contexts, but they
also have specific definitions that vary depending on whether you’re talking
about the Rust compiler, Cargo, crates.io, or something else. I personally think
of a crate as a Rust module hierarchy starting at a root .rs file (one where you
can use crate-level attributes like #![feature])—usually something like lib.rs or
main.rs. In contrast, a package is a collection of crates and metadata, so essentially
all that’s described by a Cargo.toml file. That may include a library crate,
multiple binary crates, some integration test crates, and maybe even multiple
workspace members that themselves have Cargo.toml files.


Project Structure 75
[profile]
The [profile] section lets you pass additional options to the Rust compiler
in order to change the way it compiles your crate. These options fall primarily
into three categories: performance options, debugging options, and
options that change code behavior in user-defined ways. They all have different
defaults depending on whether you are compiling in debug mode or
in release mode (other modes also exist).
The three primary performance options are opt-level, codegen-units,
and lto. The opt-level option tweaks runtime performance by telling the
compiler how aggressively to optimize your program (0 is “not at all,” 3 is “as
much as you can”). The higher the setting, the more optimized your code
will be, which may make it run faster. Extra optimization comes at the cost
of higher compile times, though, which is why optimizations are generally
enabled only for release builds.
NOTE You can also set opt-level to "s" to optimize for binary size, which may be important
on embedded platforms.
The codegen-units option is about compile-time performance. It tells the
compiler how many independent compilation tasks (code generation units) it
is allowed to split the compilation of a single crate into. The more pieces a
large crate’s compilation is split into, the faster it will compile, since more
threads can help compile the crate in parallel. Unfortunately, to achieve this
speedup, the threads need to work more or less independently, which means
code optimization suffers. Imagine, for example, that the segment of a crate
compiling in one thread could benefit from inlining some code in a different
segment—since the two segments are independent, that inlining can’t
happen! This setting, then, is a trade-off between compile-time performance
and runtime performance. By default, Rust uses an effectively unbounded
number of codegen units in debug mode (basically, “compile as fast as you
can”) and a smaller number (16 at the time of writing) in release mode.
The lto setting toggles link-time optimization (LTO), which enables the
compiler (or the linker, if you want to get technical about it) to jointly
optimize bits of your program, known as compilation units, that were originally
compiled separately. The exact details of LTO are beyond the scope
of this book, but the basic idea is that the output from each compilation
unit includes information about the code that went into that unit. After all
the units have been compiled, the linker makes another pass over all of the
units and uses that additional information to optimize the combined compiled
code. This extra pass adds to the compile time but recovers most of
the runtime performance that may have been lost due to splitting the compilation
into smaller parts. In particular, LTO can offer significant performance
boosts to performance-
sensitive programs that might benefit from
cross-crate optimization. Beware, though, that cross-crate LTO can add a
lot to your compile time.
Rust performs LTO across all the codegen units within each crate by
default in an attempt to make up for the lost optimizations caused by using
many codegen units. Since the LTO is performed only within each crate,


76 Chapter 5
rather than across crates, this extra pass isn’t too onerous, and the added
compile time should be lower than the amount of time saved by using a lot
of codegen units. Rust also offers a technique known as thin LTO, which
allows the LTO pass to be mostly parallelized, at the cost of missing some
optimizations a “full” LTO pass would have found.
NOTE LTO can be used to optimize across foreign function interface boundaries in many
cases, too. See the linker-plugin-lto rustc flag for more details.
The [profile] section also supports flags that aid in debugging, such as
debug, debug-assertions, and overflow-checks. The debug flag tells the compiler
to include debug symbols in the compiled binary. This increases the binary
size, but it means that you get function names and such, rather than just
instruction addresses, in backtraces and profiles. The debug-assertions flag
enables the debug_assert! macro and other related debug code that isn’t
compiled otherwise (through cfg(debug_assertions)). Such code may make
your program run slower, but it makes it easier to catch questionable behavior
at runtime. The overflow-checks flag, as the name implies, enables overflow
checks on integer operations. This slows them down (notice a trend
here?) but can help you catch tricky bugs early on. By default, these are all
enabled in debug mode and disabled in release mode.
[profile.*.panic]
The [profile] section has another flag that deserves its own subsection: panic.
This option dictates what happens when code in your program calls panic!,
either directly or indirectly through something like unwrap. You can set panic to
either unwind (the default on most platforms) or abort. We’ll talk more about
panics and unwinding in Chapter 9, but I’ll give a quick summary here.
Normally in Rust, when your program panics, the thread that panicked
starts unwinding its stack. You can think of unwinding as forcibly returning
recursively from the current function all the way to the bottom of that
thread’s stack. That is, if main called foo, foo called bar, and bar called baz, a
panic in baz would forcibly return from baz, then bar, then foo, and finally
from main, resulting in the program exiting. A thread that unwinds will
drop all values on the stack normally, which gives the values a chance to
clean up resources, report errors, and so on. This gives the running system
a chance to exit gracefully even in the case of a panic.
When a thread panics and unwinds, other threads continue running
unaffected. Only when (and if) the thread that ran main exits does the
program terminate. That is, the panic is generally isolated to the thread in
which the panic occurred.
This means unwinding is a double-edged sword; the program is limping
along with some failed components, which may cause all sorts of strange
behaviors. For example, imagine a thread that panics halfway through updating
the state in a Mutex. Any thread that subsequently acquires that Mutex
must now be prepared to handle the fact that the state may be in a partially
updated, inconsistent state. For this reason, some synchronization primitives
(like Mutex) will remember if a panic occurred when they were last accessed

Project Structure 77
and communicate that to any thread that tries to access the primitive subsequently.
If a thread encounters such a state, it will normally also panic, which
leads to a cascade that eventually terminates the entire program. But that is
arguably better than continuing to run with corrupted state!
The bookkeeping needed to support unwinding is not free, and it
often requires special support by the compiler and the target platform. For
example, many embedded platforms cannot unwind the stack efficiently at
all. Rust therefore supports a different panic mode: abort ensures the whole
program simply exits immediately when a panic occurs. In this mode, no
threads get to do any cleanup. This may seem severe, and it is, but it ensures
that the program is never running in a half-working state and that errors are
made visible immediately.
WARNI N G The panic setting is global—if you set it to abort, all your dependencies are also compiled
with abort.
You may have noticed that when a thread panics, it tends to print a backtrace:
the trail of function calls that led to where the panic occurred. This is
also a form of unwinding, though it is separate from the unwinding panic
behavior discussed here. You can have backtraces even with panic=abort by
passing -Cforce-unwind-tables to rustc, which makes rustc include the information
necessary to walk back up the stack while still terminating the program
on a panic.
PROFILE OVERRIDES
You can set profile options for just a particular dependency, or a particular profile,
using profile overrides. For example, Listing 5-8 shows how to enable aggressive
optimizations for the serde crate and moderate optimizations for all other crates in
debug mode, using the [profile.<profile-name>.package.<crate-name>] syntax.
[profile.dev.package.serde]
opt-level = 3
[profile.dev.package."*"]
opt-level = 2
Listing 5-8: Overriding profile options for a specific dependency or for a
specific mode
This kind of optimization override can be handy if some dependency would
be prohibitively slow in debug mode (such as decompression or video encoding),
and you need it optimized so that your test suite won’t take several days to
complete. You can also specify global profile defaults using a [profile.dev] (or
similar) section in the Cargo configuration file in ~/.cargo/config.
When you set optimization parameters for a specific dependency, keep in
mind that the parameters apply only to the code compiled as part of that crate;
if serde in this example has a generic method or type that you use in your crate,
(continued)

78 Chapter 5
the code of that method or type will be monomorphized and optimized in your
crate, and your crate’s profile settings will apply, not those in the profile override
for serde.
Conditional Compilation
Most Rust code you write is universal—it’ll work the same regardless of
what CPU or operating system it runs on. But sometimes you’ll have to do
something special to get the code to work on Windows, on ARM chips, or
when compiled against a particular platform application binary interface
(ABI). Or maybe you want to write an optimized version of a particular
function when a given CPU instruction is available, or disable some slow but
uninteresting setup code when running in a continuous integration (CI)
environment. To cater to cases like these, Rust provides mechanisms for conditional
compilation, in which a particular segment of code is compiled only
if certain conditions are true of the compilation environment.
We denote conditional compilation with the cfg keyword that you saw
earlier in the chapter in “Using Features in Your Crate.” It usually appears in
the form of the #[cfg(condition)] attribute, which says to compile the next item
only if condition is true. Rust also has #[cfg_attr(condition, attribute)], which
is compiled as #[attribute] if condition holds and is a no-op otherwise. You can
also evaluate a cfg condition as a Boolean expression using the cfg!(condition)
macro.
Every cfg construct takes a single condition made up of options, like
feature = "some-feature", and the combinators all, any, and not, which do
what you would probably expect. Options are either simple names, like unix,
or key/value pairs like those used by feature conditions.
There are a number of interesting options you can make compilation
dependent on. Let’s go through them, from most common to least common:
Feature options
You’ve already seen examples of these. Feature options take the form
feature = "name-of-feature" and are considered true if the named feature
is enabled. You can check for multiple features in a single condition
using the combinators. For example, any(feature = "f1", feature =
"f2") is true if either feature f1 or feature f2 is enabled.
Operating system options
These use key/value syntax with the key target_os and values like windows,
macos, and linux. You can also specify a family of operating systems
using target_family, which takes the value windows or unix. These are
common enough that they have received their own named short forms,
so you can use cfg(windows) and cfg(unix) directly. For example, if you
wanted a particular code segment to be compiled only on macOS and
Windows, you would write: #[cfg(any(windows, target_os = "macos"))].

Project Structure 79
Context options
These let you tailor code to a particular compilation context. The most
common of these is the test option, which is true only when the crate
is being compiled under the test profile. Keep in mind that test is set
only for the crate that is being tested, not for any of its dependencies.
This also means that test is not set in your crate when running integration
tests; it’s the integration tests that are compiled under the test
profile, whereas your actual crate is compiled normally (that is, without
test set). The same applies to the doc and doctest options, which are set
only when building documentation or compiling doctests, respectively.
There’s also the debug_assertions option, which is set in debug mode by
default.
Tool options
Some tools, like clippy and Miri, set custom options (more on that
later) that let you customize compilation when run under these tools.
Usually, these options are named after the tool in question. For example,
if you want a particular compute-intensive test not to run under
Miri, you can give it the attribute #[cfg_attr(miri, ignore)].
Architecture options
These let you compile based on the CPU instruction set the compiler
is targeting. You can specify a particular architecture with target_arch,
which takes values like x86, mips, and aarch64, or you can specify a particular
platform feature with target_feature, which takes values like avx
or sse2. For very low-level code, you may also find the target_endian and
target_pointer_width options useful.
Compiler options
These let you adapt your code to the platform ABI it is compiled against
and are available through target_env with values like gnu, msvc, and musl.
For historical reasons, this value is often empty, especially on GNU
platforms. You normally need this option only if you need to interface
directly with the environment ABI, such as when linking against an
ABI-specific symbol name using #[link].
While cfg conditions are usually used to customize code, some can also
be used to customize dependencies. For example, the dependency winrt
usually makes sense only on Windows, and the nix crate is probably useful
only on Unix-based platforms. Listing 5-9 gives an example of how you can
use cfg conditions for this:
[target.'cfg(windows)'.dependencies]
winrt = "0.7"
[target.'cfg(unix)'.dependencies]
nix = "0.17"
Listing 5-9: Conditional dependencies


80 Chapter 5
Here, we specify that winrt version 0.7 should be considered a dependency
only under cfg(windows) (so, on Windows), and nix version 0.17 only
under cfg(unix) (so, on Linux, macOS, and other Unix-based platforms).
One thing to keep in mind is that the [dependencies] section is evaluated
very early in the build process, when only certain cfg options are available.
In particular, feature and context options are not yet available at this point,
so you cannot use this syntax to pull in dependencies based on features
and contexts. You can, however, use any cfg that depends only on the target
specification or architecture, as well as any options explicitly set by tools
that call into rustc (like cfg(miri)).
NOTE While we’re on the topic of dependency specifications, I highly recommend that you set
up your CI infrastructure to perform basic auditing of your dependencies using tools
like cargo-deny and cargo-audit. These tools will detect cases where you transitively
depend on multiple major versions of a given dependency, where you depend on
crates that are unmaintained or have known security vulnerabilities, or where you
use licenses that you may want to avoid. Using such a tool is a great way to raise the
quality of your codebase in an automated way!
It’s also quite simple to add your own custom conditional compilation
options. You just have to make sure that --cfg=myoption is passed to rustc
when rustc compiles your crate. The easiest way to do this is to add your
--cfg to the RUSTFLAGS environment variable. This can come in handy in CI,
where you may want to customize your test suite depending on whether it’s
being run on CI or on a dev machine: add --cfg=ci to RUSTFLAGS in your CI
setup, and then use cfg(ci) and cfg(not(ci)) in your code. Options set this
way are also available in Cargo.toml dependencies.
Versioning
All Rust crates are versioned and are expected to follow Cargo’s implementation
of semantic versioning. Semantic versioning dictates the rules for what
kinds of changes require what kinds of version increases and for which versions
are considered compatible, and in what ways. The RFC 1105 standard
itself is well worth reading (it’s not horribly technical), but to summarize,
it differentiates between three kinds of changes: breaking changes, which
require a major version change; additions, which require a minor version
change; and bug fixes, which require only a patch version change. RFC 1105
does a decent job of outlining what constitutes a breaking change in Rust,
and we’ve touched on some aspects of it elsewhere in this book.
I won’t go into detail here about the exact semantics of the different
types of changes. Instead, I want to highlight some less straightforward ways
version numbers come up in the Rust ecosystem, which you need to keep in
mind when deciding how to version your own crates.

Project Structure 81
Minimum Supported Rust Version
The first Rust-ism is the minimum supported Rust version (MSRV). There is
much debate in the Rust community about what policy projects should
adhere to when it comes to their MSRV and versioning, and there’s no truly
good answer. The core of the problem is that some Rust users are limited to
using older versions of Rust, often in an enterprise setting where they have
little choice. If we constantly take advantage of newly stabilized APIs, those
users will not be able to compile the latest versions of our crates and will be
left behind.
There are two techniques crate authors can use to make life a little easier
for users in this position. The first is to establish an MSRV policy promising
that new versions of a crate will always compile with any stable release from
the last X months. The exact number varies, but 6 or 12 months is common.
With Rust’s six-week release cycle, that corresponds to the latest four or eight
stable releases, respectively. Any new code introduced to the project must
compile with the MSRV compiler (usually checked by CI) or be held until the
MSRV policy allows it to be merged as is. This can sometimes be a pain, as it
means these crates cannot take advantage of the latest and greatest the language
has to offer, but it will make life easier for your users.
The second technique is to make sure to increase the minor version
number of your crate any time that the MSRV changes. So, if you release
version 2.7.0 of your crate and that increases your MSRV from Rust 1.44
to Rust 1.45, then a project that is stuck on 1.44 and that depends on your
crate can use the dependency version specifier version = "2, <2.7" to keep
the project working until it can move on to Rust 1.45. It’s important that you
increment the minor version, not just the patch version, so that you can still
issue critical security fixes for the previous MSRV release by doing another
patch release if necessary.
Some projects take their MSRV support so seriously that they consider
an MSRV change a breaking change and increment the major version number.
This means that downstream projects will explicitly have to opt in to an
MSRV change, rather than opting out—but it also means that users who do
not have such strict MSRV requirements will not see future bug fixes without
updating their dependencies, which may require them to issue a breaking
change as well. As I said, none of these solutions are without drawbacks.
Enforcing an MSRV in the Rust ecosystem today is challenging. Only
a small subset of crates provide any MSRV guarantees, and even if your
dependencies do, you will need to constantly monitor them to know when
they increase their MSRV. When they do, you’ll need to do a new release
of your crate with the restricted version bounds mentioned previously to
make sure your MSRV doesn’t also change. This may in turn force you to
forego security and performance updates made to your dependencies, as
you’ll have to continue using older versions until your MSRV policy permits
updating. And that decision also carries over to your dependents. There
have been proposals to build MSRV checking into Cargo itself, but nothing
workable has been stabilized as of this writing.

82 Chapter 5
Minimal Dependency Versions
When you first add a dependency, it’s not always clear what version specifier
you should give that dependency. Programmers commonly choose the latest
version, or just the current major version, but chances are that both of those
choices are wrong. By “wrong,” I don’t mean that your crate won’t compile,
but rather that making that choice may cause strife for users of your crate
down the line. Let’s look at why each of these cases is problematic.
First, consider the case where you add a dependency on hugs = "1.7.3",
the latest published version. Now imagine that a developer somewhere
depends on your crate, but they also depend on some other crate, foo, that
itself depends on hugs. Further imagine that the author of foo is really careful
about their MSRV policy, so they depend on hugs = "1, <1.6". Here, you’ll run
into trouble. When Cargo sees hugs = "1.7.3", it considers only versions >=1.7.
But then it sees that foo’s dependency on hugs requires <1.6, so it gives up and
reports that there is no version of hugs compatible with all the requirements.
NOTE In practice, there are a number of reasons why a crate may explicitly not want a
newer version of a dependency. The most common ones are to enforce MSRV, to meet
enterprise auditing requirements (the newer version will contain code that hasn’t been
audited), and to ensure reproducible builds where only the exact listed version is used.
This is unfortunate, as it could well be that your crate compiles fine
with, say, hugs 1.5.6. Maybe it even compiles fine with any 1.X version! But
by using the latest version number, you are telling Cargo to consider only
versions at or beyond that minor version. Is the solution to use hugs = "1"
instead, then? No, that’s not quite right either. It could be that your code
truly does depend on something that was added only in hugs 1.6, so while
1.6.2 would be fine, 1.5.6 would not be. You wouldn’t notice this if you were
only ever compiling your crate in situations where a newer version ends up
getting used, but if some crate in the dependency graph specifies hugs = "1,
<1.5", your crate would not compile!
The right strategy is to list the earliest version that has all the things
your crate depends on and to make sure that this remains the case even
as you add new code to your crate. But how do you establish that beyond
trawling the changelogs, or through trial and error? Your best bet is to use
Cargo’s unstable -Zminimal-versions flag, which makes your crate use the
minimum acceptable version for all dependencies, rather than the maximum.
Then, set all your dependencies to just the latest major version number,
try to compile, and add a minor version to any dependencies that don’t.
Rinse and repeat until everything compiles fine, and you now have your
minimum version requirements!
It’s worth noting that, like with MSRV, minimal version checking faces
an ecosystem adoption problem. While you may have set all your version
specifiers correctly, the projects you depend on may not have. This makes
the Cargo minimal versions flag hard to use in practice (and is why it’s still
unstable). If you depend on foo, and foo depends on bar with a specifier
of bar = "1" when it actually requires bar = "1.4", Cargo will report that it
failed to compile foo no matter how you list foo because the -Z flag tells it

Project Structure 83
to always prefer minimal versions. You can work around this by listing bar
directly in your dependencies with the appropriate version requirement,
but these workarounds can be painful to set up and maintain. You may end
up listing a large number of dependencies that are only really pulled in
through your transitive dependencies, and you’ll have to keep that list up to
date as time goes on.
NOTE One current proposal is to present a flag that favors minimal versions for the current
crate but maximal ones for dependencies, which seems quite promising.
Changelogs
For all but the most trivial crates, I highly recommend keeping a changelog.
There is little more frustrating than seeing that a dependency has received
a major version bump and then having to dig through the Git logs to figure
out what changed and how to update your code. I recommend that you do
not just dump your Git logs into a file named changelog, but instead keep a
manual changelog. It is much more likely to be useful.
A simple but good format for changelogs is the Keep a Changelog format
documented at https://keepachangelog.com/.
Unreleased Versions
Rust considers version numbers even when the source of a dependency is a
directory or a Git repository. This means that semantic versioning is important
even when you have not yet published a release to crates.io; it matters
what version is listed in your Cargo.toml between releases. The semantic versioning
standard does not dictate how to handle this case, but I’ll provide a
workflow that works decently well without being too onerous.
After you’ve published a release, immediately update the version number
in your Cargo.toml to the next patch version with a suffix like -alpha.1.
If you just released 2.0.3, make the new version 2.0.4-alpha.1. If you just
released an alpha, increment the alpha number instead.
As you make changes to the code between releases, keep an eye out for
additive or breaking changes. If one happens, and the corresponding version
number has not changed since the last release, increment it. For example,
if the last released version is 2.0.3, the current version is 2.0.4-alpha.2,
and you make an additive change, make the version with the change 2.1.0-
alpha.1. If you made a breaking change, it becomes 3.0.0-alpha.1 instead. If
the corresponding version increase has already been made, just increment
the alpha number.
When you make a release, remove the suffix (unless you want to do a
prerelease), then publish, and start from the top.
This process is effective because it makes two common workflows work
much better. First, imagine that a developer depends on major version 2
of your crate, but they need a feature that’s currently available only in Git.
Then you commit a breaking change. If you don’t increase the major version
at the same time, their code will suddenly fail in unexpected ways,


84 Chapter 5
either by failing to compile or as a result of weird runtime issues. If you follow
the procedure laid out here, they’ll instead be notified by Cargo that a
breaking change has occurred, and they’ll have to either resolve that or pin
a specific commit.
Next, imagine that a developer needs a feature they just contributed
to your crate, but which isn’t part of any released version of your crate yet.
They’ve used your crate behind a Git dependency for a while, so other
developers on their project already have older checkouts of your crate’s
repository. If you do not increment the major version number in Git, this
developer has no way to communicate that their project now relies on the
feature that was just merged. If they push their change, their fellow developers
will find that the project no longer compiles, since Cargo will reuse the
old checkout. If, on the other hand, the developer can increment the minor
version number for the Git dependency, then Cargo will realize that the old
checkout is outdated.
This workflow is by no means perfect. It doesn’t provide a good way to
communicate multiple minor or major changes between releases, and you
still need to do a bit of work to keep track of the versions. However, it does
address two of the most common issues Rust developers run into when they
work against Git dependencies, and even if you make multiple such changes
between releases, this workflow will still catch many of the issues.
If you’re not too worried about small or consecutive version numbers
in releases, you can improve this suggested workflow by simply always incrementing
the appropriate part of the version number. Be aware, though, that
depending on how frequently you make such changes, this may make your
version numbers quite large!
Summary
In this chapter, we’ve looked at a number of mechanisms for configuring,
organizing, and publishing crates, for both your own benefit and that of
others. We’ve also gone over some common gotchas when working with
dependencies and features in Cargo that now hopefully won’t catch you out
in the future. In the next chapter we’ll turn to testing and dig into how you
go beyond Rust’s simple #[test] functions that we know and love.


### 6.TESTING

In this chapter, we’ll look at the various
ways in which you can extend Rust’s testing
capabilities and what other kinds of testing
you may want to add into your testing mix. Rust
comes with a number of built-in testing facilities that
are well covered in The Rust Programming Language,
represented primarily by the #[test] attribute and the
tests/ directory. These will serve you well across a wide
range of applications and scales and are often all you need when you are
getting started with a project. However, as the codebase develops and your
testing needs grow more elaborate, you may need to go beyond just tagging
`#[test]` onto individual functions.
This chapter is divided into two main sections. The first part covers
Rust testing mechanisms, like the standard testing harness and conditional
testing code. The second looks at other ways to evaluate the correctness of
your Rust code, such as benchmarking, linting, and fuzzing.


86 Chapter 6
Rust Testing Mechanisms
To understand the various testing mechanisms Rust provides, you must first
understand how Rust builds and runs tests. When you run cargo test --lib,
the only special thing Cargo does is pass the --test flag to rustc. This flag
tells rustc to produce a test binary that runs all the unit tests, rather than
just compiling the crate’s library or binary. Behind the scenes, --test has
two primary effects. First, it enables cfg(test) so that you can conditionally
include testing code (more on that in a bit). Second, it makes the compiler
generate a test harness: a carefully generated main function that invokes each
`#[test] function in your program when it’s run`.
The Test Harness
The compiler generates the test harness main function through a mix of
procedural macros, which we’ll discuss in greater depth in Chapter 7, and
a light sprinkling of magic. Essentially, the harness transforms every function
annotated by #[test] into a test descriptor—this is the procedural macro
part. It then exposes the path of each of the descriptors to the generated
main function—this is the magic part. The descriptor includes information
like the test’s name, any additional options it has set (like #[should_panic]),
and so on. At its core, the test harness iterates over the tests in the crate,
runs them, captures their results, and prints the results. So, it also includes
logic to parse command line arguments (for things like --test-threads=1),
capture test output, run the listed tests in parallel, and collect test results.
As of this writing, Rust developers are working on making the magic
part of test harness generation a publicly available API so that developers
can build their own test harnesses. This work is still at the experimental
stage, but the proposal aligns fairly closely with the model as it exists today.
Part of the magic that needs to be figured out is how to ensure that #[test]
functions are available to the generated main function even if they are inside
private submodules.
Integration tests (the tests in tests/) follow the same process as unit
tests, with the one exception that they are each compiled as their own
separate crate, meaning they can access only the main crate’s public interface
and are run against the main crate compiled without #[cfg(test)]. A
test harness is generated for each file in tests/. Test harnesses are not generated
for files in subdirectories under tests/ to allow you to have shared
submodules
for your tests.
NOTE If you explicitly want a test harness for a file in a subdirectory, you can opt in to that
by calling the file main.rs.
Rust does not require that you use the default test harness. You can
instead opt out of it and implement your own main method that represents
the test runner by setting harness = false for a given integration test in
Cargo.toml, as shown in Listing 6-1. The main method that you define will
then be invoked to run the test.

Testing 87
[[test]]
name = "custom"
path = "tests/custom.rs"
harness = false
Listing 6-1: Opting out of the standard test harness
Without the test harness, none of the magic around #[test] happens.
Instead, you’re expected to write your own main function to run the testing
code you want to execute. Essentially, you’re writing a normal Rust binary
that just happens to be run by cargo test. That binary is responsible for
handling all the things that the default harness normally does (if you want
to support them), such as command line flags. The harness property is set
separately for each integration test, so you can have one test file that uses
the standard harness and one that does not.
ARGUMENTS TO THE DEFAULT TEST HARNESS
The default test harness supports a number of command line arguments to
configure how the tests are run. These aren’t passed to cargo test directly
but rather to the test binary that Cargo compiles and runs for you when you
run cargo test. To access that set of flags, pass -- to cargo test, followed by
the arguments to the test binary. For example, to see the help text for the test
binary, you’d run cargo test -- --help.
A number of handy configuration options are available through these command
line arguments. The --nocapture flag disables the output capturing that
normally happens when you run Rust tests. This is useful if you want to observe
a test’s output in real time rather than all at once after the test has failed. You
can use the --test-threads option to limit how many tests run concurrently,
which is helpful if you have a test that hangs or segfaults and you want to figure
out which one it is by running the tests sequentially. There’s also a --skip option
for skipping tests that match a certain pattern, --ignored to run tests that would
normally be ignored (such as those that require an external program to be running),
and --list to just list all the available tests.
Keep in mind that these arguments are all implemented by the default test
harness, so if you disable it (with harness = false), you’ll have to implement the
ones you need yourself in your main function!
Integration tests without a harness are primarily useful for benchmarks,
as we’ll see later, but they also come in handy when you want to run
tests that don’t fit the standard “one function, one test” model. For example,
you’ll frequently see harnessless tests used with fuzzers, model checkers,
and tests that require a custom global setup (like under WebAssembly
or when working with custom targets).



88 Chapter 6
`#[cfg(test)]`
When Rust builds code for testing, it sets the compiler configuration flag
test, which you can then use with conditional compilation to have code
that is compiled out unless it is specifically being tested. On the surface,
this may seem odd: don’t you want to test exactly the same code that’s going
into production? You do, but having code exclusively available when testing
allows you to write better, more thorough tests, in a few ways.
MOCKING
When writing tests, you often want tight control over the code you’re testing as
well as any other types that your code may interact with. For example, if you
are testing a network client, you probably do not want to run your unit tests
over a real network but instead want to directly control what bytes are emitted
by the “network” and when. Or, if you’re testing a data structure, you want your
test to use types that allow you to control what each method returns on each
invocation. You may also want to gather metrics such as how often a given
method was called or whether a given byte sequence was emitted.
These “fake” types and implementations are known as mocks, and they
are a key feature of any extensive unit test suite. While you can often do the
work needed to get this kind of control manually, it’s nicer to have a library take
care of most of the nitty-gritty details for you. This is where automated mocking
comes into play. A mocking library will have facilities for generating types
(including functions) with particular properties or signatures, as well as mechanisms
to control and introspect those generated items during a test execution.
Mocking in Rust generally happens through generics—as long as your
program, data structure, framework, or tool is generic over anything you might
want to mock (or takes a trait object), you can use a mocking library to generate
conforming types that will instantiate those generic parameters. You then
write your unit tests by instantiating your generic constructs with the generated
mock types, and you’re off to the races!
In situations where generics are inconvenient or inappropriate, such as
if you want to avoid making a particular aspect of your type generic to users,
you can instead encapsulate the state and behavior you want to mock in a
dedicated struct. You would then generate a mocked version of that struct and
its methods and use conditional compilation to use either the real or mocked
implementation depending on cfg(test) or a test-only feature like cfg(feature
= "test_mock_foo").
At the moment, there isn’t a single mocking library, or even a single mocking
approach, that has emerged as the One True Answer in the Rust community.
The most extensive and thorough mocking library I know of is the mockall crate,
but that is still under active development, and there are many other contenders.

Testing 89
Test-Only APIs
First, having test-only code allows you to expose additional methods, fields,
and types to your (unit) tests so the tests can check not only that the public
API behaves correctly but also that the internal state is correct. For example,
consider the HashMap type from hashbrown, the crate that implements the
standard library HashMap. The HashMap type is really just a wrapper around
a RawTable type, which is what implements most of the hash table logic.
Suppose that after doing a HashMap::insert on an empty map, you want to
check that a single bucket in the map is nonempty, as shown in Listing 6-2.
```rust
#[test]
fn insert_just_one() {
let mut m = HashMap::new();
m.insert(42, ());
let full = m.table.buckets.iter().filter(Bucket::is_full).count();
assert_eq!(full, 1);
}
```
Listing 6-2: A test that accesses inaccessible internal state and thus does not compile
This code will not compile as written, because while the test code can
access the private table field of HashMap, it cannot access the also private
buckets field of RawTable, as RawTable lives in a different module. We could fix
this by making the buckets field visibility pub(crate), but we really don’t want
HashMap to be able to touch buckets in general, as it could accidentally corrupt
the internal state of the RawTable. Even making buckets available as read-only
could be problematic, as new code in HashMap may then start depending on
the internal state of RawTable, making future modifications more difficult.
The solution is to use #[cfg(test)]. We can add a method to RawTable
that allows access to buckets only while testing, as shown in Listing 6-3,
and thereby avoid adding footguns for the rest of the code. The code from
Listing 6-2 can then be updated to call buckets() instead of accessing the
private buckets field.
```rust
impl RawTable {
#[cfg(test)]
pub(crate) fn buckets(&self) -> &[Bucket] {
&self.buckets
}
}
```
Listing 6-3: Using #[cfg(test)] to make internal state accessible in the testing context
Bookkeeping for Test Assertions
The second benefit of having code that exists only during testing is that
you can augment the program to perform additional runtime bookkeeping
that can then be inspected by tests. For example, imagine you’re writing
your own version of the BufWriter type from the standard library. When
testing it, you want to make sure that BufWriter does not issue system calls

90 Chapter 6
unnecessarily. The most obvious way to do so is to have the BufWriter keep
track of how many times it has invoked write on the underlying Write.
However, in production this information isn’t important, and keeping
track of it introduces (marginal) performance and memory overhead. With
#[cfg(test)], you can have the bookkeeping happen only when testing, as
shown in Listing 6-4.
struct BufWriter<T> {
#[cfg(test)]
write_through: usize,
// other fields...
}
impl<T: Write> Write for BufWriter<T> {
fn write(&mut self, buf: &[u8]) -> Result<usize> {
// ...
if self.full() {
#[cfg(test)]
self.write_through += 1;
let n = self.inner.write(&self.buffer[..])?;
// ...
}
}
Listing 6-4: Using #[cfg(test)] to limit bookkeeping to the testing context
Keep in mind that test is set only for the crate that is being compiled as
a test. For unit tests, this is the crate being tested, as you would expect. For
integration tests, however, it is the integration test binary being compiled as
a test—the crate you are testing is just compiled as a library and so will not
have test set.
Doctests
Rust code snippets in documentation comments are automatically run as
test cases. These are commonly referred to as doctests. Because doctests
appear in the public documentation of your crate, and users are likely to
mimic what they contain, they are run as integration tests. This means that
the doctests don’t have access to private fields and methods, and test is not
set on the main crate’s code. Each doctest is compiled as its own dedicated
crate and is run in isolation, just as if the user had copy-pasted the doctest
into their own program.
Behind the scenes, the compiler performs some preprocessing on
doctests
to make them more concise. Most importantly, it automatically
adds an fn main around your code. This allows doctests to focus only on the
important bits that the user is likely to care about, like the parts that actually
use types and methods from your library, without including unnecessary
boilerplate.
You can opt out of this auto-wrapping by defining your own fn main in
the doctest. You may want to do this, for example, if you want to write an

Testing 91
asynchronous main function using something like #[tokio::main] async fn
main, or if you want to add additional modules to the doctest.
To use the ? operator in your doctest, you don’t normally have to use a
custom main function as rustdoc includes some heuristics to set the return
type to Result<(), impl Debug> if your code looks like it makes use of ? (for
example, if it ends with Ok(())). If type inference gives you a hard time
about the error type for the function, you can disambiguate it by changing
the last line of the doctest to be explicitly typed, like this: Ok::<(), T>(()).
Doctests have a number of additional features that come in handy as
you write documentation for more complex interfaces. The first is the ability
to hide individual lines. If you prefix a line of a doctest with a #, that line
is included when the doctest is compiled and run, but it is not included in
the code snippet generated in the documentation. This lets you easily hide
details that are not important to the current example, such as implementing
traits for dummy types or generating values. It is also useful if you wish
to present a sequence of examples without showing the same leading code
each time. Listing 6-5 gives an example of what a doctest with hidden lines
might look like.
/// Completely frobnifies a number through I/O.
///
/// In this first example we hide the value generation.
/// ```
/// # let unfrobnified_number = 0;
/// # let already_frobnified = 1;
/// assert!(frobnify(unfrobnified_number).is_ok());
/// assert!(frobnify(already_frobnified).is_err());
/// ```
///
/// Here's an example that uses ? on multiple types
/// and thus needs to declare the concrete error type,
/// but we don't want to distract the user with that.
/// We also hide the use that brings the function into scope.
/// ```
/// # use mylib::frobnify;
/// frobnify("0".parse()?)?;
/// # Ok::<(), anyhow::Error>(())
/// ```
///
/// You could even replace an entire block of code completely,
/// though use this _very_ sparingly:
/// ```
/// # /*
/// let i = ...;
/// # */
/// # let i = 42;
/// frobnify(i)?;
/// ```
fn frobnify(i: usize) -> std::io::Result<()> {
Listing 6-5: Hiding lines in a doctest with #

92 Chapter 6
NOTE Use this feature with care; it can be frustrating to users if they copy-paste an example
and then it doesn’t work because of required steps that you’ve hidden.
Much like #[test] functions, doctests also support attributes that
modify how the doctest is run. These attributes go immediately after the
triple-backtick used to denote a code block, and multiple attributes can be
separated by commas.
Like with test functions, you can specify the should_panic attribute to
indicate that the code in a particular doctest should panic when run, or
ignore to check the code segment only if cargo test is run with the --ignored
flag. You can also use the no_run attribute to indicate that a given doctest
should compile but should not be run.
The attribute compile_fail tells rustdoc that the code in the documentation
example should not compile. This indicates to the user that a particular
use is not possible and serves as a useful test to remind you to update
the documentation should the relevant aspect of your library change. You
can also use this attribute to check that certain static properties hold for
your types. Listing 6-6 shows an example of how you can use compile_fail to
check that a given type does not implement Send, which may be necessary to
uphold safety guarantees in unsafe code.
```compile_fail
# struct MyNonSendType(std::rc::Rc<()>);
fn is_send<T: Send>() {}
is_send::<MyNonSendType>();
```
Listing 6-6: Testing that code fails to compile with compile_fail
compile_fail is a fairly crude tool in that it gives no indication of why the
code does not compile. For example, if code doesn’t compile because of a
missing semicolon, a compile_fail test will appear to have been successful.
For that reason, you’ll usually want to add the attribute only after you have
made sure that the test indeed fails to compile with the expected error.
If you need more fine-grained tests for compilation errors, such as when
developing macros, take a look at the trybuild crate.
Additional Testing Tools
There’s a lot more to testing than just running test functions and seeing that
they produce the expected result. A thorough survey of testing techniques,
methodologies, and tools is outside the scope of this book, but there are
some key Rust-specific pieces that you should know about as you expand
your testing repertoire.
Linting
You may not consider a linter’s checks to be tests, but in Rust they often
can be. The Rust linter clippy categorizes a number of its lints as correctness

Testing 93
lints. These lints catch code patterns that compile but are almost certainly
bugs. Some examples are a = b; b = a, which fails to swap a and b;
std::mem::forget(t), where t is a reference; and for x in y.next(), which will
iterate only over the first element in y. If you are not running clippy as part
of your CI pipeline already, you probably should be.
Clippy comes with a number of other lints that, while usually helpful,
may be more opinionated than you’d prefer. For example, the type_complexity
lint, which is on by default, issues a warning if you use a particularly involved
type in your program, like Rc<Vec<Vec<Box<(u32, u32, u32, u32)>>>>. While that
warning encourages you to write code that is easier to read, you may find it too
pedantic to be broadly useful. If some part of your code erroneously triggers
a particular lint, or you just want to allow a specific instance of it, you can opt
out of the lint just for that piece of code with #[allow(clippy::name_of_lint)].
The Rust compiler also comes with its own set of lints in the form of
warnings, though these are usually more directed toward writing idiomatic
code than checking for correctness. Instead, correctness lints in the compiler
are simply treated as errors (take a look at rustc -W help for a list).
NOTE Not all compiler warnings are enabled by default. Those disabled by default are usually
still being refined, or are more about style than content. A good example of this is
the “idiomatic Rust 2018 edition” lint, which you can enable with #![warn(rust_2018
_idioms)]. When this lint is enabled, the compiler will tell you if you’re failing to take
advantage of changes brought by the Rust 2018 edition. Some other lints that you may
want to get into the habit of enabling when you start a new project are missing_docs
and missing_debug_implementations, which warn you if you’ve forgotten to document
any public items in your crate or add Debug implementations for any public types,
respectively.
Test Generation
Writing a good test suite is a lot of work. And even when you do that work,
the tests you write test only the particular set of behaviors you were considering
at the time you wrote them. Luckily, you can take advantage of
a number of test generation techniques to develop better and more thorough
tests. These generate input for you to use to check your application’s
correctness. Many such tools exist, each with their own strengths and
weaknesses, so here I’ll cover only the main strategies used by these tools:
fuzzing and property testing.
Fuzzing
Entire books have been written about fuzzing, but at a high level the idea is
simple: generate random inputs to your program and see if it crashes. If the
program crashes, that’s a bug. For example, if you’re writing a URL parsing
library, you can fuzz-test your program by systematically generating random
strings and throwing them at the parsing function until it panics. Done


94 Chapter 6
naively, this would take a while to yield results: if the fuzzer starts with a,
then b, then c, and so on, it will take it a long time to generate a tricky URL
like http://[:]. In practice, modern fuzzers use code coverage metrics to
explore different paths in your code, which lets them reach higher degrees
of coverage faster than if the inputs were truly chosen at random.
Fuzzers are great at finding strange corner cases that your code doesn’t
handle correctly. They require little setup on your part: you just point the
fuzzer at a function that takes a “fuzzable” input, and off it goes. For example,
Listing 6-7 shows an example of how you might fuzz-test a URL parser.
libfuzzer_sys::fuzz_target!(|data: &[u8]| {
if let Ok(s) = std::str::from_utf8(data) {
let _ = url::Url::parse(s);
}
});
Listing 6-7: Fuzzing a URL parser with libfuzzer
The fuzzer will generate semi-random inputs to the closure, and any
that form valid UTF-8 strings will be passed to the parser. Notice that the
code here doesn’t check whether the parsing succeeds or fails—instead, it’s
looking for cases where the parser panics or otherwise crashes due to internal
invariants that are violated.
The fuzzer keeps running until you terminate it, so most fuzzing tools
come with a built-in mechanism to stop after a certain number of test cases
have been explored. If your input isn’t a trivially fuzzable type—something
like a hash table—you can usually use a crate like arbitrary to turn the byte
string that the fuzzer generates into a more complex Rust type. It feels like
magic, but under the hood it’s actually implemented in a very straightforward
fashion. The crate defines an Arbitrary trait with a single method,
arbitrary, that constructs the implementing type from a source of random
bytes. Primitive types like u32 or bool read the necessary number of bytes
from that input to construct a valid instance of themselves, whereas more
complex types like HashMap or BTreeSet produce one number from the input
to dictate their length and then call Arbitrary that number of times on their
inner types. There’s even an attribute, #[derive(Arbitrary)], that implements
Arbitrary by just calling arbitrary on each contained type! To explore fuzzing
further, I recommend starting with cargo-fuzz.
Property-Based Testing
Sometimes you want to check not only that your program doesn’t crash but
also that it does what it’s expected to do. It’s great that your add function
didn’t panic, but if it tells you that the result of add(1, 4) is 68, it’s probably
still wrong. This is where property-based testing comes into play; you describe
a number of properties your code should uphold, and then the property
testing framework generates inputs and checks that those properties
indeed hold.

Testing 95
A common way to use property-based testing is to first write a simple
but naive version of the code you want to test that you are confident is correct.
Then, for a given input, you give that input to both the code you want
to test and the simplified but naive version. If the result or output of the
two implementations is the same, your code is good—that is the correctness
property you’re looking for—but if it’s not, you’ve likely found a bug.
You can also use property-based testing to check for properties not directly
related to correctness, such as whether operations take strictly less time for
one implementation than another. The common principle is that you want
any difference in outcome between the real and test versions to be informative
and actionable so that every failure allows you to make improvements.
The naive implementation might be one from the standard library that
you’re trying to replace or augment (like std::collections::VecDeque), or it
might be a simpler version of an algorithm that you’re trying optimize (like
naive versus optimized matrix multiplication).
If this approach of generating inputs until some condition is met
sounds a lot like fuzzing, that’s because it is—smarter people than I have
argued that fuzzing is “ just” property-based testing where the property
you’re testing for is “it doesn’t crash.”
One downside of property-based testing is that it relies more heavily on
the provided descriptions of the inputs. Whereas fuzzing will keep trying
all possible inputs, property testing tends to be guided by developer annotations
like “a number between 0 and 64” or “a string that contains three
commas.” This allows property testing to more quickly reach cases that fuzzers
may take a long time to encounter randomly, but it does require manual
work and may miss important but niche buggy inputs. As fuzzers and property
testers grow closer, however, fuzzers are starting to gain this kind of
constraint-based searching capability as well.
If you’re curious about property-based test generation, I recommend
starting with the proptest crate.
TESTING SEQUENCES OF OPERATIONS
Since fuzzers and property testers allow you to generate arbitrary Rust types,
you aren’t limited to testing a single function call in your crate. For example, say
you want to test that some type Foo behaves correctly if you perform a particular
sequence of operations on it. You could define an enum Operation that lists
operations, and make your test function take a Vec<Operation>. Then you could
instantiate a Foo and perform each operation on that Foo, one after the other.
Most testers have support for minimizing inputs, so they will even search for
the smallest sequence of operations that still violates a property if a propertyviolating
input is found!

96 Chapter 6
Test Augmentation
Let’s say you have a magnificent test suite all set up, and your code passes
all the tests. It’s glorious. But then, one day, one of the normally reliable
tests inexplicably fails or crashes with a segmentation fault. There are two
common reasons for these kinds of nondeterministic test failures: race conditions,
where your test might fail only if two operations occur on different
threads in a particular order, and undefined behavior in unsafe code, such
as if some unsafe code reads a particular value out of uninitialized memory.
Catching these kinds of bugs with normal tests can be difficult—often
you don’t have sufficient low-level control over thread scheduling, memory
layout and content, or other random-ish system factors to write a reliable
test. You could run each test many times in a loop, but even that may not
catch the error if the bad case is sufficiently rare or unlikely. Luckily, there
are tools that can help augment your tests to make catching these kinds of
bugs much easier.
The first of these is the amazing tool Miri, an interpreter for Rust’s
mid-level intermediate representation (MIR). MIR is an internal, simplified
representation of Rust that helps the compiler find optimizations and
check properties without having to consider all of the syntax sugar of Rust
itself. Running your tests through Miri is as simple as running cargo miri
test. Miri interprets your code rather than compiling and running it like a
normal binary, which makes the tests run a decent amount slower. But in
return, Miri can keep track of the entire program state as each line of your
code executes. This allows Miri to detect and report if your program ever
exhibits certain types of undefined behavior, such as uninitialized memory
reads, uses of values after they’ve been dropped, or out-of-bounds pointer
accesses. Rather than having these operations yield strange program behaviors
that may only sometimes result in observable test failures (like crashes),
Miri detects them when they happen and tells you immediately.
For example, consider the very unsound code in Listing 6-8, which creates
two exclusive references to a value.
let mut x = 42;
let x: *mut i32 = &mut x;
let (x1, x2) = unsafe { (&mut *x, &mut *x) };
println!("{} {}", x1, x2);
Listing 6-8: Wildly unsafe code that Miri detects is incorrect
At the time of writing, if you run this code through Miri, you get an
error that points out exactly what’s wrong:
error: Undefined Behavior: trying to reborrow for Unique at alloc1383, but
parent tag <2772> does not have an appropriate item in the borrow stack
--> src/main.rs:4:6
|
4 | let (x1, x2) = unsafe { (&mut *x, &mut *x) };
| ^^ trying to reborrow for Unique at alloc1383, but parent tag <2772>
does not have an appropriate item in the borrow stack

Testing 97
NOTE Miri is still under development, and its error messages aren’t always the easiest to
understand. This is a problem that’s being actively worked on, so by the time you read
this, the error output may have already gotten much better!
Another tool worth looking at is Loom, a clever library that tries to
ensure your tests are run with every relevant interleaving of concurrent
operations. At a high level, Loom keeps track of all cross-thread synchronization
points and runs your tests over and over, adjusting the order in
which threads proceed from those synchronization points each time. So, if
thread A and thread B both take the same Mutex, Loom will ensure that the
test runs once with A taking it first and once with B taking it first. Loom
also keeps track of atomic accesses, memory orderings, and accesses to
UnsafeCell (which we’ll discuss in Chapter 9) and checks that threads do not
access them inappropriately. If a test fails, Loom can give you an exact rundown
of which threads executed in what order so you can determine how
the crash happened.
Performance Testing
Writing performance tests is difficult because it is often hard to accurately
model a workload that reflects real-world usage of your crate. But having
such tests is important; if your code suddenly runs 100 times slower, that
really should be considered a bug, yet without a performance test you may
not spot the regression. If your code runs 100 times faster, that might also
indicate that something is off. Both of these are good reasons to have automated
performance tests as part of your CI—if performance changes drastically
in either direction, you should know about it.
Unlike with functional testing, performance tests do not have a common,
well-defined output. A functional test will either succeed or fail,
whereas a performance test may give you a throughput number, a latency
profile, a number of processed samples, or any other metric that might
be relevant to the application in question. Also, a performance test may
require running a function in a loop a few hundred thousand times, or it
might take hours running across a distributed network of multicore boxes.
For that reason, it is difficult to speak about how to write performance tests
in a general sense. Instead, in this section, we’ll look at some of the issues
you may encounter when writing performance tests in Rust and how to mitigate
them. Three particularly common pitfalls that are often overlooked
are performance variance, compiler optimizations, and I/O overhead. Let’s
explore each of these in turn.
Performance Variance
Performance can vary for a huge variety of reasons, and many factors affect
how fast a particular sequence of machine instructions run. Some are obvious,
like the CPU and memory clock speed, or how loaded the machine otherwise
is, but many are much more subtle. For example, your kernel version may
change paging performance, the length of your username might change the

98 Chapter 6
layout of memory, and the temperature in the room might cause the CPU to
clock down. Ultimately, it is highly unlikely that if you run a benchmark twice,
you’ll get the same result. In fact, you may observe significant variance, even if
you are using the same hardware. Or, viewed from another perspective, your
code may have gotten slower or faster, but the effect may be invisible due to
differences in the benchmarking environment.
There are no perfect ways to eliminate all variance in your performance
results, unless you happen to be able to run benchmarks repeatedly
on a highly diverse fleet of machines. Even so, it’s important to try to
handle this measurement variance as best we can to extract a signal from
the noisy measurements benchmarks give us. In practice, our best friend in
combating variance is to run each benchmark many times and then look
at the distribution of measurements rather than just a single one. Rust has
tools that can help with this. For example, rather than ask “How long did
this function take to run on average?” crates like hdrhistogram enable us to
look at statistics like “What range of runtime covers 95% of the samples
we observed?” To be even more rigorous, we can use techniques like null
hypothesis testing from statistics to build some confidence that a measured
difference indeed corresponds to a true change and is not just noise.
A lecture on statistical hypothesis testing is beyond the scope of this
book, but luckily much of this work has already been done by others. The
criterion crate, for instance, does all of this and more for you. All you
have to do is give it a function that it can call to run one iteration of your
benchmark, and it will run it the appropriate number of times to be fairly
sure that the result is reliable. It then produces a benchmark report, which
includes a summary of the results, analysis of outliers, and even graphical
representations of trends over time. Of course, it can’t eliminate the effects
of just testing on a particular configuration of hardware, but it at least categorizes
the noise that is measurable across executions.
Compiler Optimizations
Compilers these days are really clever. They eliminate dead code, compute
complex expressions at compile time, unroll loops, and perform other dark
magic to squeeze every drop of performance out of our code. Normally this
is great, but when we’re trying to measure how fast a particular piece of
code is, the compiler’s smartness can give us invalid results. For example,
take the code to benchmark Vec::push in Listing 6-9.
let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
vs.push(i);
}
println!("took {:?}", start.elapsed());
Listing 6-9: A suspiciously fast performance benchmark

Testing 99
If you were to look at the assembly output of this code compiled in
release mode using something like the excellent godbolt.org or cargo-asm,
you’d immediately notice that something was wrong: the calls to Vec::with
_capacity and Vec::push, and indeed the whole for loop, are nowhere to be
seen. They have been optimized out completely. The compiler realized
that nothing in the code actually required the vector operations to be performed
and eliminated them as dead code. Of course, the compiler is completely
within its rights to do so, but for benchmarking purposes, this is not
particularly helpful.
To avoid these kinds of optimizations for benchmarking, the standard
library provides std::hint::black_box. This function has been the topic of
much debate and confusion and is still pending stabilization at the time of
writing, but is so useful it’s worth discussing here nonetheless. At its core,
it’s simply an identity function (one that takes x and returns x) that tells the
compiler to assume that the argument to the function is used in arbitrary
(legal) ways. It does not prevent the compiler from applying optimizations
to the input argument, nor does it prevent the compiler from optimizing
how the return value is used. Instead, it encourages the compiler to actually
compute the argument to the function (under the assumption that it will
be used) and to store that result somewhere accessible to the CPU such that
black_box could be called with the computed value. The compiler is free to,
say, compute the input argument at compile time, but it should still inject
the result into the program.
This function is all we need for many, though admittedly not all, of our
benchmarking needs. For example, we can annotate Listing 6-9 so that the
vector accesses are no longer optimized out, as shown in Listing 6-10.
let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
black_box(vs.as_ptr());
vs.push(i);
black_box(vs.as_ptr());
}
println!("took {:?}", start.elapsed());
Listing 6-10: A corrected version of Listing 6-9
We’ve told the compiler to assume that vs is used in arbitrary ways
on each iteration of the loop, both before and after the calls to push. This
forces the compiler to perform each push in order, without merging or otherwise
optimizing consecutive calls, since it has to assume that “arbitrary
stuff that cannot be optimized out” (that’s the black_box part) may happen
to vs between each call.
Note that we used vs.as_ptr() and not, say, &vs. That’s because of the
caveat that the compiler should assume black_box can perform any legal operation
on its argument. It is not legal to mutate the Vec through a shared reference,
so if we used black_box(&vs), the compiler might notice that vs will not
change between iterations of the loop and implement optimizations based on
that observation!

100 Chapter 6
I/O Overhead Measurement
When writing benchmarks, it’s easy to accidentally measure the wrong
thing. For example, we often want to get information in real time about
how far along the benchmark is. To do that, we might write code like that in
Listing 6-11, intended to measure how fast my_function runs:
let start = std::time::Instant::now();
for i in 0..1_000_000 {
println!("iteration {}", i);
my_function();
}
println!("took {:?}", start.elapsed());
Listing 6-11: What are we really benchmarking here?
This may look like it achieves the goal, but in reality, it does not actually
measure how fast my_function is. Instead, this loop is most likely to tell
us how long it takes to print a million numbers. The println! in the body of
the loop does a lot of work behind the scenes: it turns a binary integer into
decimal digits for printing, locks standard output, writes out a sequence
of UTF-8 code points using at least one system call, and then releases the
standard output lock. Not only that, but the system call might block if your
terminal is slow to print out the input it receives. That’s a lot of cycles! And
the time it takes to call my_function might pale in comparison.
A similar thing happens when your benchmark uses random numbers.
If you run my_function(rand::random()) in a loop, you may well be mostly measuring
the time it takes to generate a million random numbers. The story is
the same for getting the current time, reading a configuration file, or starting
a new thread—these things all take a long time, relatively speaking, and
may end up overshadowing the time you actually wanted to measure.
Luckily, this particular issue is often easy to work around once you are
aware of it. Make sure that the body of your benchmarking loop contains
almost nothing but the particular code you want to measure. All other code
should run either before the benchmark begins or outside of the measured
part of the benchmark. If you’re using criterion, take a look at the different
timing loops it provides—they’re all there to cater to benchmarking cases
that require different measurement strategies!
Summary
In this chapter, we explored the built-in testing capabilities that Rust
offers in great detail. We also looked at a number of testing facilities and
techniques that are useful when testing Rust code. This is the last chapter
that focuses on higher-level aspects of intermediate Rust use in this book.
Starting with the next chapter on declarative and procedural macros, we
will be focusing much more on Rust code. See you on the next page!


### 7.MACROS

Macros are, in essence, a tool for making
the compiler write code for you. You give
the compiler a formula for generating code
given some input parameters, and the compiler
replaces every invocation of the macro with the result
of running through the formula. You can think of
macros as automatic code substitution where you get
to define the rules for the substitution.
Rust’s macros come in many different shapes and sizes to make it easy
to implement many different forms of code generation. The two primary
types are declarative macros and procedural macros, and we will explore both
of them in this chapter. We’ll also look at some of the ways macros can come
in handy in your everyday coding and some of the pitfalls that arise with
more advanced use.
Programmers coming from C-based languages may be used to the
unholy land of C and C++ where you can use #define to change each true to
false, or to remove all occurrences of the else keyword. If that’s the case for


102 Chapter 7
you, you’ll need to disassociate macros from a feeling of doing something
“bad.” Macros in Rust are far from the Wild West of C macros. They follow
(mostly) well-defined rules and are fairly misuse-resistant.
Declarative Macros
Declarative macros are those defined using the macro_rules! syntax, which
lets you conveniently define function-like macros without having to resort to
writing a dedicated crate for the purpose (as you do with procedural macros).
Once you’ve defined a declarative macro, you can invoke it using the name
of the macro followed by an exclamation mark. I like to think of this kind
of macro as a sort of compiler-assisted search and replace: it does the job
for many regular, well-structured transformation tasks, and for eliminating
repetitive boilerplate. In your experience with Rust up until this point, most
of the macros you have recognized as macros are likely to have been declarative
macros. Note, however, that not all function-like macros are declarative
macros; macro_rules! itself is one example of this, and format_args! is another.
The ! suffix merely indicates to the compiler that the macro invocation will
be replaced with different source code at compile time.
NOTE Since Rust’s parser specifically recognizes and parses macro invocations annotated
with !, you can use them only in places where the parser allows them. They work in
most places you’d expect, like in expression position or in an impl block, but not everywhere.
For example, you cannot (at the time of writing) invoke a function-like macro
where an identifier or match arm is expected.
It may not be immediately obvious why declarative macros are called
declarative. After all, don’t you “declare” everything in your program? In
this context, declarative refers to the fact that you don’t say how the macro’s
inputs should be translated into the output, just that you want the output to
look like A when the input is B. You declare that it shall be so, and the compiler
figures out all the parsing rewiring that has to happen to make your
declaration reality. This makes declarative macros concise and expressive,
though it also has a tendency to make them rather cryptic since you have a
limited language with which to express your declarations.
When to Use Them
Declarative macros are primarily useful when you find yourself writing the
same code over and over, and you’d like to, well, not do that. They’re best
suited for fairly mechanical replacements—if you’re aiming to do fancy
code transformations or lots of code generation, procedural macros are
likely a better fit.
I most frequently use declarative macros in cases where I find myself
writing repetitive and structurally similar code, such as in tests and trait

Macros 103
implementations. For tests, I often want to run the same test multiple times
but with slightly different configurations. I might have something like what
is shown in Listing 7-1.
fn test_inner<T>(init: T, frobnify: bool) { ... }
#[test]
fn test_1u8_frobnified() {
test_inner(1u8, true);
}
// ...
#[test]
fn test_1i128_not_frobnified() {
test_inner(1i128, false);
}
Listing 7-1: Repetitive testing code
While this works, it’s too verbose, too repetitive, and too prone to manual
error. With macros we can do much better, as shown in Listing 7-2.
macro_rules! test_battery {
($($t:ty as $name:ident),*)) => {
$(
mod $name {
#[test]
fn frobnified() { test_inner::<$t>(1, true) }
#[test]
fn unfrobnified() { test_inner::<$t>(1, false) }
}
)*
}
}
test_battery! {
u8 as u8_tests,
// ...
i128 as i128_tests
);
Listing 7-2: Making a macro repeat for you
This macro expands each comma-separated directive into its own module
that then contains two tests, one that calls test_inner with true, and one
with false. While the macro definition isn’t trivial, it makes adding more
tests much easier. Each type is one line in the test_battery! invocation, and
the macro will take care of generating tests for both true and false arguments.
We could also have it generate tests for different values for init.
We’ve now significantly reduced the likelihood that we’ll forget to test a
particular
configuration!
The story for trait implementations is similar. If you define your own
trait, you’ll often want to implement that trait for a number of types in the
standard library, even if those implementations are trivial. Let’s imagine
you invented the Clone trait and want to implement it for all the Copy types

104 Chapter 7
in the standard library. Instead of manually writing an implementation for
each one, you can use a macro like the one in Listing 7-3.
macro_rules! clone_from_copy {
($($t:ty),*) => {
$(impl Clone for $t {
fn clone(&self) -> Self { *self }
})*
}
}
clone_from_copy![bool, f32, f64, u8, i8, /* ... */];
Listing 7-3: Using a macro to implement a trait for many similar types in one fell swoop
Here, we generate an implementation of Clone for each provided type
whose body just uses * to copy out of &self. You may wonder why we don’t
add a blanket implementation of Clone for T where T: Copy. We could do
that, but a big reason not to is that it would force types in other crates to
also use that same implementation of Clone for their own types that happen
to be Copy. An experimental compiler feature called specialization could offer
a workaround, but at the time of writing the stabilization of that feature
is still some way off. So, for the time being, we’re better off enumerating
the types specifically. This pattern also extends beyond simple forwarding
implementations: for example, you could easily alter the code in Listing 7-3
to implement an AddOne trait to all integer types!
NOTE If you ever find yourself wondering if you should use generics or a declarative macro,
you should use generics. Generics are generally more ergonomic than macros and
integrate much better with other constructs in the language. Consider this rule of
thumb: if your code changes based on type, use generics; otherwise, use macros.
How They Work
Every programming language has a grammar that dictates how the individual
characters that make up the source code can be turned into tokens.
Tokens are the lowest-level building blocks of a language, such as numbers,
punctuation characters, string and character literals, and identifiers; at
this level, there’s no distinction between language keywords and variable
names. For example, the text (value + 4) would be represented by the fivetoken
sequence (, value, +, 4, ) in Rust-like grammar. The process of turning
text into tokens also provides a layer of abstraction between the rest of the
compiler and the gnarly low-level details of parsing text. For example, in
the token representation, there is no notion of whitespace, and /*"foo"*/
and "/*foo*/" have distinct representations (the former is no token, and the
latter is a string literal token with the content /*foo*/).
Once the source code has been turned into a sequence of tokens, the
compiler walks that sequence and assigns syntactic meaning to the tokens. For
example, ()-delimited tokens make up a group, ! tokens denote macro invocations,
and so on. This is the process of parsing, which ultimately produces
an abstract syntax tree (AST) that describes the structure represented by the

Macros 105
source code. As an example, consider the expression let x = || 4, which consists
of the sequence of tokens let (keyword), x (identifier), = (punctuation),
two instances of | (punctuation), and 4 (literal). When the compiler turns that
into a syntax tree, it represents it as a statement whose pattern is the identifier x
and whose right-hand expression is a closure that has an empty argument list and a
literal expression of the integer literal 4 as its body. Notice how the syntax tree representation
is much richer than the token sequence, since it assigns syntactic
meaning to the token combinations following the language’s grammar.
Rust macros dictate the syntax tree that a given sequence of tokens gets
turned into—when the compiler encounters a macro invocation during
parsing, it has to evaluate the macro to determine the replacement tokens,
which will ultimately become the syntax tree for the macro invocation. At
this point, however, the compiler is still parsing the tokens and might not be
in a position to evaluate a macro yet, since all it has done is parse the tokens
of the macro definition. Instead, then, the compiler defers the parsing of
anything contained within the delimiters of a macro invocation and remembers
the input token sequence. When the compiler is ready to evaluate the
indicated macro, it evaluates the macro over the token sequence, parses the
tokens it yields, and substitutes the resulting syntax tree into the tree where
the macro invocation was.
Technically, the compiler does do a little bit of parsing for the input
to a macro. Specifically, it parses out basic things like string literals and
delimited groups and so produces a sequence of token trees rather than just
tokens. For example, the code x - (a.b + 4) parses as a sequence of three
token trees. The first token tree is a single token that is the identifier x, the
second is a single token that is the punctuation character -, and the third
is a group (using parentheses as the delimiter), which itself consists of a
sequence of five token trees: a (an identifier), . (punctuation), b (another
identifier), + (another punctuation token), and 4 (a literal). This means
that the input to a macro does not necessarily have to be valid Rust, but it
must consist of code that the Rust compiler can parse. For example, you
couldn’t write for <- x in Rust outside of a macro invocation, but inside of
a macro invocation you can, as long as the macro produces valid syntax.
On the other hand, you cannot pass for { to a macro because it doesn’t
have a closing brace.
Declarative macros always generate valid Rust as output. You cannot
have a macro generate, say, the first half of a function invocation or an if
without the block that follows it. A declarative macro must generate an
expression (basically anything that you can assign to a variable), a statement
such as let x = 1;, an item like a trait definition or impl block, a type, or a
match pattern. This makes Rust macros resistant to misuse: you simply cannot
write a declarative macro that generates invalid Rust code, because the
macro definition itself would not compile!
That’s really all there is to declarative macros at a high level—when
the compiler encounters a macro invocation, it passes the tokens contained
within the invocation delimiters to the macro, parses the resulting token
stream, and replaces the macro invocation with the resulting AST.

106 Chapter 7
How to Write Declarative Macros
An exhaustive explanation of all the syntax that declarative macros support
is outside the scope of this book. However, we’ll cover the basics as there are
some oddities worth pointing out.
Declarative macros consist of two main parts: matchers and transcribers.
A given macro can have many matchers, and each matcher has an associated
transcriber. When the compiler finds a macro invocation, it walks
the macro’s
matchers from first to last, and when it finds a matcher that
matches the tokens in the invocation, it substitutes the invocation by walking
the tokens of the corresponding transcriber. Listing 7-4 shows how the
different parts of a declarative macro rule fit together.
macro_rules! /* macro name */ {
(/* 1st matcher */) => { /* 1st transcriber */ };
(/* 2nd matcher */) => { /* 2nd transcriber */ };
}
Listing 7-4: Declarative macro definition components
Matchers
You can think of a macro matcher as a token tree that the compiler tries to
twist and bend in predefined ways to match the input token tree it was given
at the invocation site. As an example, consider a macro with the matcher
$a:ident + $b:expr. That matcher will match any identifier (:ident) followed
by a plus sign followed by any Rust expression (:expr). If the macro is invoked
with x + 3 * 5, the compiler notices that the matcher matches if it sets $a = x
and $b = 3 * 5. Even though * never appears in the matcher, the compiler
realizes that 3 * 5 is a valid expression and that it can therefore be matched
with $b:expr, which accepts anything that is an expression (the :expr part).
Matchers can get pretty hairy, but they have huge expressive power,
much like regular expressions. For a not-too-hairy example, this matcher
accepts a sequence ($()) of one or more (+) comma-separated (),) key/value
pairs given in key => value format:
$($key:expr => $value:expr),+
And, crucially, code that invokes a macro with this matcher can give an
arbitrarily complex expression for the key or value—the magic of matchers
will make sure that the key and value expressions are partitioned
appropriately.
Macro rules support a wide variety of fragment types; you’ve already seen
:ident for identifiers and :expr for expressions, but there is also :ty for types
and even :tt for any single token tree! You can find a full list of the fragment
types in Chapter 3 of the Rust language reference (https://doc.rust-lang
.org/reference/macros-by-example.html). These, plus the mechanism for matching
a pattern repeatedly ($()), enable you to match most straightforward
code patterns. If, however, you find that it is difficult to express the pattern
you want with a matcher, you may want to try a procedural macro instead,


Macros 107
where you don’t need to follow the strict syntax that macro_rules! requires.
We’ll look at these in more detail later in the chapter.
Transcribers
Once the compiler has matched a declarative macro matcher, it generates
code using the matcher’s associated transcriber. The variables defined by
a macro matcher are called metavariables, and the compiler substitutes any
occurrence of each metavariable in the transcriber (like $key in the example
in the previous section) with the input that matches that part of the matcher.
If you have repetition in the matcher (like $(),+ in that same example), you
can use the same syntax in the transcriber and it will be repeated once for
each match in the input, with each expansion holding the appropriate substitution
for each metavariable for that iteration. For example, for the $key
and $value matcher, we could write the following transcriber to generate an
insert call into some map for each $key/$value pair that was matched:
$(map.insert($key, $value);)+
Notice that here we want a semicolon for each repetition, not just to delimit
the repetition, so we place the semicolon inside the repetition parentheses.
NOTE You must use a metavariable in each repetition in the transcriber so that the compiler
knows which repetition in the matcher to use (in case there is more than one).
Hygiene
You may have heard that Rust macros are hygienic, and perhaps that being
hygienic makes them safer or nicer to work with, without necessarily understanding
what that means. When we say Rust macros are hygienic, we mean
that a declarative macro (generally) cannot affect variables that aren’t explicitly
passed to it. A trivial example is that if you declare a variable with the
name foo, and then call a macro that also defines a variable named foo,
the macro’s foo is by default not visible at the call site (the place where the
macro is called from). Similarly, macros cannot access variables defined at
the call site (even self) unless they are explicitly passed in.
You can, most of the time, think of macro identifiers as existing in their
own namespace that is separate from that of the code they expand into. For
an example, take a look at the code in Listing 7-5, which has a macro that
tries (and fails) to shadow a variable at the call site.
macro_rules! let_foo {
($x:expr) => {
let foo = $x;
}
}
let foo = 1;
// expands to let foo = 2;
let_foo!(2);
assert_eq!(foo, 1);
Listing 7-5: Macros exist in their own little universes. Mostly.


108 Chapter 7
After the compiler expands let_foo!(2), the assert looks like it should
fail. However, the foo from the original code and the one generated by the
macro exist in different universes and have no relationship to one another
beyond that they happen to share a human-readable name. In fact, the
compiler will complain that the let foo in the macro is an unused variable.
This hygiene is very helpful in making macros easier to debug—you
don’t have to worry about accidentally shadowing or overwriting variables
in the macro caller just because you happened to choose the same variable
names!
This hygienic separation does not apply beyond variable identifiers,
however. Declarative macros do share a namespace for types, modules, and
functions with the call site. This means your macro can define new functions
that can be called in the invoking scope, add new implementations to
a type defined elsewhere (and not passed in), introduce a new module that
can then be accessed where the macro was invoked, and so on. This is by
design—if macros could not affect the broader code like this, it would be
much more cumbersome to use them to generate types, trait implementations,
and functions, which is where they come in most handy.
The lack of hygiene for types in macros is particularly important when
writing a macro you want to export from your crate. For the macro to truly
be reusable, you cannot assume anything about what types will be in scope
at the caller. Maybe the code that calls your macro has a mod std {} defined
or has imported its own Result type. To be on the safe side, make sure you
use fully specified types like ::core::option::Option or ::alloc::boxed::Box.
If you specifically need to refer to something in the crate that defines the
macro, use the special metavariable $crate.
NOTE Avoid using ::std paths if you can so that the macro will continue to work in
no_std crates.
You can choose to share identifiers between a macro and its caller if you
want the macro to affect a specific variable in the caller’s scope. The key is
to remember where the identifier originated, because that’s the namespace
the identifier will be tied to. If you put let foo = 1; in a macro, the identifier
foo originates in the macro and will never be available to the identifier
namespace at the caller. If, on the other hand, the macro takes $foo:ident
as an argument and then writes let $foo = 1;, when the caller invokes the
macro with !(foo), the identifier will have originated in the caller and will
therefore refer to foo in the caller’s scope.
The identifier does not have to be quite so explicitly passed, either; any
identifier that appears in code that originates outside the macro will refer
to the identifier in the caller’s scope. In the example in Listing 7-6, the variable
identifier appears in an :expr but nonetheless has access to the variable
in the caller’s scope.
macro_rules! please_set {
($i:ident, $x:expr) => {
$i = $x;
}

Macros 109
}
let mut x = 1;
please_set!(x, x + 1);
assert_eq!(x, 2);
Listing 7-6: Giving macros access to identifiers at the call site
We could have used = $i + 1 in the macro instead, but we could not
have used = x + 1 as the name x is not available in the macro’s definition
scope.
One last note on declarative macros and scoping: unlike pretty much
everything else in Rust, declarative macros exist in the source code only after
they are declared. If you try to use a macro that you define further down in
the file, this will not work! This applies globally to your project; if you declare
a macro in one module and want to use it in another, the module you declare
the macro in must appear earlier in the crate, not later. If foo and bar are modules
at the root of a crate, and foo declares a macro that bar wants to use, then
mod foo must appear before mod bar in lib.rs!
NOTE There is one exception to this odd scoping of macros (formally called textual scoping),
and that is if you mark the macro with #[macro_export]. That annotation effectively
hoists the macro to the root of the crate and marks it as pub so that it can then be
used anywhere in your crate or by your crate’s dependents.
Procedural Macros
You can think of a procedural macro as a combination of a parser and code
generation, where you write the glue code in between. At a high level, with
procedural macros, the compiler gathers the sequence of input tokens to
the macro and runs your program to figure out what tokens to replace
them with.
Procedural macros are so called because you define how to generate
code given some input tokens rather than just writing what code gets generated.
There are very few smarts involved on the compiler’s side—as far as it is
aware, the procedural macro is more or less a source code preprocessor that
may replace code arbitrarily. The requirement that your input can be parsed
as a stream of Rust tokens still holds, but that’s about it!
Types of Procedural Macros
Procedural macros come in three different flavors, each specialized to a
particular common use case:
• Function-like macros, like the ones that macro_rules! generates
• Attribute macros, like #[test]
• Derive macros, like #[derive(Serialize)]
All three types use the same underlying mechanism: the compiler provides
your macro with a sequence of tokens, and it expects you to produce


110 Chapter 7
a sequence of tokens in return that are (probably) related to the input tree.
However, they differ in how the macro is invoked and how its output is handled.
We’ll cover each one briefly.
Function-Like Macros
The function-like macro is the simplest form of procedural macro. Like a
declarative macro, it simply replaces the macro code at the call site with the
code that the procedural macro returns. However, unlike with declarative
macros, all the guard rails are off: these macros (like all procedural macros)
are not required to be hygienic and will not protect you from interacting
with identifiers in the surrounding code at the call site. Instead, your
macros are expected to explicitly call out which identifiers should overlap
with the surrounding code (using Span::call_site) and which should be
treated as private to the macro (using Span::mixed_site, which we’ll discuss
later).
Attribute Macros
The attribute macro also replaces the item that the attribute is assigned to
wholesale, but this one takes two inputs: the token tree that appears in the
attribute (minus the attribute’s name) and the token tree of the entire item
it is attached to, including any other attributes that item may have. Attribute
macros allow you to easily write a procedural macro that transforms an
item, such as by adding a prelude or epilogue to a function definition (like
#[test] does) or by modifying the fields of a struct.
Derive Macros
The derive macro is slightly different from the other two in that it adds
to, rather than replaces, the target of the macro. Even though this limitation
may seem severe, derive macros were one of the original motivating
factors behind the creation of procedural macros. Specifically, the serde
crate needed derive macros to be able to implement its now-well-known
#[derive(Serialize, Deserialize)] magic.
Derive macros are arguably the simplest of the procedural macros,
since they have such a rigid form: you can append items only after the
annotated item; you can’t replace the annotated item, and you cannot have
the derivation take arguments. Derive macros do allow you to define helper
attributes—attributes that can be placed inside the annotated type to give
clues to the derive macro (like #[serde(skip)])—but these function mostly
like markers and are not independent macros.
The Cost of Procedural Macros
Before we talk about when each of the different procedural macro types is
appropriate, it’s worth discussing why you may want to think twice before
you reach for a procedural macro—namely, increased compile time.
Procedural macros can significantly increase compile times for two
main reasons. The first is that they tend to bring with them some pretty


Macros 111
heavy dependencies. For example, the syn crate, which provides a parser
for Rust token streams that makes the experience of writing procedural
macros much easier, can take tens of seconds to compile with all features
enabled. You can (and should) mitigate this by disabling features you do
not need and compiling your procedural macros in debug mode rather
than release mode. Code often compiles several times faster in debug
mode, and for most procedural macros, you won’t even notice the difference
in execution time.
The second reason why procedural macros increase compile time is
that they make it easy for you to generate a lot of code without realizing it.
While the macro saves you from having to actually type the generated code,
it does not save the compiler from having to parse, compile, and optimize
it. As you use more procedural macros, that generated boilerplate adds up,
and it can bloat your compile times.
That said, the actual execution time of procedural macros is rarely a
factor in overall compile time. While the compiler has to wait for the procedural
macro to do its thing before it can continue, in practice, most procedural
macros don’t do any heavy computation. That said, if your procedural
macro is particularly involved, you may end up with your compiles spending
a significant chunk of execution time on your procedural macro code,
which is worth keeping an eye out for!
So You Think You Want a Macro
Let’s now look at some good uses for each type of procedural macro. We’ll
start with the easy one: derive macros.
When to Use Derive Macros
Derive macros are used for one thing, and one thing only: to automate the
implementation of a trait where automation is possible. Not all traits have
obvious automated implementations, but many do. In practice, you should
consider adding a derive macro for a trait only if the trait is implemented
often and if its implementation for any given type is fairly obvious. The first
of these conditions may seem like common sense; if your trait is going to be
implemented only once or twice, it’s probably not worth writing and maintaining
a convoluted derive macro for it.
The second condition may seem stranger, however: what does it mean
for the implementation to be “obvious”? Consider a trait like Debug. If you
were told what Debug does and were shown a type, you would probably
expect an implementation of Debug to output the name of each field alongside
the debug representation of its value. And that’s what derive(Debug)
does. What about Clone? You’d probably expect it to just clone every field—
and again, that’s what derive(Clone) does. With derive(serde::Serialize), we
expect it to serialize every field and its value, and it does just that. In general,
you want the derivation of a trait to match the developer’s intuition for
what it probably does. If there is no obvious derivation for a trait, or worse
yet, if your derivation does not match the obvious implementation, then
you’re probably better off not giving it a derive macro.

112 Chapter 7
When to Use Function-Like Macros
Function-like macros are harder to give a general rule of thumb for. You
might say you should use function-like macros when you want a functionlike
macro but can’t express it with macro_rules!, but that’s a fairly subjective
guideline. You can do a lot with declarative macros if you really put your
mind to it, after all!
There are two particularly good reasons to reach for a function-like
macro:
• If you already have a declarative macro, and its definition is becoming
so hairy that the macro is hard to maintain.
• If you have a pure function that you need to be able to execute at compile
time but cannot express it with const fn. An example of this is the phf
crate, which generates a hash map or set using a perfect hash function
when given a set of keys provided at compile time. Another is hex-
literal,
which takes a string of hexadecimal characters and replaces it with the
corresponding bytes. In general, anything that does not merely transform
the input at compile time but actually computes over it is likely to
be a good candidate.
I do not recommend reaching for a function-like macro just so that you
can break hygiene within your macro. Hygiene for function-like macros is a
feature that avoids many debugging headaches, and you should think very
carefully before you intentionally break it.
When to Use Attribute Macros
That leaves us with attribute macros. Though these are arguably the most
general of procedural macros, they are also the hardest to know when to
use. Over the years and time and time again, I have seen four ways in which
attribute macros add tremendous value.
Test generation
It is very common to want to run the same test under multiple different
configurations, or many similar tests with the same bootstrapping code.
While a declarative macro may let you express this, your code is often
easier to read and maintain if you have an attribute like #[foo_test] that
introduces a setup prelude and postscript in each annotated test, or a
repeatable attribute like #[test_case(1)] #[test_case(2)] to mark that a
given test should be repeated multiple times, once with each input.
Framework annotations
Libraries like rocket use attribute macros to augment functions and
types with additional information that the framework then uses without
the user having to do a lot of manual configuration. It’s so much more
convenient to be able to write #[get("/<name>")] fn hello(name: String)
than to have to set up a configuration struct with function pointers and

Macros 113
the like. Essentially, the attributes make up a miniature domain-specific
language (DSL) that hides a lot of boilerplate that’d otherwise be necessary.
Similarly, the asynchronous I/O framework tokio lets you use
#[tokio::main] async fn main() to automatically set up a runtime and run
your asynchronous code, thereby saving you from writing the same runtime
setup in every asynchronous application’s main function.
Transparent middleware
Some libraries want to inject themselves into your application in unobtrusive
ways to provide added value that does not change the application’s
functionality. For example, tracing and logging libraries like tracing and
metric collection libraries like metered allow you to transparently instrument
a function by adding an attribute to it, and then every call to that
function will run some additional code dictated by the library.
Type transformers
Sometimes you want to go beyond merely deriving traits for a type and
actually change the type’s definition in some fundamental way. In these
cases, attribute macros are the way to go. The pin_project crate is a great
example of this: its primary purpose is not to implement a particular
trait but rather to ensure that all pinned access to fields of a given type
happens according to the strict rules that are set forth by Rust’s Pin type
and the Unpin trait (we’ll talk more about those types in Chapter 8).
It does this by generating additional helper types, adding methods to
the annotated type, and introducing static safety checks to ensure that
users don’t accidentally shoot themselves in the foot. While pin_project
could have been implemented with a procedural derive macro, that
derived trait implementation would likely not have been obvious, which
violates one of our rules for when to use procedural macros.
How Do They Work?
At the heart of all procedural macros is the TokenStream type, which can be
iterated over to get the individual TokenTree items that make up that token
stream. A TokenTree is either a single token—like an identifier, punctuation,
or a literal—or another TokenStream enclosed in a delimiter like () or {}. By
walking a TokenStream, you can parse out whatever syntax you wish as long as
the individual tokens are valid Rust tokens. If you want to parse your input
specifically as Rust code, you will likely want to use the syn crate, which
implements a complete Rust parser and can turn a TokenStream into an easyto-
traverse Rust AST.
With most procedural macros, you want to not only parse a TokenStream
but also produce Rust code to be injected into the program that invokes the
procedural macro. There are two main ways to do so. The first is to manually
construct a TokenStream and extend it one TokenTree at a time. The second is to
use TokenStream’s implementation of FromStr, which lets you parse a string that


114 Chapter 7
contains Rust code into a TokenStream with "".parse::<TokenStream>(). You can
also mix and match these; if you want to prepend some code to your macro’s
input, just construct a TokenStream for the prologue, and then use the Extend
trait to append the original input.
NOTE TokenStream also implements Display, which pretty-prints the tokens in the stream.
This comes in super handy for debugging!
Tokens are very slightly more magical than I’ve described so far in that
every token, and indeed every TokenTree, also has a span. Spans are how the
compiler ties generated code back to the source code that generated it.
Every token’s span marks where that token originated. For example, consider
a (declarative) macro like the one in Listing 7-7, which generates a
trivial Debug implementation for the provided type.
macro_rules! name_as_debug {
($t:ty) => {
impl ::core::fmt::Debug for $t {
fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result
{ ::core::write!(f, ::core::stringify!($t)) }
} }; }
Listing 7-7: A very simple macro for implementing Debug
Now let’s imagine that someone invokes this macro with name_as_debug!
(u31). Technically, the compiler error occurs inside the macro, specifically
where we write for $t (the other use of $t can handle an invalid type). But
we’d like the compiler to point the user at the u31 in their code—and indeed,
that’s what spans let us do.
The span of the $t in the generated code is the code mapped to $t in
the macro invocation. That information is then carried through the compiler
and associated with the eventual compiler error. When that compiler
error is eventually printed, the compiler will print the error from inside the
macro saying that the type u31 does not exist but will highlight the u31 argument
in the macro invocation, since that’s the error’s associated span!
Spans are quite flexible, and they enable you to write procedural
macros that can produce sophisticated error messages if you use the
compile_
error! macro. As its name implies, compile_error! causes the compiler
to emit an error wherever it is placed with the provided string as the
message. This may not seem very useful, until you pair it with a span. By
setting the span of the TokenTree you generate for the compile_error! invocation
to be equal to the span of some subset of the input, you are effectively
telling the compiler to emit this compiler error and point the user to this
part of the source. Together, these two mechanisms let a macro produce
errors that seem to stem from the relevant part of the code, even though
the actual compiler error is somewhere in the generated code that the
user never even sees!


Macros 115
NOTE If you’ve ever been curious how syn’s error handling works, its Error type implements
an Error::to_compile_error method, which turns it into a TokenStream that
holds only a compile_error! directive. What’s particularly neat with syn’s Error type
is that it internally holds a collection of errors, each of which produces a distinct
compile_
error! directive with its own span so that you can easily produce multiple
independent errors from your procedural macro.
The power of spans doesn’t end there; spans are also how Rust’s macro
hygiene is implemented. When you construct an Ident token, you also give
the span for that identifier, and that span dictates the scope of that identifier.
If you set the identifier’s span to be Span::call_site(), the identifier
is resolved where the macro was called from and will thus not be isolated
from the surrounding scope. If, on the other hand, you set it to Span::mixed
_site() then (variable) identifiers are resolved at the macro definition site,
and so will be completely hygienic with respect to similarly named variables
at the call site. Span::mixed_site is so called because it matches the rules
around identifier hygiene for macro_rules!, which, as we discussed earlier,
“mixes” identifier resolution between using the macro definition site for
variables and using the call site for types, modules, and everything else.
Summary
In this chapter we covered both declarative and procedural macros, and
looked at when you might find each of them useful in your own code. We
also took a deeper dive into the mechanisms that underpin each type of
macro and some of the features and gotchas to be aware of when you write
your own macros. In the next chapter, we’ll start our journey into asynchronous
programming and the Future trait. I promise—it’s just on
the next page.


### 8.ASYNCHRONOUS PROGRAMMING

Asynchronous programming is, as the
name implies, programming that is not
synchronous.
At a high level, an asynchronous
operation is one that executes in the
background—the program won’t wait for the asynchronous
operation to complete but will instead
continue
to the next line of code immediately.
If you’re not already familiar with asynchronous programming, that definition
may feel insufficient as it doesn’t actually explain what asynchronous
programming is. To really understand the asynchronous programming
model and how it works in Rust, we have to first dig into what the alternative
is. That is, we need to understand the synchronous programming
model before we can understand the asynchronous one. This is important
in both clarifying the concepts and demonstrating the trade-offs of using
asynchronous programming: an asynchronous solution is not always the
right one! We’ll start this chapter by taking a quick journey through what


118 Chapter 8
motivates asynchronous programming as a concept in the first place; then
we’ll dig into how asynchrony in Rust actually works under the hood.
What’s the Deal with Asynchrony?
Before we get to the details of the synchronous and asynchronous programming
models, we first need to take a quick look at what your computer is
actually doing when it runs your programs.
Computers are fast. Really fast. So fast, in fact, that they spend most of
their time waiting for things to happen. Unless you’re decompressing files,
encoding audio, or crunching numbers, chances are that your CPU mostly
sits idle, waiting for operations to complete. It’s waiting for a network packet
to arrive, for the mouse to move, for the disk to finish writing some bytes, or
maybe even just for a read from main memory to complete. From the CPU’s
perspective, eons go by between most such events. When one does occur, the
CPU runs a few more instructions, then goes back to waiting again. Take a
look at your CPU utilization—it’s probably somewhere in the low single digits,
and that’s likely where it hovers the majority of the time.
Synchronous Interfaces
Synchronous interfaces allow your program (or rather, a single thread in
your program) to execute only a single operation at a time; each operation
has to wait for the previous synchronous operation to finish before it gets
to run. Most interfaces you see in the wild are synchronous: you call them,
they go do some stuff, and eventually they return when the operation has
completed and your program can continue from there. The reason for this,
as we’ll see later in this chapter, is that making an operation asynchronous
takes a fair bit of extra machinery. Unless you need the benefits of asynchrony,
sticking to the synchronous model requires much less pomp and
circumstance.
Synchronous interfaces hide all this waiting; the application calls a
function that says “write these bytes to this file,” and some time later, that
function completes and the next line of code executes. Behind the scenes,
what really happens is that the operating system queues up a write operation
to the disk and then puts the application to sleep until the disk reports
that it has finished the write. The application experiences this as the function
taking a long time to execute, but in reality it isn’t really executing at
all, just waiting.
An interface that performs operations sequentially in this way is also
often referred to as blocking, since the operation in the interface that has
to wait for some external event to happen in order for it to make progress
blocks further execution until that event happens. Whether you refer to an
interface as synchronous or blocking, the basic idea is the same: the application
does not move on until the current operation finishes. While the
operation is waiting, so is the application.
Synchronous interfaces are usually considered to be easy to work with
and simple to reason about, since your code executes just one line at a time.

Asynchronous Programming 119
But they also allow the application to do only one thing at a time. That
means if you want your program to wait for either user input or a network
packet, you’re out of luck unless your operating system provides an operation
specifically for that. Similarly, even if your application could do some
other useful work while the disk is writing a file, it doesn’t have that option
as the file write operation blocks the execution!
Multithreading
By far the most common solution to allowing concurrent execution is to use
multithreading. In a multithreaded program, each thread is responsible for
executing a particular independent sequence of blocking operations, and
the operating system multiplexes among the threads so that if any thread can
make progress, progress is made. If one thread blocks, some other thread
may still be runnable, and so the application can continue to do useful
work.
Usually, these threads communicate with each other using a synchronization
primitive like a lock or a channel so that the application can still coordinate
their efforts. For example, you might have one thread that waits for
user input, one thread that waits for network packets, and another thread
that waits for either of those threads to send a message on a channel shared
between all three threads.
Multithreading gives you concurrency—the ability to have multiple independent
operations that can be executed at any one time. It’s up to the system
running the application (in this case, the operating system) to choose
among the threads that aren’t blocked and decide which to execute next.
If one thread is blocked, it can choose to run another one that can make
progress instead.
Multithreading combined with blocking interfaces gets you quite far,
and large swaths of production-ready software are built in this way. But this
approach is not without its shortcomings. First, keeping track of all these
threads quickly gets cumbersome; if you have to spin up a thread for every
concurrent task, including simple ones like waiting for keyboard input, the
threads add up fast, and so does the additional complexity needed to keep
track of how all those threads interact, communicate, and coordinate.
Second, switching between threads gets costly the more of them there
are. Every time one thread stops running and another one starts back up
in its place, you need to do a round-trip to the operating system scheduler,
and that’s not free. On some platforms, spawning new threads is also a fairly
heavyweight process. Applications with high performance needs often mitigate
this cost by reusing threads and using operating system calls that allow
you to block on many related operations, but ultimately you are left with the
same problem: blocking interfaces require that you have as many threads as
the number of blocking calls you want to make.
Finally, threads introduce parallelism into your program. The distinction
between concurrency and parallelism is subtle, but important: concurrency
means that the execution of your tasks is interleaved, whereas
parallelism means that multiple tasks are executing at the same time. If you
have two tasks, their execution expressed in ASCII might look like _-_-_

120 Chapter 8
(concurrency) versus ===== (parallelism). Multithreading does not necessarily
imply parallelism—even though you have many threads, you might have
only a single core, so only one thread is executing at a given time—but the
two usually go hand in hand. You can make two threads mutually exclusive
in their execution by using a Mutex or other synchronization primitive, but
that introduces additional complexity—threads want to run in parallel.
And while parallelism is often a good thing—who doesn’t want their program
to run faster on more cores—it also means that your program must
handle truly simultaneous access to shared data structures. This means
moving from Rc, Cell, and RefCell to the more powerful but also slower Arc
and Mutex. While you may want to use the latter types in your concurrent
program to enable parallelism, threading forces you to use them. We’ll look
at multithreading in much greater detail in Chapter 10.
Asynchronous Interfaces
Now that we’ve explored synchronous interfaces, we can look at the alternative:
asynchronous or nonblocking interfaces. An asynchronous interface
is one that may not yield a result straightaway, and may instead indicate
that the result will be available at some later time. This gives the caller the
opportunity to do something else in the meantime rather than having to go
to sleep until that particular operation completes. In Rust parlance, an asynchronous
interface is a method that returns a Poll, as defined in Listing 8-1.
enum Poll<T> {
Ready(T),
Pending
}
Listing 8-1: The core of asynchrony: the “here you are or come back later” type
Poll usually shows up in the return type of functions whose names start
with poll—these are methods that signal they can attempt an operation
without blocking. We’ll get into how exactly they do that later in this chapter,
but in general they attempt to perform as much as they can of the operation
before they would normally block, and then return. And crucially,
they remember where they left off so that they can resume execution later
when additional progress can again be made.
These nonblocking functions allow us to easily perform multiple tasks
concurrently. For example, if you want to read from either the network or
the user’s keyboard, whichever has an event available first, all you have to
do is poll both in a loop until one of them returns Poll::Ready. No need for
any additional threads or synchronization!
The word loop here should make you a little nervous. You don’t want your
program to burn through a loop three billion times a second when it may
be minutes until the next input occurs. In the world of blocking interfaces,
this wasn’t a problem since the operating system simply put the thread to
sleep and then took care of waking it up when a relevant event occurred, but
how do we avoid burning cycles while waiting in this brave new nonblocking
world? That’s what much of the remainder of this chapter will be about.

Asynchronous Programming 121
Standardized Polling
To get to a world where every library can be used in a nonblocking fashion,
we could have every library author cook up their own poll methods, all with
slightly different names, signatures, and return types—but that would quickly
get unwieldy. Instead, in Rust, polling is standardized through the Future
trait. A simplified version of Future is shown in Listing 8-2 (we’ll get back to
the real one later in this chapter).
trait Future {
type Output;
fn poll(&mut self) -> Poll<Self::Output>;
}
Listing 8-2: A simplified view of the Future trait
Types that implement the Future trait are known as futures and represent
values that may not be available yet. A future could represent the next
time a network packet comes in, the next time the mouse cursor moves,
or just the point at which some amount of time has elapsed. You can read
Future<Output = Foo> as “a type that will produce a Foo in the future.” Types
like this are often referred to in other languages as promises—they promise
that they will eventually yield the indicated type. When a future eventually
returns Poll::Ready(T), we say that the future resolves into a T.
With this trait in place, we can generalize the pattern of providing poll
methods. Instead of having methods like poll_recv and poll_keypress, we can
have methods like recv and keypress that both return impl Future with an
appropriate Output type. This doesn’t change the fact that you have to poll
them—we’ll deal with that later—but it does mean that at least there is a
standardized interface to these kinds of pending values, and we don’t need
to use the poll_ prefix everywhere.
NOTE In general, you should not poll a future again after it has returned Poll::Ready. If
you do, the future is well within its rights to panic. A future that is safe to poll after it
has returned Ready is sometimes referred to as a fused future.
Ergonomic Futures
Writing a type that implements Future in the way I’ve described so far is
quite a pain. To see why, first take a look at the fairly straightforward asynchronous
code block in Listing 8-3 that simply tries to forward messages
from the input channel rx to the output channel tx.
async fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {
while let Some(t) = rx.next().await {
tx.send(t).await;
}
}
Listing 8-3: Implementing a channel-forwarding future using async and await


122 Chapter 8
This code, written using async and await syntax, looks very similar to its
equivalent synchronous code and is easy to read. We simply send each message
we receive in a loop until there are no more messages, and each await
point corresponds to a place where a synchronous variant might block. Now
think about if you instead had to express this code by manually implementing
the Future trait. Since each call to poll starts at the top of the function,
you’d need to package the necessary state to continue from the last place
the code yielded. The result is fairly grotesque, as Listing 8-4 demonstrates.
enum Forward<T> { 1
WaitingForReceive(ReceiveFuture<T>, Option<Sender<T>>),
WaitingForSend(SendFuture<T>, Option<Receiver<T>>),
}
impl<T> Future for Forward<T> {
type Output = (); 2
fn poll(&mut self) -> Poll<Self::Output> {
match self { 3
Forward::WaitingForReceive(recv, tx) => {
if let Poll::Ready((rx, v)) = recv.poll() {
if let Some(v) = v {
let tx = tx.take().unwrap(); 4
*self = Forward::WaitingForSend(tx.send(v), Some(rx)); 5
// Try to make progress on sending.
return self.poll(); 6
} else {
// No more items.
Poll::Ready(())
}
} else {
Poll::Pending
}
}
Forward::WaitingForSend(send, rx) => {
if let Poll::Ready(tx) = send.poll() {
let rx = rx.take().unwrap();
*self = Forward::WaitingForReceive(rx.receive(), Some(tx));
// Try to make progress on receiving.
return self.poll();
} else {
Poll::Pending
}
}
}
}
}
Listing 8-4: Manually implementing a channel-forwarding future
You’ll rarely have to write code like this in Rust anymore, but it gives
important insight into how things work under the hood, so let’s walk through
it. First, we define our future type as an enum 1, which we’ll use to keep track
of what we’re currently waiting on. This is a consequence of the fact that



Asynchronous Programming 123
when we return Poll::Pending, the next call to poll will start at the top of the
function again. We need some way to know what we were in the middle of so
that we know which operation to continue on. Furthermore, we need to keep
track of different information depending on what we’re doing: if we’re waiting
for a receive to finish, we need to keep that ReceiveFuture (the definition
of which is not shown in this example) so that we can poll it the next time we
are polled ourselves, and the same goes for SendFuture. The Options here might
strike you as weird too; we’ll get back to those shortly.
When we implement Future for Forward, we declare its output type as
() 2 because this future doesn’t actually return anything. Instead, the
future resolves (with no result) when it has finished forwarding everything
from the input channel to the output channel. In a more complete example,
the Output of our forwarding type might be a Result so that it could communicate
errors from receive() and send() back up the stack to the function
that’s polling for the completion of the forwarding. But this code is complicated
enough already, so we’ll leave that for another day.
When Forward is polled, it needs to resume wherever it last left off,
which we find out by matching on the enum variant currently held in
self 3. For whichever branch we go into, the first step is to poll the future
that blocks progress for the current operation; if we’re trying to receive, we
poll the ReceiveFuture, and if we’re trying to send, we poll the SendFuture. If
that call to poll returns Poll::Pending, then we can make no progress, and
we return Poll::Pending ourselves. But if the current future resolves, we
have work to do!
When one of the inner futures resolves, we need to update what the
current operation is by switching which enum variant is stored in self.
In order to do so, we have to move out of self to call Receiver::receive or
Sender::send—but we can’t do that because all we have is &mut self. So, we
store the state we have to move in an Option, which we move out of with
Option::take 4. This is silly since we’re about to overwrite self anyway 5,
and hence the Options will always be Some, but sometimes tricks are needed
to make the borrow checker happy.
Finally, if we do make progress, we then poll self again 6 so that if
we can immediately make progress on the pending send or receive, we do
so. This is actually necessary for correctness when implementing the real
Future trait, which we’ll get back to later, but for now think of this as an
optimization.
We just hand-wrote a state machine: a type that has a number of possible
states and moves between them in response to particular events. This was a
fairly simple state machine, at that. Imagine having to write code like this for
more complicated use cases where you have additional intermediate steps!
Beyond writing the unwieldy state machine, we have to know the types
of the futures that Sender::send and Receiver::receive return so that we can
store them in our type. If those methods instead returned impl Future, we’d
have no way to write out the types for our variants. The send and receive
methods also have to take ownership of the sender and the receiver; if they
did not, the lifetimes of the futures they returned would be tied to the




124 Chapter 8
borrow of self, which would end when we return from poll. But that would
not work, since we’re trying to store those futures in self.
NOTE You may have noticed that Receiver looks a lot like an asynchronous version of
Iterator. Others have noticed the same thing, and the standard library is on its way
to adding a trait specifically for types that can meaningfully implement poll_next.
Down the line, these asynchronous iterators (often referred to as streams) may end up
with first-class language support, such as the ability to loop over them directly!
Ultimately, this code is hard to write, hard to read, and hard to change. If
we wanted to add error handling, for example, the code complexity would
increase significantly. Luckily, there’s a better way!
async/await
Rust 1.39 gave us the async keyword and the closely related await postfix
operator, which we used in the original example in Listing 8-3. Together,
they provide a much more convenient mechanism for writing asynchronous
state machines like the one in Listing 8-5. Specifically, they let you write the
code in such a way that it doesn’t even look like a state machine!
async fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {
while let Some(t) = rx.next().await {
tx.send(t).await;
}
}
Listing 8-5: Implementing a channel-forwarding future using async and await, repeated
from Listing 8-3
If you don’t have much experience with async and await, the difference
between Listing 8-4 and Listing 8-5 might give you an idea of why the Rust
community was so excited to see them land. But since this is an intermediate
book, let’s dive a little deeper to understand just how this short segment
of code can replace the much longer manual implementation. To do that,
we first need to talk about generators—the mechanism by which async and
await are implemented.
Generators
Briefly described, a generator is a chunk of code with some extra compilergenerated
bits that enables it to stop, or yield, its execution midway through
and then resume from where it last yielded later on. Take the forward function
in Listing 8-3, for example. Imagine that it gets to the call to send, but
the channel is currently full. The function can’t make any more progress,
but it also cannot block (this is nonblocking code, after all), so it needs to
return. Now suppose the channel eventually clears and we want to proceed
with the send. If we call forward again from the top, it’ll call next again and
the item we previously tried to send will be lost, so that’s no good. Instead,
we turn forward into a generator.


Asynchronous Programming 125
Whenever the forward generator cannot make progress anymore, it
needs to store its current state somewhere so that when its execution eventually
resumes, it resumes in the right place with the right state. It saves the
state through an associated data structure that’s generated by the compiler,
which contains all the state of the generator at a given point in time. A
method on that data structure (also generated) then allows the generator
to resume from its current state, stored in &mut self, and updates the state
again when the generator again cannot make progress.
This “return but allow me to resume later” operation is called yielding,
which effectively means it returns while keeping some extra state on the
side. When we later want to resume a call to forward, we invoke the known
entry point into the generator (the resume method, which is poll for async
generators), and the generator inspects the previously stored state in self
to decide what to do next. This is exactly the same thing we did manually in
Listing 8-4! In other words, the code in Listing 8-5 loosely desugars to the
hypothetical code shown in Listing 8-6.
generator fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {
loop {
let mut f = rx.next();
let r = if let Poll::Ready(r) = f.poll() { r } else { yield };
if let Some(t) = r {
let mut f = tx.send(t);
let _ = if let Poll::Ready(r) = f.poll() { r } else { yield };
} else { break Poll::Ready(()); }
}
}
Listing 8-6: Desugaring async/await into a generator
At the time of writing, generators are not actually usable in Rust—they
are only used internally by the compiler to implement async/await—but that
may change in the future. Generators come in handy in a number of cases,
such as to implement iterators without having to carry around a struct or to
implement an impl Iterator that figures out how to yield items one at a time.
If you look closely at Listings 8-5 and 8-6, they may seem a little magical
once you know that every await or yield is really a return from the function.
After all, there are several local variables in the function, and it’s not clear
how they’re restored when we resume later on. This is where the compilergenerated
part of generators comes into play. The compiler transparently
injects code to persist those variables into and read them from the generator’s
associated data structure, rather than the stack, at the time of execution.
So if you declare, write to, or read from some local variable a, you are
really operating on something akin to self.a. Problem solved! It’s all really
quite marvelous.
One subtle but important difference between the manual forward implementation
and the async/await version is that the latter can hold references
across yield points. This enables functions like Receiver::next and Sender::send
in Listing 8-5 to take &mut self rather than the self they took in Listing 8-4.
If we tried to use a &mut self receiver for these methods in the manual state

126 Chapter 8
machine implementation, the borrow checker would have no way to enforce
that the Receiver stored inside Forward cannot be referenced between when
Receiver::next is called and when the future it returns resolves, and so it would
reject the code. Only by moving the Receiver into the future can we convince
the compiler that the Receiver is not otherwise accessible. Meanwhile, with
async/await, the borrow checker can inspect the code before the compiler
turns it into a state machine and verify that rx is indeed not accessed again
until after the future is dropped, when the await on it returns.
THE SIZE OF GENERATORS
The data structure used to back a generator’s state must be able to hold the combined
state at any one yield point. If your async fn contains, say, a [u8; 8192],
those 8KiB must be stored in the generator itself. Even if your async fn contains
only smaller local variables, it must also contain any future that it awaits, since it
needs to be able to poll such a future later, when poll is invoked.
This nesting means that generators, and thus futures based on async
functions and blocks, can get quite large without any visible indicator of that
increased size in your code. This can in turn impact your program’s runtime
performance, since those giant generators may have to be copied across function
calls and in and out of data structures, which amounts to a fair amount of
memory copying. In fact, you can usually identify when the size of your generator-
based futures is affecting performance by looking for excessive amounts of
time spent in the memcpy function in your application’s performance profiles!
Finding these large futures isn’t always easy, however, and often requires
manually identifying long or complex chains of async functions. Clippy may
be able to help with this in the future, but at the time of writing, you’re on your
own. When you do find a particularly large future, you have two options: you
can try to reduce the amount of local state the async functions need, or you
can move the future to the heap (with Box::pin) so that moving the future just
requires moving the pointer to it. The latter is by far the easiest way to go, but
it also introduces an extra allocation and a pointer indirection. Your best bet is
usually to put the problematic future on the heap, measure your performance,
and then use your performance benchmarks to guide you from there.
Pin and Unpin
We’re not quite done. While generators are neat, a challenge arises from
the technique as I’ve described it so far. In particular, it’s not clear what
happens if the code in the generator (or, equivalently, the async block) takes
a reference to a local variable. In the code from Listing 8-5, the future that
rx.next() returns must necessarily hold a reference to rx if a next message
is not immediately available so that it knows where to try again when the
generator next resumes. When the generator yields, the future and the reference
the future contains get stashed away inside the generator. But what


Asynchronous Programming 127
now happens if the generator is moved? Specifically, look at the code in
Listing 8-7, which calls forward.
async fn try_forward<T>(rx: Receiver<T>, tx: Sender<T>) -> Option<impl Future>
{
let mut f = forward(rx, tx);
if f.poll().is_pending() { Some(f) } else { None }
}
Listing 8-7: Moving a future after polling it
The try_forward function polls forward only once, to forward as many
messages as possible without blocking. If the receiver may still produce more
messages (that is, if it returned Poll::Pending instead of Poll::Ready(None)),
those messages are deferred to be forwarded at some later time by returning
the forwarding future to the caller, which may choose to poll again at a time
when it sees fit.
Let’s work through what happens here with what we know about async
and await so far. When we poll the forward generator, it goes through the
while loop some unknown number of times and eventually returns either
Poll::Ready(()) if the receiver ended, or Poll::Pending otherwise. If it returns
Poll::Pending, the generator contains a future returned from either rx.next()
or tx.send(t). Those futures both contain a reference to one of the arguments
initially provided to forward (rx and tx, respectively), which must also
be stored in the generator. But when try_forward returns the entire generator,
the fields of the generator also move. Thus, rx and tx no longer reside
at the same locations in memory, and the references stored in the stashedaway
future are no longer pointing to the right data!
What we’ve run into here is a case of a self-referential data structure: one
that holds both data and references to that data. With generators, these selfreferential
structures are very easy to construct, and being unable to support
them would be a significant blow to ergonomics because it would mean you
wouldn’t be able to hold references across any yield point. The (ingenious)
solution for supporting self-referential data structures in Rust comes in the
form of the Pin type and the Unpin trait. Very briefly, Pin is a wrapper type that
prevents the wrapped type from being (safely) moved, and Unpin is a marker
trait that says the implementing type can be removed safely from a Pin.
Pin
There’s a lot of nuance to cover here, so let’s start with a concrete use of
the Pin wrapper. Listing 8-2 gave you a simplified version of the Future trait,
but we’re now ready to peel back one part of the simplification. Listing 8-8
shows the Future trait somewhat closer to its final form.
trait Future {
type Output;
fn poll(self: Pin<&mut Self>) -> Poll<Self::Output>;
}
Listing 8-8: A less simplified view of the Future trait with Pin

128 Chapter 8
In particular, this definition requires that you call poll on Pin<&mut Self>.
Once you have a value behind a Pin, that constitutes a contract that that value
will never move again. This means that you can construct self-references
internally to your heart’s delight, exactly as you want for generators.
NOTE While Future makes use of Pin, Pin is not tied to the Future trait—you can use Pin for
any self-referential data structure.
But how do you get a Pin to call poll? And how can Pin ensure that
the contained value won’t move? To see how this magic works, let’s look
at the definition of std::pin::Pin and some of its key methods, shown in
Listing 8-9.
struct Pin<P> { pointer: P }
impl<P> Pin<P> where P: Deref {
pub unsafe fn new_unchecked(pointer: P) -> Self;
}
impl<'a, T> Pin<&'a mut T> {
pub unsafe fn get_unchecked_mut(self) -> &'a mut T;
}
impl<P> Deref for Pin<P> where P: Deref {
type Target = P::Target;
fn deref(&self) -> &Self::Target;
}
Listing 8-9: std::pin::Pin and its key methods
There’s a lot to unpack here, and we’re going to have to go over the
definition in Listing 8-9 a few times before all the bits make sense, so please
bear with me.
First, you’ll notice that Pin holds a pointer type. That is, rather than hold
some T directly, it holds a type P that dereferences through Deref into T. This
means that rather than have a Pin<MyType>, you’ll have a Pin<Box<MyType>> or
Pin<Rc<MyType>> or Pin<&mut MyType>. The reason for this design is simple—
Pin’s primary goal is to make sure that once you place a T behind a Pin, that
T won’t move, as doing so might invalidate self-references stored in the T. If
the Pin just held a T directly, then simply moving the Pin would be enough to
invalidate that invariant! In the remainder of this section, I’ll refer to P as
the pointer type and T as the target type.
Next, notice that Pin’s constructor, new_unchecked, is unsafe. This is
because the compiler has no way to actually check that the pointer type
indeed promises that the pointed-to (target) type won’t move again. Consider,
for example, a variable foo on the stack. If Pin’s constructor were safe,
we could do Pin::new(&mut foo), call a method that requires Pin<&mut Self>
(and thus assumes that Self won’t move again), and then drop the Pin. At
this point, we could modify foo as much as we liked, since it is no longer
borrowed—including moving it! We could then pin it again and call the
same method, which would be none the wiser that any self-referential pointers
it may have constructed the first time around would now be invalid.

Asynchronous Programming 129
PIN CONSTRUCTOR SAFETY
The other reason the constructor for Pin is unsafe is that its safety depends on
the implementation of traits that are themselves safe. For example, the way
that Pin<P> implements get_unchecked_mut is to use the implementation of
DerefMut::deref_mut for P. While the call to get_unchecked_mut is unsafe, the
impl DerefMut for P is not. Yet it receives a &mut self, and can thus freely (and
without unsafe code) move the T. The same thing applies to Drop. The safety
requirement for Pin::new_unchecked is therefore not only that the pointer type
will not let the target type be moved again (like in the Pin<&mut T> example),
but also that its Deref, DerefMut, and Drop implementations do not move the
pointed-to value behind the &mut self they receive.
We then get to the get_unchecked_mut method, which gives you a mutable
reference to the T behind the Pin’s pointer type. This method is also unsafe,
because once we give out a &mut T, the caller has to promise it won’t use
that &mut T to move the T or otherwise invalidate its memory, lest any selfreferences
be invalidated. If this method weren’t unsafe, a caller could
call a method that takes Pin<&mut Self> and then call the safe variant of
get_unchecked_mut on two Pin<&mut _>s, then use mem::swap to swap the values
behind the Pin. If we were to then call a method that takes Pin<&mut Self>
again on either Pin, its assumption that the Self hasn’t moved would be violated,
and any internal references it stored would be invalid!
Perhaps surprisingly, Pin<P> always implements Deref<Target = T>, and
that is entirely safe. The reason for this is that a &T does not let you move T
without writing other unsafe code (UnsafeCell, for example, as we’ll discuss
in Chapter 9). This is a good example of why the scope of an unsafe block
extends beyond just the code it contains. If you wrote some code in one part
of the application that (unsafely) replaced a T behind an & using UnsafeCell,
then it could be that that &T initially came from a Pin<&mut T>, and that you
have now violated the invariant that the T behind the Pin may never move,
even though the place where you unsafely replaced the &T did not even mention
Pin!
NOTE If you’ve browsed through the Pin documentation while reading this chapter, you may
have noticed Pin::set, which takes a &mut self and a <P as Deref>::Target and
safely changes the value behind the Pin. This is possible because set does not return
the value that was previously pinned—it simply drops it in place and stores the new
value there instead. Therefore, it does not violate the pinning invariants: the old
value was never accessed outside of a Pin after it was placed there.
Unpin: The Key to Safe Pinning
At this point you might ask: given that getting a mutable reference is unsafe
anyway, why not have Pin hold a T directly? That is, rather than require an

130 Chapter 8
indirection through a pointer type, you could instead make the contract
for get_unchecked_mut that it is only safe to call if you haven’t moved the Pin.
The answer to that question lies in a neat safe use of Pin that the pointer
design enables. Recall that the whole reason we want Pin in the first place is
so we can have target types that may contain references to themselves (like
a generator) and give their methods a guarantee that the target type hasn’t
moved and thus that internal self-references remain valid. Pin lets us use the
type system to enforce that guarantee, which is great. But unfortunately,
with the design so far, Pin is very unwieldy to work with. This is because it
always requires unsafe code, even if you are working with a target type that
doesn’t contain any self-references, and so doesn’t care whether it’s been
moved or not.
This is where the marker trait Unpin comes into play. An implementation
of Unpin for a type simply asserts that the type is safe to move out of a Pin
when used as a target type. That is, the type promises that it will never use
any of Pin’s guarantees about the referent not moving again when used as a
target type, and thus those guarantees may be broken. Unpin is an auto-trait,
like Send and Sync, and so is auto-implemented by the compiler for any type
that contains only Unpin members. Only types that explicitly opt out of Unpin
(like generators) and types that contain those types are !Unpin.
For target types that are Unpin, we can provide a much simpler safe
interface to Pin, as shown in Listing 8-10.
impl<P> Pin<P> where P: Deref, P::Target: Unpin {
pub fn new(pointer: P) -> Self;
}
impl<P> DerefMut for Pin<P> where P: DerefMut, P::Target: Unpin {
fn deref_mut(&mut self) -> &mut Self::Target;
}
Listing 8-10: The safe API to Pin for Unpin target types
To make sense of the safe API in Listing 8-10, think about the safety
requirements of the unsafe methods from Listing 8-9: the function
Pin::new_unchecked is unsafe because the caller must promise that the referent
cannot be moved outside of the Pin, and that the implementations
of Deref, DerefMut, and Drop for the pointer type do not move the referent
through the reference they receive. Those requirements are there to
ensure that once we give out a Pin to a T, we never move that T again. But
if the T is Unpin, it has declared that it does not care if it is moved even if it
was previously pinned, so it’s fine if the caller does not satisfy any of those
requirements!
Similarly, get_unchecked_mut is unsafe because the caller must guarantee
that it doesn’t move the T out of the &mut T—but with T: Unpin, T has declared
that it’s fine being moved even after being pinned, so that safety requirement
is no longer important. This means that for Pin<P> where P::Target:
Unpin, we can simply provide safe variants of both those methods (DerefMut
being the safe version of get_unchecked_mut). In fact, we can even provide a
Pin::into_inner that simply gives back the owned P if the target type is Unpin,
since the Pin is essentially irrelevant!


Asynchronous Programming 131
Ways of Obtaining a Pin
With our new understanding of Pin and Unpin, we can now make progress
toward using the new Future definition from Listing 8-8 that requires
Pin<&mut Self>. The first step is to construct the required type. If the future
type is Unpin, that step is easy—we just use Pin::new(&mut future). If it is not
Unpin, we can pin the future in one of two main ways: by pinning to the
heap or pinning to the stack.
Let’s start with pinning to the heap. The primary contract of Pin is that
once something has been pinned, it cannot move. The pinning API takes
care of honoring that contract for all methods and traits on Pin, so the main
role of any function that constructs a Pin is to ensure that if the Pin itself
moves, the referent value does not move too. The easiest way to ensure that is
to place the referent on the heap, and then place just a pointer to the referent
in the Pin. You can then move the Pin to your heart’s delight, but the target
will remain where it was. This is the rationale behind the (safe) method
Box::pin, which takes a T and returns a Pin<Box<T>>. There’s no magic to it; it
simply asserts that Box follows the Pin constructor, Deref, and Drop contracts.
UNPIN BOX
While we’re on the topic of Box, take a look at the implementation of Unpin for
Box. The Box type unconditionally implements Unpin for any T, even if that T is
not Unpin. This might strike you as odd, given the earlier assertion that Unpin
is an auto-trait that is generally implemented for a type only if all of the type’s
members are also Unpin. Box is an exception to this for the same reason that it
can provide a safe Pin constructor: if you move a Box<T>, you do not move the
T. In other words, the unconditional implementation asserts that you can move a
Box<T> out of a Pin even if T cannot be moved out of a Pin. Note, however, that
this does not enable you to move a T that is !Unpin out of a Pin<Box<T>>.
The other option, pinning to the stack, is a little more involved, and at
the time of writing requires a smidgen of unsafe code. We have to ensure
that the pinned value cannot be accessed after the Pin with a &mut to it has
been dropped. We accomplish that by shadowing the value as shown in the
macro in Listing 8-11 or by using one of the crates that provide exactly this
macro. One day it may even make it into the standard library!
macro_rules! pin_mut {
($var:ident) => {
let mut $var = $var;
let mut $var = unsafe { Pin::new_unchecked(&mut $var) };
}
}
Listing 8-11: Macro for pinning to the stack

132 Chapter 8
By taking the name of the variable to pin to the stack, the macro
ensures that the caller has the value it wants to pin somewhere on the
stack already. The shadowing of $var ensures that the caller cannot drop
the Pin and continue to use the unpinned value (which would breach the
Pin contract
for any target type that’s !Unpin). By moving the value stored
in $var, the macro also ensures that the caller cannot drop the $var binding
the macro declarations without also dropping the original variable.
Specifically, without that line, the caller could write (note the extra scope):
let foo = /* */; { pin_mut!(foo); foo.poll() }; foo.mut_self_method();
Here, we give a pinned instance of foo to poll, but then we later use a
&mut to foo without a Pin, which violates the Pin contract. With the extra reassignment,
on the other hand, that code would also move foo into the new
scope, rendering it unusable after the scope ends.
Pinning on the stack therefore requires unsafe code, unlike Box::pin,
but avoids the extra allocation that Box introduces and also works in no_std
environments.
Back to the Future
We now have our pinned future, and we know what that means. But you
may have noticed that none of this important pinning stuff shows up in
most asynchronous code you write with async and await. And that’s because
the compiler hides it from you.
Think back to when we discussed Listing 8-5, when I told you that
<expr>.await desugars into something like:
loop { if let Poll::Ready(r) = expr.poll() { break r } else { yield } }
That was an ever-so-slight simplification because, as we’ve seen, you can
call Future::poll only if you have a Pin<&mut Self> for the future. The desugaring
is actually a bit more sophisticated, as shown in Listing 8-12.
1 match expr {
mut pinned => loop {
2 match unsafe { Pin::new_unchecked(&mut pinned) }.poll() {
Poll::Ready(r) => break r,
Poll::Pending => yield,
}
}
}
Listing 8-12: A more accurate desugaring of <expr>.await
The match 1 is a neat shorthand to not only ensure that the expansion
remains a valid expression, but also move the expression result into
a variable that we can then pin on the stack. Beyond that, the main new
addition is the call to Pin::new_unchecked 2. That call is safe because for the
containing async block to be polled, it must already be pinned due to the
signature of Future::poll. And the async block was polled for us to reach

Asynchronous Programming 133
the call to Pin::new_unchecked, so the generator state is pinned. Since pinned
is stored in the generator that corresponds to the async block (it must be so
that yield will resume correctly), we know that pinned will not move again.
Furthermore, pinned is not accessible except through a Pin once we’re in the
loop, so no code is able to move out of the value in pinned. Thus, we meet all
the safety requirements of Pin::new_unchecked, and the code is safe.
Going to Sleep
We went pretty deep into the weeds with Pin, but now that we’re out the
other side, there is another issue around futures that may have been making
your brain itch. If a call to Future::poll returns Poll::Pending, you need
something to call poll again at a later time to check whether you can make
progress yet. That something is usually called the executor. Your executor
could be a simple loop that polls all the futures you are waiting on until
they’ve all returned Poll::Ready, but that would burn a lot of CPU cycles you
could probably have used for other, more useful things, like running your
web browser. Instead, we want the executor to do whatever useful work it
can do, and then go to sleep. It should stay asleep until one of the futures
can make progress, and only then wake up to do another pass, before going
to sleep again.
Waking Up
The condition that determines when to check back with a given future varies
widely. It might be “when a network packet arrives on this port,” “when
the mouse cursor moves,” “when someone sends on this channel,” “when
the CPU receives a particular interrupt,” or even “after this much time has
passed.” On top of that, developers can write their own futures that wrap
multiple other futures, and thus, they may have several wake-up conditions.
Some futures may even introduce their own entirely custom wake events.
To accommodate these many use cases, Rust introduces the notion of
a Waker: a way to wake the executor to signal that progress can be made. The
Waker is what makes the whole machinery around futures work. The executor
constructs a Waker that integrates with the mechanism the executor uses to
go to sleep, and passes the Waker in to any Future it polls. How? With the additional
parameter to Future::poll that I’ve hidden from you so far. Sorry about
that. Listing 8-13 gives the final and true definition for Future—no more lies!
trait Future {
type Output;
fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output>;
}
Listing 8-13: The actual Future trait with Context
The &mut Context contains the Waker. The argument is a Context, not a
Waker directly, so that we can augment the asynchronous ecosystem with
additional context for futures should that be deemed necessary.


134 Chapter 8
The primary method on Waker is wake (and the by-reference variant wake
_by_ref), which should be called when the future can again make progress.
The wake method takes no arguments, and its effects are entirely defined
by the executor that constructed the Waker. You see, Waker is secretly generic
over the executor. Or, more precisely, whatever constructed the Waker gets to
dictate what happens when Waker::wake is called, when a Waker is cloned, and
when a Waker is dropped. This all happens through a manually implemented
vtable, which functions similarly to the dynamic dispatch we discussed way
back in Chapter 2.
It’s a somewhat involved process to construct a Waker, and the mechanics
of it aren’t all that important for using one, but you can see the building
blocks in the RawWakerVTable type in the standard library. It has a constructor
that takes the function pointers for wake and wake_by_ref as well as Clone and
Drop. The RawWakerVTable, which is usually shared among all of an executor’s
wakers, is bundled up with a raw pointer intended to hold data specific to
each Waker instance (like which future it’s for) and is turned into a RawWaker.
That is in turn passed to Waker::from_raw to produce a safe Waker that can be
passed to Future::poll.
Fulfilling the Poll Contract
So far we’ve skirted around what a future actually does with a Waker. The
idea is fairly simple: if Future::poll returns Poll::Pending, it is the future’s
responsibility to ensure that something calls wake on the provided Waker
when the future is next able to make progress. Most futures uphold this
property by returning Poll::Pending only if some other future also returned
Poll::Pending; in this way, it trivially fulfills the contract of poll since the
inner future must follow that same contract. But there can’t be turtles all
the way down. At some point, you reach a future that does not poll other
futures but instead does something like write to a network socket or attempt
to receive on a channel. These are commonly referred to as leaf futures since
they have no children. A leaf future has no inner future but instead directly
represents some resource that may not yet be ready to return a result.
NOTE The poll contract is the reason why the recursive poll call 6 back in Listing 8-4 is
necessary for correctness.
Leaf futures typically come in one of two shapes: those that wait for
events that originate within the same process (like a channel receiver), and
those that wait for events external to the process (like a TCP packet read).
Those that wait for internal events all tend to follow the same pattern: store
the Waker where the code that will wake you up can find it, and have that
code call wake on the Waker when it generates the relevant event. For example,
consider a leaf future that has to wait for a message on an in-memory channel.
It stores its Waker inside the part of the channel that is shared between
the sender and the receiver and then returns Poll::Pending. When a sender
later comes along and injects a message into the channel, it notices the Waker
left there by the waiting receiver and calls wake on the Waker before returning
from send. Now the receiver is awoken, and the poll contract is upheld.


Asynchronous Programming 135
Leaf futures that deal with external events are more involved, as the
code that generates the event they’re waiting for knows nothing of futures
or wakers. Most often the generating code is the operating system kernel,
which knows when a disk is ready or a timer expires, but it could also be
a C library that invokes a callback into Rust when an operation completes
or some other such external entity. A leaf future wrapping an external
resource like this could spin up a thread that executes a blocking system
call (or waits for the C callback) and then use the internal waking mechanism,
but that would be wasteful; you would spin up a thread every time
an operation had to wait and be left with lots of single-use threads sitting
around waiting for things.
Instead, executors tend to provide implementations of leaf futures
that communicate behind the scenes with the executor to arrange for
the appropriate interaction with the operating system. How exactly this
is orchestrated depends on the executor and the operating system, but
roughly speaking the executor keeps track of all the event sources that it
should listen for the next time it goes to sleep. When a leaf future realizes
it must wait for an external event, it updates that executor’s state (which it
knows about since it’s provided by the executor crate) to include that external
event source alongside its Waker. When the executor can no longer make
progress, it gathers all of the event sources the various pending leaf futures
are waiting for and does a big blocking call to the operating system, telling
it to return when any of the resources the leaf futures are waiting on have
a new event. On Linux, this is usually achieved with the epoll system call;
Windows, the BSDs, macOS, and pretty much every other operating system
provide similar mechanisms. When that call returns, the executor calls wake
on all the wakers associated with event sources that the operating system
reported events for, and thus the poll contract is fulfilled.
NOTE A reactor is the part of an executor that leaf futures register event sources with and
that the executor waits on when it has no more useful work to do. It is possible to
separate the executor and the reactor, though bundling them together often improves
performance as the two can be co-optimized more readily.
A knock-on effect of the tight integration between leaf futures and
the executor is that leaf futures from one executor crate often cannot be
used with a different executor. Or at least, they cannot be used unless the
leaf future’s executor is also running. When the leaf future goes to store
its Waker and register the event source it’s waiting for, the executor it was
built against needs to have that state set up and needs to be running so
that the event source will actually be monitored and wake eventually called.
There are ways around this, such as having leaf futures spawn an executor
if one is not already running, but this is not always advisable as it means
that an application can transparently end up with multiple executors running
at the same time, which can reduce performance and mean you must
inspect the state of multiple executors when debugging.
Library crates that wish to support multiple executors have to be generic
over their leaf resources. For example, instead of using a particular executor’s



136 Chapter 8
TcpStream or File future type, a library can store a generic T: AsyncRead +
AsyncWrite. However, the ecosystem has yet to settle on exactly what these traits
should look like and which traits are needed, so for the moment it’s fairly difficult
to make code truly generic over the executor. For example, while AsyncRead
and AsyncWrite are somewhat common across the ecosystem (or can be easily
adapted if necessary), no traits currently exist for running a future in the
background (spawning, which we’ll discuss later) or for representing a timer.
Waking Is a Misnomer
You may already have realized that Waker::wake doesn’t necessarily seem to
wake anything. For example, for external events (as described in the previous
section), the executor is already awake, and it might seem silly for it to
then call wake on a Waker that belongs to that executor anyway! The reality is
that Waker::wake is a bit of a misnomer—in reality, it signals that a particular
future is runnable. That is, it tells the executor that it should make sure to
poll this particular future when it gets around to it rather than go to sleep
again, since this future can make progress. This might wake the executor if
it is currently sleeping so it will go poll that future, but that’s more of a side
effect than its primary purpose.
It is important for the executor to know which futures are runnable
for two reasons. First, it needs to know when it can stop polling a future
and go to sleep; it’s not sufficient to just poll each future until it returns
Poll::Pending, since polling a later future might make it possible to progress
an earlier future. Consider the case where two futures bounce messages
back and forth on channels to one another. When you poll one, the other
becomes ready, and vice versa. In this case, the executor should never go to
sleep, as there is always more work to do.
Second, knowing which futures are runnable lets the executor avoid
polling futures unnecessarily. If an executor manages thousands of pending
futures, it shouldn’t poll all of them just because an event made one of them
runnable. If it did, executing asynchronous code would get very slow indeed.
Tasks and Subexecutors
The futures in an asynchronous program form a tree: a future may contain
any number of other futures, which in turn may contain other futures, all the
way down to the leaf futures that interact with wakers. The root of each tree is
the future you give to whatever the executor’s main “run” function is. These
root futures are called tasks, and they are the only point of contact between
the executor and the futures tree. The executor calls poll on the task, and
from that point forward the code of each contained future must figure out
which inner future(s) to poll in response, all the way down to the relevant leaf.
Executors generally construct a separate Waker for each task they poll so
that when wake is later called, they know which task was just made runnable
and can mark it as such. That is what the raw pointer in RawWaker is for—to differentiate
between tasks while sharing the code for the various Waker methods.
When the executor eventually polls a task, that task starts running from
the top of its implementation of Future::poll and must decide from there how


Asynchronous Programming 137
to get to the future deeper down that can now make progress. Since each
future knows only about its own fields, and nothing about the whole tree, this
all happens through calls to poll that each traverse one edge in the tree.
The choice of which inner future to poll is often obvious, but not always.
In the case of async/await, the future to poll is the one we’re blocked waiting
for. But in a future that waits for the first of several futures to make progress
(often called a select), or for all of a set of futures (often called a join),
there are many options. A future that has to make such a choice is basically
a subexecutor. It could poll all of its inner futures, but doing so could be
quite wasteful. Instead, these subexecutors often wrap the Waker they receive
in poll’s Context with their own Waker type before they invoke poll on any
inner future. In the wrapping code, they mark the future they just polled as
runnable in their own state before they call wake on the original Waker. That
way, when the executor eventually polls the subexecutor future again, the
subexecutor can consult its own internal state to figure out which of its inner
futures caused the current call to poll, and then only poll those.
BLOCKING IN ASYNC CODE
You must be careful about calling synchronous code from asynchronous code,
since any time an executor thread spends executing the current task is time it’s
not spending running other tasks. If a task occupies the current thread for a
prolonged period of time without yielding back to the executor, which might
happen when executing a blocking system call (like std::sync::sleep), running
a subexecutor that doesn’t yield occasionally, or running in a tight loop with no
awaits, then other tasks the current executor thread is responsible for won’t get
to run during that time. Usually, this manifests as long delays between when
certain tasks can make progress (such as when a client connects) and when
they actually get to execute.
Some multithreaded executors implement work-stealing techniques, where
idle executor threads steal tasks from busy executor threads, but this is more of
a mitigation than a solution. Ultimately, you could end up in a situation where
all the executor threads are blocked, and thus no tasks get run until one of the
blocking operations completes.
In general, you should be very careful with executing compute-intensive
operations or calling functions that could block in an asynchronous context.
Such operations should either be converted to asynchronous operations where
possible or executed on dedicated threads that then communicate using a
primitive that does support asynchrony, like a channel. Some executors also
provide mechanisms for indicating that a particular segment of asynchronous
code might block or for yielding voluntarily in the context of loops that might
otherwise not yield, which can compose part of the solution. A good rule of
thumb is that no future should be able to run for more than 1 ms without returning
Poll::Pending.


138 Chapter 8
Tying It All Together with spawn
When working with asynchronous executors, you may come across an
operation that spawns a future. We’re now in a position to explore what
that means! Let’s do so by way of example. First, consider the simple server
implementation in Listing 8-14.
async fn handle_client(socket: TcpStream) -> Result<()> {
// Interact with the client over the given socket.
}
async fn server(socket: TcpListener) -> Result<()> {
while let Some(stream) = socket.accept().await? {
handle_client(stream).await?;
}
}
Listing 8-14: Handling connections sequentially
The top-level server function is essentially one big future that listens
for new connections and does something when a new connection arrives.
You hand that future to the executor and say “run this,” and since you don’t
want your program to then exit immediately, you’ll probably have the executor
block on that future. That is, the call to the executor to run the server
future will not return until the server future resolves, which may be never
(another client could always arrive later).
Now, every time a new client connection comes in, the code in
Listing 8-14 makes a new future (by calling handle_client) to handle that
connection. Since the handling is itself a future, we await it and then move
on to the next client connection.
The downside of this approach is that we only ever handle one connection
at a time—there is no concurrency. Once the server accepts a connection,
the handle_client function is called, and since we await it, we don’t go
around the loop again until handle_client’s return future resolves (presumably
when that client has left).
We could improve on this by keeping a set of all the client futures and
having the loop in which the server accepts new connections also check all
the client futures to see if any can make progress. Listing 8-15 shows what
that might look like.
async fn server(socket: TcpListener) -> Result<()> {
let mut clients = Vec::new();
loop {
poll_client_futures(&mut clients)?;
if let Some(stream) = socket.try_accept()? {
clients.push(handle_client(stream));
}
}
}
Listing 8-15: Handling connections with a manual executor

Asynchronous Programming 139
This at least handles many connections concurrently, but it’s quite
convoluted. It’s also not very efficient because the code now busy-loops,
switching between handling the connections we already have and accepting
new ones. And it has to check each connection each time, since it won’t
know which ones can make progress (if any). It also can’t await at any point,
since that would prevent the other futures from making progress. You could
implement your own wakers to ensure that the code polls only the futures
that can make progress, but ultimately this is going down the path of developing
your own mini-executor.
Another downside of sticking with just the one task for the server that
internally contains the futures for all of the client connections is that the
server ends up being single-threaded. There is just the one task and to poll
it the code must hold an exclusive reference to the task’s future (poll takes
Pin<&mut Self>), which only one thread can hold at a time.
The solution is to make each client future its own task and leave it to
the executor to multiplex among all the tasks. Which, you guessed it, you
do by spawning the future. The executor will continue to block on the
server future, but if it cannot make progress on that future, it will use its
execution machinery to make progress on the other tasks in the meantime
behind the scenes. And best of all, if the executor is multithreaded and
your client futures are Send, it can run them in parallel since it can hold
&muts to the separate tasks concurrently. Listing 8-16 gives an example of
what this might look like.
async fn server(socket: TcpListener) -> Result<()> {
while let Some(stream) = socket.accept().await? {
// Spawn a new task with the Future that represents this client.
// The current task will continue to just poll for more connections
// and will run concurrently (and possibly in parallel) with handle_client.
spawn(handle_client(stream));
}
}
Listing 8-16: Spawning futures to create more tasks that can be polled concurrently
When you spawn a future and thus make it a task, it’s sort of like spawning
a thread. The future continues running in the background and is multiplexed
concurrently with any other tasks given to the executor. However,
unlike a spawned thread, spawned tasks still depend on being polled by
the executor. If the executor stops running, either because you drop it or
because your code no longer runs the executor’s code, those spawned tasks
will stop making progress. In the server example, imagine what will happen
if the main server future resolves for some reason. Since the executor
has returned control back to your code, it cannot continue doing, well,
anything. Multi-threaded executors often spawn background threads that
continue to poll tasks even if the executor yields control back to the user’s
code, but not all executors do this, so check your executor before you rely
on that behavior!

140 Chapter 8
Summary
In this chapter, we’ve taken a look behind the scenes of the asynchronous
constructs available in Rust. We’ve seen how the compiler implements generators
and self-referential types, and why that work was necessary to support
what we now know as async/await. We’ve also explored how futures are
executed, and how wakers allow executors to multiplex among tasks when
only some of them can make progress at any given moment. In the next
chapter, we’ll tackle what is perhaps the deepest and most discussed area
of Rust: unsafe code. Take a deep breath, and then turn the page.

### 9.UNSAFE CODE

The mere mention of unsafe code often
elicits strong responses from many in the
Rust community, and from many of those
watching Rust from the sidelines. While some
maintain it’s “no big deal,” others decry it as “the reason
all of Rust’s promises are a lie.” In this chapter,
I hope to pull back the curtain a bit to explain what unsafe is, what it isn’t,
and how you should go about using it safely. At the time of writing, and
likely also when you read this, Rust’s precise requirements for unsafe code
are still being determined, and even if they were all nailed down, the complete
description would be beyond the scope of this book. Instead, I’ll do
my best to arm you with the building blocks, intuition, and tooling you’ll
need to navigate your way through most unsafe code.
Your main takeaway from this chapter should be this: unsafe code is the
mechanism Rust gives developers for taking advantage of invariants that,
for whatever reason, the compiler cannot check. We’ll look at the ways in
which unsafe does that, what those invariants may be, and what we can do
with it as a result.
142 Chapter 9
INVARIANTS
Throughout this chapter, I’ll be talking a lot about invariants. Invariant is just a
fancy way of saying “something that must be true for your program to be correct.”
For example, in Rust, one invariant is that references, using & and &mut,
do not dangle—they always point to valid values. You can also have application-
or library-specific invariants, like “the head pointer is always ahead of the
tail pointer” or “the capacity is always a power of two.” Ultimately, invariants
represent all the assumptions required for your code to be correct. However,
you may not always be aware of all the invariants that your code uses, and
that’s where bugs creep in.
Crucially, unsafe code is not a way to skirt the various rules of Rust, like
borrow checking, but rather a way to enforce those rules using reasoning
that is beyond the compiler. When you write unsafe code, the onus is on
you to ensure that the resulting code is safe. In a way, unsafe is misleading
as a keyword when it is used to allow unsafe operations through unsafe {};
it’s not that the contained code is unsafe, it’s that the code is allowed to perform
otherwise unsafe operations because in this particular context, those
operations are safe.
The rest of this chapter is split into four sections. We’ll start with a brief
examination of how the keyword itself is used, then explore what unsafe
allows you to do. Next, we’ll look at the rules you must follow in order to
write safe unsafe code. Finally, I’ll give you some advice about how to actually
go about writing unsafe code safely.
The unsafe Keyword
Before we discuss the powers that unsafe grants you, we need to talk about
its two different meanings. The unsafe keyword serves a dual purpose in
Rust: it marks a particular function as unsafe to call and it enables you to
invoke unsafe functionality in a particular code block. For example, the
method in Listing 9-1 is marked as unsafe, even though it contains no
unsafe code. Here, the unsafe keyword serves as a warning to the caller that
there are additional guarantees that someone who writes code that invokes
decr must manually check.
impl<T> SomeType<T> {
pub unsafe fn decr(&self) {
self.some_usize -= 1;
}
}
Listing 9-1: An unsafe method that contains only safe code
Unsafe Code 143
Listing 9-2 illustrates the second usage. Here, the method itself is not
marked as unsafe, even though it contains unsafe code.
impl<T> SomeType<T> {
pub fn as_ref(&self) -> &T {
unsafe { &*self.ptr }
}
}
Listing 9-2: A safe method that contains unsafe code
These two listings differ in their use of unsafe because they embody
different contracts. decr requires the caller to be careful when they call the
method, whereas as_ref assumes that the caller was careful when invoking
other unsafe methods (like decr). To see why, imagine that SomeType is really
a reference-counted type like Rc. Even though decr only decrements a number,
that decrement may in turn trigger undefined behavior through the
safe method as_ref. If you call decr and then drop the second-to-last Rc of a
given T, the reference count drops to zero and the T will be dropped—but
the program might still call as_ref on the last Rc, and end up with a dangling
reference.
NOTE Undefined behavior describes the consequences of a program that violates invariants
of the language at runtime. In general, if a program triggers undefined behavior,
the outcome is entirely unpredictable. We’ll cover undefined behavior in greater
detail later in this chapter.
Conversely, as long as there is no way to corrupt the Rc reference count
using safe code, it is always safe to dereference the pointer inside the Rc
the way the code for as_ref does—the fact that &self exists is proof that the
pointer must still be valid. We can use this to give the caller a safe API to
an otherwise unsafe operation, which is a core piece of how to use unsafe
responsibly.
For historical reasons, every unsafe fn contains an implicit unsafe block
in Rust today. That is, if you declare an unsafe fn, you can always invoke any
unsafe methods or primitive operations inside that fn. However, that decision
is now considered a mistake, and it’s currently being reverted through
the already accepted and implemented RFC 2585. This RFC warns about
having an unsafe fn that performs unsafe operations without an explicit
unsafe block inside it. The lint will also likely become a hard error in future
editions of Rust. The idea is to reduce the “footgun radius”—if every unsafe
fn is one giant unsafe block, then you might accidentally perform unsafe
operations without realizing it! For example, in decr in Listing 9-1, under
the current rules you could also have added *std::ptr::null() without any
unsafe annotation.
The distinction between unsafe as a marker and unsafe blocks as a
mechanism to enable unsafe operations is important, because you must
think about them differently. An unsafe fn indicates to the caller that
they have to be careful when calling the fn in question and that they must
ensure that the function’s documented safety invariants hold.
144 Chapter 9
Meanwhile, an unsafe block implies that whoever wrote that block carefully
checked that the safety invariants for any unsafe operations performed
inside it hold. If you want an approximate real-world analogy, unsafe fn is an
unsigned contract that asks the author of calling code to “solemnly swear X,
Y, and Z.” Meanwhile, unsafe {} is the calling code’s author signing off on all
the unsafe contracts contained within the block. Keep that in mind as we
go through the rest of this chapter.
Great Power
So, once you sign the unsafe contract with unsafe {}, what are you allowed
to do? Honestly, not that much. Or rather, it doesn’t enable that many new
features. Inside an unsafe block, you are allowed to dereference raw pointers
and call unsafe fns.
That’s it. Technically, there are a few other things you can do, like
accessing mutable and external static variables and accessing fields of
unions, but those don’t change the discussion much. And honestly, that’s
enough. Together, these powers allow you to wreak all sorts of havoc, like
turning types into one another with mem::transmute, dereferencing raw pointers
that point to who knows where, casting &'a to &'static, or making types
shareable across thread boundaries even though they’re not thread-safe.
In this section, we won’t worry too much about what can go wrong with
these powers. We’ll leave that for the boring, responsible, grown-up section
that comes after. Instead, we’ll look at these neat shiny new toys and what
we can do with them.
Juggling Raw Pointers
One of the most fundamental reasons to use unsafe is to deal with Rust’s raw
pointer types: *const T and *mut T. You should think of these as more or less
analogous to &T and &mut T, except that they don’t have lifetimes and are not
subject to the same validity rules as their & counterparts, which we’ll discuss
later in the chapter. These types are interchangeably referred to as pointers
and raw pointers, mostly because many developers instinctively refer to
references as pointers, and calling them raw pointers makes the distinction
clearer.
Since fewer rules apply to * than &, you can cast a reference to a pointer
even outside an unsafe block. Only if you want to go the other way, from *
to &, do you need unsafe. You’ll generally turn a pointer back into a reference
to do useful things with the pointed-to data, such as reading or modifying
its value. For that reason, a common operation to use on pointers is
unsafe { &*ptr } (or &mut *). The * there may look strange as the code is just
constructing a reference, not dereferencing the pointer, but it makes sense
if you look at the types; if you have a *mut T and want a &mut T, then &mut ptr
would just give you a &mut *mut T. You need the * to indicate that you want
the mutable reference to what ptr is a pointer to.
Unsafe Code 145
POINTER TYPES
You may be wondering what the difference is between *mut T and *const T
and std::ptr::NonNull<T>. Well, the exact specification is still being worked
out, but the primary practical difference between *mut T and *const T/
NonNull<T> is that *mut T is invariant in T (see “Lifetime Variance” in Chapter 1),
whereas the other two are covariant. As the names imply, *const T and
NonNull<T> differ primarily in that NonNull<T> is not allowed to be a null pointer,
whereas *const T is.
My best advice in choosing among these types is to use your intuition
about whether you would have written &mut or & if you were able to name the
relevant lifetime. If you would have written &, and you know that the pointer is
never null, use NonNull<T>. It benefits from a cool optimization called the niche
optimization: basically, since the compiler knows that the type can never be
null, it can use that information to represent types like Option<NonNull<T>> without
any extra overhead, since the None case can be represented by setting the
NonNull to be a null pointer! The null pointer value is a niche in the NonNull<T>
type. If the pointer might be null, use *const T. And if you would have written
&mut T, use *mut T.
Unrepresentable Lifetimes
As raw pointers do not have lifetimes, they can be used in circumstances
where the liveness of the value being pointed to cannot be expressed statically
within Rust’s lifetime system, such as a self-pointer in a self-referential
struct like the generators we discussed in Chapter 8. A pointer that points
into self is valid for as long as self is around (and doesn’t move, which is
what Pin is for), but that isn’t a lifetime you can generally name. And while
the entire self-referential type may be 'static, the self-pointer isn’t—if it
were static, then even if you gave away that pointer to someone else, they
could continue to use it forever, even after self was gone! Take the type in
Listing 9-3 as an example; here we attempt to store the raw bytes that make
up a value alongside its stored representation.
struct Person<'a> {
name: &'a str,
age: usize,
}
struct Parsed {
bytes: [u8; 1024],
parsed: Person<'???>,
}
Listing 9-3: Trying, and failing, to name the lifetime of a self-referential reference
146 Chapter 9
The reference inside Person wants to refer to data stored in bytes in
Parsed, but there is no lifetime we can assign to that reference from Parsed.
It’s not 'static or something like 'self (which doesn’t exist), because if
Parsed is moved, the reference is no longer valid.
Since pointers do not have lifetimes, they circumvent this problem
because you don’t have to be able to name the lifetime. Instead, you just have
to make sure that when you do use the pointer, it’s still valid, which is what you
sign off on when you write unsafe { &*ptr }. In the example in Listing 9-3,
Person would instead store a *const str and then unsafely turn that into a &str
at the appropriate times when it can guarantee that the pointer is still valid.
A similar issue arises with a type like Arc, which has a pointer to a value
that’s shared for some duration, but that duration is known only at runtime
when the last Arc is dropped. The pointer is kind-of, sort-of 'static, but not
really—like in the self-referential case, the pointer is no longer valid when
the last Arc reference goes away, so the lifetime is more like 'self. In Arc’s
cousin, Weak, the lifetime is also “when the last Arc goes away,” but since a
Weak isn’t an Arc, the lifetime isn’t even tied to self. So, Arc and Weak both
use raw pointers internally.
Pointer Arithmetic
With raw pointers, you can do arbitrary pointer arithmetic, just like you
can in C, by using .offset(), .add(), and .sub() to move the pointer to any
byte that lives within the same allocation. This is most often used in highly
space-optimized data structures, like hash tables, where storing an extra
pointer for each element would add too much overhead and using slices
isn’t possible. Those are fairly niche use cases, and we won’t be talking
more about them in this book, but I encourage you to read the code for
hashbrown::RawTable (https://github.com/rust-lang/hashbrown/) if you want to
learn more!
The pointer arithmetic methods are unsafe to call even if you don’t
want to turn the pointer into a reference afterwards. There are a couple
of reasons for this, but the main one is that it is illegal to make a pointer
point beyond the end of the allocation that it originally pointed to. Doing
so triggers undefined behavior, and the compiler is allowed to decide to
eat your code and replace it with arbitrary nonsense that only a compiler
could understand. If you do use these methods, read the documentation
carefully!
To Pointer and Back Again
Often when you need to use pointers, it’s because you have some normal
Rust type, like a reference, a slice, or a string, and you have to move to the
world of pointers for a bit and then go back to the original normal type.
Some of the key standard library types therefore provide you with a way to
turn them into their raw constituent parts, such as a pointer and a length
for a slice, and a way to turn them back into the whole using those same
parts. For example, you can get a slice’s data pointer with as_ptr and its
length with []::len. You can then reconstruct the slice by providing those
Unsafe Code 147
same values to std::slice::from_raw_parts. Vec, Arc, and String have similar
methods that return a raw pointer to the underlying allocation, and Box has
Box::into_raw and Box::from_raw, which do the same thing.
Playing Fast and Loose with Types
Sometimes, you have a type T and want to treat it as some other type U.
Whether that’s because you need to do lightning-fast zero-copy parsing
or because you need to fiddle with some lifetimes, Rust provides you with
some (very unsafe) tools to do so.
The first and by far most widely used of these is pointer casting: you can
cast a *const T to any other *const U (and the same for mut), and you don’t
even need unsafe to do it. The unsafety comes into play only when you later
try to use the cast pointer as a reference, as you have to assert that the raw
pointer can in fact be used as a reference to the type it’s pointing to.
This kind of pointer type casting comes in particularly handy when working
with foreign function interfaces (FFI)—you can cast any Rust pointer to a
*const std::ffi::c_void or *mut std::ffi::c_void, and then pass that to a C function
that expects a void pointer. Similarly, if you get a void pointer from C that
you previously passed in, you can trivially cast it back into its original type.
Pointer casts are also useful when you want to interpret a sequence
of bytes as plain old data—types like integers, Booleans, characters, and
arrays, or #[repr(C)] structs of these—or write such types directly out as a
byte stream without serialization. There are a lot of safety invariants to keep
in mind if you want to try to do that, but we’ll leave that for later.
Calling Unsafe Functions
Arguably unsafe’s most commonly used feature is that it enables you to
call unsafe functions. Deeper down the stack, most of those functions are
unsafe because they operate on raw pointers at some fundamental level, but
higher up the stack you tend to interact with unsafety primarily through
function calls.
There’s really no limit to what calling an unsafe function might enable,
as it is entirely up to the libraries you interact with. But in general, unsafe
functions can be divided into three camps: those that interact with non-
Rust interfaces, those that skip safety checks, and those that have custom
invariants.
Foreign Function Interfaces
Rust lets you declare functions and static variables that are defined in a
language other than Rust using extern blocks (which we’ll discuss at length
in Chapter 11). When you declare such a block, you’re telling Rust that the
items appearing within it will be implemented by some external source when
the final program binary is linked, such as a C library you are integrating
with. Since externs exist outside of Rust’s control, they are inherently unsafe
to access. If you call a C function from Rust, all bets are off—it might overwrite
your entire memory contents and clobber all your neatly arranged
148 Chapter 9
references into random pointers into the kernel somewhere. Similarly, an
extern static variable could be modified by external code at any time, and
could be filled with all sorts of bad bytes that don’t reflect its declared type
at all. In an unsafe block, though, you can access externs to your heart’s
delight, as long as you’re willing to vouch for the other side of the extern
behaving according to Rust’s rules.
I’ll Pass on Safety Checks
Some unsafe operations can be made entirely safe by introducing additional
runtime checks. For example, accessing an item in a slice is unsafe
since you might try to access an item beyond the length of the slice. But,
given how common the operation is, it’d be unfortunate if indexing into a
slice was unsafe. Instead, the safe implementation includes bounds checks
that (depending on the method you use) either panic or return an Option if
the index you provide is out of bounds. That way, there is no way to cause
undefined behavior even if you pass in an index beyond the slice’s length.
Another example is in hash tables, which hash the key you provide rather
than letting you provide the hash yourself; this ensures that you’ll never try
to access a key using the wrong hash.
However, in the endless pursuit of ultimate performance, some developers
may find these safety checks add just a little too much overhead in their
tightest loops. To cater to situations where peak performance is paramount
and the caller knows that the indexes are in bounds, many data structures
provide alternate versions of particular methods without these safety
checks. Such methods usually include the word unchecked in the name to
indicate that they blindly trust the provided arguments to be safe and that
they do not do any of those pesky, slow safety checks. Some examples are
NonNull::new_unchecked, slice::get_unchecked, NonZero::new_unchecked, Arc::get
_mut_unchecked, and str::from_utf8_unchecked.
In practice, the safety and performance trade-off for unchecked methods
is rarely worth it. As always with performance optimization, measure
first, then optimize.
Custom Invariants
Most uses of unsafe rely on custom invariants to some degree. That is, they
rely on invariants beyond those provided by Rust itself, which are specific to
the particular application or library. Since so many functions fall into this
category, it’s hard to give a good general summary of this class of unsafe
functions. Instead, I’ll give some examples of unsafe functions with custom
invariants that you may come across in practice and want to use:
MaybeUninit::assume_init
The MaybeUninit type is one of the few ways in which you can store
values that are not valid for their type in Rust. You can think of a
MaybeUninit<T> as a T that may not be legal to use as a T at the moment.
For example, a MaybeUninit<NonNull> is allowed to hold a null pointer,
a MaybeUninit<Box> is allowed to hold a dangling heap pointer, and a
Unsafe Code 149
MaybeUninit<bool> is allowed to hold the bit pattern for the number 3
(normally it must be 0 or 1). This comes in handy if you are constructing
a value bit by bit or are dealing with zeroed or uninitialized memory
that will eventually be made valid (such as by being filled through
a call to std::io::Read::read). The assume_init function asserts that the
MaybeUninit now holds a valid value for the type T and can therefore be
used as a T.
ManuallyDrop::drop
The ManuallyDrop type is a wrapper type around a type T that does not
drop that T when the ManuallyDrop is dropped. Or, phrased differently, it
decouples the dropping of the outer type (ManuallyDrop) from the dropping
of the inner type (T). It implements safe access to the T through
DerefMut<Target = T> but also provides a drop method (separately from
the drop method of the Drop trait) to drop the wrapped T without dropping
the ManuallyDrop. That is, the drop function takes &mut self despite
dropping the T, and so leaves the ManuallyDrop behind. This comes in
handy if you have to explicitly drop a value that you cannot move, such
as in implementations of the Drop trait. Once that value is dropped, it
is no longer safe to try to access the T, which is why the call to drop is
unsafe—it asserts that the T will never be accessed again.
std::ptr::drop_in_place
drop_in_place lets you call a value’s destructor directly through a pointer
to that value. This is unsafe because the pointee will be left behind
after the call, so if some code then tries to dereference the pointer, it’ll
be in for a bad time! This method is particularly useful when you may
want to reuse memory, such as in an arena allocator, and need to drop
an old value in place without reclaiming the surrounding memory.
Waker::from_raw
In Chapter 8 we talked about the Waker type and how it is made up of a
data pointer and a RawWaker that holds a manually implemented vtable.
Once a Waker has been constructed, the raw function pointers in the
vtable, such as wake and drop, can be called from safe code (through
Waker::wake and drop(waker), respectively). Waker::from_raw is where the
asynchronous executor asserts that all the pointers in its vtable are
in fact valid function pointers that follow the contract set forth in the
documentation of RawWakerVTable.
std::hint::unreachable_unchecked
The hint module holds functions that give hints to the compiler about
the surrounding code but do not actually produce any machine code.
The unreachable_unchecked function in particular tells the compiler that it
is impossible for the program to reach a section of the code at runtime.
This in turn allows the compiler to make optimizations based on that
150 Chapter 9
knowledge, such as eliminating conditional branches to that location.
Unlike the unreachable! macro, which panics if the code does reach the
line in question, the effects of an erroneous unreachable_unchecked are
hard to predict. The compiler optimizations may cause peculiar and
hard-to-debug behavior, not to mention that your program will continue
running when something it believed to be true was not!
std::ptr::{read,write}_{unaligned,volatile}
The ptr module holds a number of functions that let you work with odd
pointers—those that do not meet the assumptions that Rust generally
makes about pointers. The first of these functions are read_unaligned and
write_unaligned, which let you access pointers that point to a T even if
that T is not stored according to T’s alignment (see the section on alignment
in Chapter 2). This might happen if the T is contained directly in
a byte array or is otherwise packed in with other values without proper
padding. The second notable pair of functions is read_volatile and
write_volatile, which let you operate on pointers that don’t point to
normal memory. Concretely, these functions will always access the given
pointer (they won’t be cached in a register, for example, even if you
read the same pointer twice in a row), and the compiler won’t reorder
the volatile accesses relative to other volatile accesses. Volatile operations
come in handy when working with pointers that aren’t backed
by normal DRAM memory—we’ll discuss this further in Chapter 11.
Ultimately, these methods are unsafe because they dereference the
given pointer (and to an owned T, at that), so you as the caller need to
sign off on all the contracts associated with doing so.
std::thread::Builder::spawn_unchecked
The normal thread::spawn that we know and love requires that the
provided closure is 'static. That bound stems from the fact that the
spawned thread might run for an indeterminate amount of time; if
we were allowed to use a reference to, say, the caller’s stack, the caller
might return well before the spawned thread exits, rendering the reference
invalid. Sometimes, however, you know that some non-'static
value in the caller will outlive the spawned thread. This might happen
if you join the thread before dropping the value in question, or if the
value is dropped only strictly after you know the spawned thread will
no longer use it. That’s where spawn_unchecked comes in—it does not
have the 'static bound and thus lets you implement those use cases as
long as you’re willing to sign the contract saying that no unsafe accesses
will happen as a result. Be careful of panics, though; if the caller panics,
it might drop values earlier than you planned and cause undefined
behavior in the spawned thread!
Note that all of these methods (and indeed all unsafe methods in the
standard library) provide explicit documentation for their safety invariants,
as should be the case for any unsafe method.
Unsafe Code 151
Implementing Unsafe Traits
Unsafe traits aren’t unsafe to use, but unsafe to implement. This is because
unsafe code is allowed to rely on the correctness (defined by the trait’s documentation)
of the implementation of unsafe traits. For example, to implement
the unsafe trait Send, you need to write unsafe impl Send for .... Like
unsafe functions, unsafe traits generally have custom invariants that are (or
at least should be) specified in the documentation for the trait. Thus, it’s
difficult to cover unsafe traits as a group, so here too I’ll give some common
examples from the standard library that are worth going over.
Send and Sync
The Send and Sync traits denote that a type is safe to send or share across
thread boundaries, respectively. We’ll talk more about these traits in
Chapter 10, but for now what you need to know is that they are auto-traits,
and so they’ll usually be implemented for most types for you by the compiler.
But, as tends to be the case with auto-traits, Send and Sync will not be
implemented if any members of the type in question are not themselves Send
or Sync.
In the context of unsafe code, this problem occurs primarily due to
raw pointers, which are neither Send nor Sync. At first glance, this might
seem reasonable: the compiler has no way to know who else may have a raw
pointer to the same value or how they may be using it at the moment, so
how can the type be safe to send across threads? Now that we’re seasoned
unsafe developers though, that argument seems weak—after all, dereferencing
a raw pointer is already unsafe, so why should handling the invariants of
Send and Sync be any different?
Strictly speaking, raw pointers could be both Send and Sync. The problem
is that if they were, the types that contain raw pointers would automatically
be Send and Sync themselves, even though their author might not
realize that was the case. The developer might then unsafely dereference
the raw pointers without ever thinking about what would happen if those
types were sent or shared across thread boundaries, and thus inadvertently
introduce undefined behavior. Instead, the raw pointer types block these
automatic implementations as an additional safeguard to unsafe code to
make authors explicitly sign the contract that they have also followed the
Send and Sync invariants.
NOTE A common mistake with unsafe implementations of Send and Sync is to forget to add
bounds to generic parameters: unsafe impl<T: Send> Send for MyUnsafeType<T> {}.
GlobalAlloc
The GlobalAlloc trait is how you implement a custom memory allocator
in Rust. We won’t talk too much about that topic in this book, but the
trait itself is interesting. Listing 9-4 gives the required methods for the
GlobalAlloc trait.
152 Chapter 9
pub unsafe trait GlobalAlloc {
pub unsafe fn alloc(&self, layout: Layout) -> *mut u8;
pub unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);
}
Listing 9-4: The GlobalAlloc trait with its required methods
At its core, the trait has one method for allocating a new chunk of
memory, alloc, and one for deallocating a chunk of memory, dealloc. The
Layout argument describes the type’s size and alignment, as we discussed in
Chapter 2. Each of those methods is unsafe and carries a number of safety
invariants that its callers must uphold.
GlobalAlloc itself is also unsafe because it places restrictions on the
implementer of the trait, not the caller of its methods. Only the unsafety
of the trait ensures that implementers agree to uphold the invariants that
Rust itself assumes of its memory allocator, such as in the standard library’s
implementation of Box. If the trait was not unsafe, an implementer could
safely implement GlobalAlloc in a way that produced unaligned pointers or
incorrectly sized allocations, which would trigger unsafety in otherwise safe
code that assumes that allocations are sane. This would break the rule that
safe code should not be able to trigger memory unsafety in other safe code,
and thus cause all sorts of mayhem.
Surprisingly Not Unpin
The Unpin trait is not unsafe, which comes as a surprise to many Rust developers.
It may even come as a surprise to you after reading Chapter 8. After
all, the trait is supposed to ensure that self-referential types aren’t invalidated
if they’re moved after they have established internal pointers (that is,
after they’ve been placed in a Pin). It seems strange, then, that Unpin can be
used to safely remove a type from a Pin.
There are two main reasons why Unpin isn’t an unsafe trait. First, it’s
unnecessary. Implementing Unpin for a type that you control does not grant
you the ability to safely pin or unpin a !Unpin type; that still requires unsafety
in the form of a call to Pin::new_unchecked or Pin::get_unchecked_mut. Second,
there is already a safe way for you to unpin any type you control: the Drop trait!
When you implement Drop for a type, you’re passed &mut self, even if your
type was previously stored in a Pin and is !Unpin, all without any unsafety. That
potential for unsafety is covered by the invariants of Pin::new_unchecked, which
must be upheld to create a Pin of such an !Unpin type in the first place.
When to Make a Trait Unsafe
Few traits in the wild are unsafe, but those that are all follow the same pattern.
A trait should be unsafe if safe code that assumes that trait is implemented
correctly can exhibit memory unsafety if the trait is not implemented
correctly.
The Send trait is a good example to keep in mind here—safe code can
easily spawn a thread and pass a value to that spawned thread, but if Rc were
Unsafe Code 153
Send, that sequence of operations could trivially lead to memory unsafety.
Consider what would happen if you cloned an Rc<Box> and sent it to another
thread: the two threads could easily both try to deallocate the Box since they
do not correctly synchronize access to the Rc’s reference count.
The Unpin trait is a good counterexample. While it is possible to write
unsafe code that triggers memory unsafety if Unpin is implemented incorrectly,
no entirely safe code can trigger memory unsafety due to an implementation
of Unpin. It’s not always easy to determine that a trait can be safe
(indeed, the Unpin trait was unsafe throughout most of the RFC process),
but you can always err on the side of making the trait unsafe, and then
make it safe later on if you realize that is the case! Just keep in mind that
that is a backward incompatible change.
Also keep in mind that just because it feels like an incorrect (or even
malicious) implementation of a trait would cause a lot of havoc, that’s not
necessarily a good reason to make it unsafe. The unsafe marker should first
and foremost be used to highlight cases of memory unsafety, not just something
that can trigger errors in business logic. For example, the Eq, Ord,
Deref, and Hash traits are all safe, even though there is likely much code out
in the world that would go haywire if faced with a malicious implementation
of, say, Hash that returned a different random hash each time it was
called. This extends to unsafe code too—there is almost certainly unsafe
code out there that would be memory-unsafe in the presence of such an
implementation of Hash—but that does not mean Hash should be unsafe.
The same is true for an implementation of Deref that dereferenced to a different
(but valid) target each time. Such unsafe code would be relying on a
contract of Hash or Deref that does not actually hold; Hash never claimed that
it was deterministic, and neither did Deref. Or rather, the authors of those
implementations never used the unsafe keyword to make that claim!
NOTE An important implication of traits like Eq, Hash, and Deref being safe is that unsafe
code can rely only on the safety of safe code, not its correctness. This applies not only
to traits, but to all unsafe/safe code interactions.
Great Responsibility
So far, we’ve looked mainly at the various things that you are allowed to do
with unsafe code. But unsafe code is allowed to do those things only if it
does so safely. Even though unsafe code can, say, dereference a raw pointer,
it must do so only if it knows that pointer is valid as a reference to its pointee
at that moment in time, subject to all of Rust’s normal requirements of
references. In other words, unsafe code is given access to tools that could be
used to do unsafe things, but it must do only safe things using those tools.
That, then, raises the question of what safe even means in the first
place. When is it safe to dereference a pointer? When is it safe to transmute
between two different types? In this section, we’ll explore some of the key
Unsafe Code 155
dramatically change the semantics of the code, or even miscompile surrounding
code. What that does to your program is entirely dependent on
what the code in question does. The unpredictable impact of undefined
behavior is the reason why all undefined behavior should be considered a
serious bug, no matter how it currently manifests.
WHY UNDEFINED BEHAVIOR?
An argument that often comes up in conversations about undefined behavior
is that the compiler should emit an error if code exhibits undefined behavior
instead of doing something weird and unpredictable. That way, it would be
near-impossible to write bad unsafe code!
Unfortunately, that would be impossible because undefined behavior is
rarely explicit or obvious. Instead, what usually happens is that the compiler
simply applies optimizations under the assumption that the code follows the
specification. Should that turn out to not be the case—which is rarely clear until
runtime—it’s difficult to predict what the effect might be. Maybe the optimization
is still valid, and nothing bad happens; but maybe it’s not, and the semantics
of the code end up slightly different from that of the unoptimized version.
If we were to tell compiler developers that they aren’t allowed to assume
anything about the underlying code, what we’d really be telling them is that
they cannot perform a wide range of the optimizations that they implement with
great success today. Nearly all sophisticated optimizations make assumptions
about what the code in question can and cannot do according to the language
specification.
If you want a good illustration of how specifications and compiler optimizations
interact in strange ways where it’s hard to assign blame, I recommend
reading through Ralf Jung’s blog post “We Need Better Language Specs”
(https://www.ralfj.de/blog/2020/12/14/provenance.html).
Validity
Perhaps the most important concept to understand before writing unsafe
code is validity, which dictates the rules for what values inhabit a given
type—or, less formally, the rules for a type’s values. The concept is simpler
than it sounds, so let’s dive into some concrete examples.
Reference Types
Rust is very strict about what values its reference types can hold. Specifically,
references must never dangle, must always be aligned, and must always
point to a valid value for their target type. In addition, a shared and an
exclusive reference to a given memory location can never exist at the same
time, and neither can multiple exclusive references to a location. These
156 Chapter 9
rules apply regardless of whether your code uses the references or not—you
are not allowed to create a null reference even if you then immediately discard
it!
Shared references have the additional constraint that the pointee is
not allowed to change during the reference’s lifetime. That is, any value
the pointee contains must remain exactly the same over its lifetime. This
applies transitively, so if you have an & to a type that contains a *mut T, you
are not allowed to ever mutate the T through that *mut even though you
could write code to do so using unsafe. The only exception to this rule is a
value wrapped by the UnsafeCell type. All other types that provide interior
mutability, like Cell, RefCell, and Mutex, internally use an UnsafeCell.
An interesting result of Rust’s strict rules for references is that for many
years, it was impossible to safely take a reference to a field of a packed or
partially uninitialized struct that used repr(Rust). Since repr(Rust) leaves
a type’s layout undefined, the only way to get the address of a field was by
writing &some_struct.field as *const _. However, if some_struct is packed,
then some_struct.field may not be aligned, and thus creating an & to it is
illegal! Further, if some_struct isn’t fully initialized, then the some_struct reference
itself cannot exist! In Rust 1.51.0, the ptr::addr_of! macro was stabilized,
which added a mechanism for directly obtaining a reference to a field
without first creating a reference, fixing this particular problem. Internally,
it is implemented using something called raw references (not to be confused
with raw pointers), which directly create pointers to their operands rather
than going via a reference. Raw references were introduced in RFC 2582
but haven’t been stabilized themselves yet at the time of writing.
Primitive Types
Some of Rust’s primitive types have restrictions on what values they can
hold. For example, a bool is defined as being 1 byte large but is only allowed
to hold the value 0x00 or the value 0x01, and a char is not allowed to hold
a surrogate or a value above char::MAX. Most of Rust’s primitive types, and
indeed most of Rust’s types overall, also cannot be constructed from uninitialized
memory. These restrictions may seem arbitrary, but again often stem
from the need to enable optimizations that wouldn’t be possible otherwise.
A good illustration of this is the niche optimization, which we discussed
briefly when talking about pointer types earlier in this chapter. To recap,
the niche optimization tucks away the enum discriminant value in the
wrapped type in certain cases. For example, since a reference cannot ever
be all zeros, an Option<&T> can use all zeros to represent None, and thus avoid
spending an extra byte (plus padding) to store the discriminator byte. The
compiler can optimize Booleans in the same way and potentially take it
even further. Consider the type Option<Option<bool>>>. Since the compiler
knows that the bool is either 0x00 or 0x01, it’s free to use 0x02 to represent
Some(None) and 0x03 to represent None. Very nice and tidy! But if someone
were to come along and treat the byte 0x03 as a bool, and then place that
value in an Option<Option<bool>> optimized in this way, bad things would
happen.
Unsafe Code 157
It bears repeating that it’s not important whether the Rust compiler currently
implements this optimization or not. The point is that it is allowed to,
and therefore any unsafe code you write must conform to that contract or
risk hitting a bug later on should the behavior change.
Owned Pointer Types
Types that point to memory they own, like Box and Vec, are generally subject
to the same optimizations as if they held an exclusive reference to the
pointed-to memory unless they’re explicitly accessed through a shared
reference. Specifically, the compiler assumes that the pointed-to memory
is not shared or aliased elsewhere, and makes optimizations based on
that assumption. For example, if you extracted the pointer from a Box and
then constructed two Boxes from that same pointer and wrapped them in
ManuallyDrop to prevent a double-free, you’d likely be entering undefined
behavior territory. That’s the case even if you only ever access the inner
type through shared references. (I say “likely” because this isn’t fully settled
in the language reference yet, but a rough consensus has arisen.)
Storing Invalid Values
Sometimes you need to store a value that isn’t currently valid for its type.
The most common example of this is if you want to allocate a chunk of
memory for some type T and then read in the bytes from, say, the network.
Until all the bytes have been read in, the memory isn’t going to be a valid T.
Even if you just tried to read the bytes into a slice of u8, you would have to
zero those u8s first, because constructing a u8 from uninitialized memory is
also undefined behavior.
The MaybeUninit<T> type is Rust’s mechanism for working with values
that aren’t valid. A MaybeUninit<T> stores exactly a T (it is #[repr(transparent)]),
but the compiler knows to make no assumptions about the validity of that T.
It won’t assume that references are non-null, that a Box<T> isn’t dangling, or
that a bool is either 0 or 1. This means it’s safe to hold a T backed by uninitialized
memory inside a MaybeUninit (as the name implies). MaybeUninit is
also a very useful tool in other unsafe code where you have to temporarily
store a value that may be invalid. Maybe you have to store an aliased Box<T>
or stash a char surrogate for a second—MaybeUninit is your friend.
You will generally do only three things with a MaybeUninit: create it using
the MaybeUninit::uninit method, write to its contents using MaybeUninit::as
_mut_ptr, or take the inner T once it is valid again with MaybeUninit::assume_init.
As its name implies, uninit creates a new MaybeUninit<T> of the same size as
a T that initially holds uninitialized memory. The as_mut_ptr method gives
you a raw pointer to the inner T that you can then write to; nothing stops
you from reading from it, but reading from any of the uninitialized bits is
undefined behavior. And finally, the unsafe assume_init method consumes
the MaybeUninit<T> and returns its contents as a T following the assertion that
the backing memory now makes up a valid T.
Listing 9-5 shows an example of how we might use MaybeUninit to safely
initialize a byte array without explicitly zeroing it.
158 Chapter 9
fn fill(gen: impl FnMut() -> Option<u8>) {
let mut buf = [MaybeUninit::<u8>::uninit(); 4096];
let mut last = 0;
for (i, g) in std::iter::from_fn(gen).take(4096).enumerate() {
buf[i] = MaybeUninit::new(g);
last = i + 1;
}
// Safety: all the u8s up to last are initialized.
let init: &[u8] = unsafe {
MaybeUninit::slice_assume_init_ref(&buf[..last])
};
// ... do something with init ...
}
Listing 9-5: Using MaybeUninit to safely initialize an array
While we could have declared buf as [0; 4096] instead, that would
require the function to first write out all those zeros to the stack before
executing, even if it’s going to overwrite them all again shortly thereafter.
Normally that wouldn’t have a noticeable impact on performance, but if
this was in a sufficiently hot loop, it might! Here, we instead allow the array
to keep whatever values happened to be on the stack when the function was
called, and then overwrite only what we end up needing.
NOTE Be careful with dropping partially initialized memory. If a panic causes an unexpected
early drop before the MaybeUninit<T> has been fully initialized, you’ll have to
take care to drop only the parts of T that are now valid, if any. You can just drop the
MaybeUninit and have the backing memory forgotten, but if it holds, say, a Box, you
might end up with a memory leak!
Panics
An important and often overlooked aspect of ensuring that code using
unsafe operations is safe is that the code must also be prepared to handle
panics. In particular, as we discussed briefly in Chapter 5, Rust’s default
panic handler on most platforms will not crash your program on a panic
but will instead unwind the current thread. An unwinding panic effectively
drops everything in the current scope, returns from the current function,
drops everything in the scope that enclosed the function, and so on,
all the way down the stack until it hits the first stack frame for the current
thread. If you don’t take unwinding into account in your unsafe code, you
may be in for trouble. For example, consider the code in Listing 9-6, which
tries to efficiently push many values into a Vec at once.
impl<T: Default> Vec<T> {
pub fn fill_default(&mut self) {
let fill = self.capacity() - self.len();
if fill == 0 { return; }
let start = self.len();
unsafe {
self.set_len(start + n);
Unsafe Code 159
for i in 0..fill {
*self.get_unchecked_mut(start + i) = T::default();
}
}
}
}
Listing 9-6: A seemingly safe method for filling a vector with Default values
Consider what happens to this code if a call to T::default panics. First,
fill_default will drop all its local values (which are just integers) and then
return. The caller will then do the same. At some point up the stack, we
get to the owner of the Vec. When the owner drops the vector, we have a
problem: the length of the vector now indicates that we own more Ts than
we actually produced due to the call to set_len. For example, if the very
first call to T::default panicked when we aimed to fill eight elements, that
means Vec::drop will call drop on eight Ts that actually contain uninitialized
memory!
The fix in this case is simple: the code must update the length after writing
all the elements. We wouldn’t have realized there was a problem if we
didn’t carefully consider the effect of unwinding panics on the correctness
of our unsafe code.
When you’re combing through your code for these kinds of problems,
you’ll want to look out for any statements that may panic, and consider
whether your code is safe if they do. Alternatively, check whether you can
convince yourself that the code in question will never panic. Pay particular
attention to anything that calls user-provided code—in those cases, you have
no control over the panics and should assume that the user code will panic.
A similar situation arises when you use the ? operator to return early
from a function. If you do this, make sure that your code is still safe if it
does not execute the remainder of the code in the function. It’s rarer for ?
to catch you off guard since you opted into it explicitly, but it’s worth keeping
an eye out for.
Casting
As we discussed in Chapter 2, two different types that are both #[repr(Rust)]
may be represented differently in memory even if they have fields of the
same type and in the same order. This in turn means that it’s not always
obvious whether it is safe to cast between two different types. In fact, Rust
doesn’t even guarantee that two instances of a single type with generic
arguments that are themselves laid out the same way are represented the
same way. For example, in Listing 9-7, A and B are not guaranteed to have
the same in-memory representation.
struct Foo<T> {
one: bool,
two: PhantomData<T>,
}
struct Bar;
struct Baz;
160 Chapter 9
type A = Foo<Bar>;
type B = Foo<Baz>;
Listing 9-7: Type layout is not predictable.
The lack of guarantees for repr(Rust) is important to keep in mind
when you do type casting in unsafe code—just because two types feel like
they should be interchangeable, that is not necessarily the case. Casting
between two types that have different representations is a quick path to
undefined behavior. At the time of writing, the Rust community is actively
working out the exact rules for how types are represented, but for now, very
few guarantees are given, so that’s what we have to work with.
Even if identical types were guaranteed to have the same in-memory
representation, you’d still run into the same problem when types are
nested. For example, while UnsafeCell<T>, MaybeUninit<T>, and T all really
just hold a T, and you can cast between them to your heart’s delight, that
goes out the window once you have, for example, an Option<MaybeUninit<T>>.
Though Option<T> may be able to take advantage of the niche optimization
(using some invalid value of T to represent None for the Option), MaybeUninit<T>
can hold any bit pattern, so that optimization does not apply, and an extra
byte must be kept for the Option discriminator.
It’s not just optimizations that can cause layouts to diverge once wrapper
types come into play. As an example, take the code in Listing 9-8; here,
the layout of Wrapper<PhantomData<u8>> and Wrapper<PhantomData<i8>> is completely
different even though the provided types are both empty!
struct Wrapper<T: SneakyTrait> {
item: T::Sneaky,
iter: PhantomData<T>,
}
trait SneakyTrait {
type Sneaky;
}
impl SneakyTrait for PhantomData<u8> {
type Sneaky = ();
}
impl SneakyTrait for PhantomData<i8> {
type Sneaky = [u8; 1024];
}
Listing 9-8: Wrapper types make casting hard to get right.
All of this isn’t to say that you can never cast types in Rust. Things get a
lot easier, for example, when you control all of the types involved and their
trait implementations, or if types are #[repr(C)]. You just need to be aware
that Rust gives very few guarantees about in-memory representations, and
write your code accordingly!
The Drop Check
The Rust borrow checker is, in essence, a sophisticated tool for ensuring
the soundness of code at compile time, which is in turn what gives Rust a
Unsafe Code 161
way to express code being “safe.” How exactly the borrow checker does its
job is beyond the scope of this book, but one check, the drop check, is worth
going through in some detail since it has some direct implications for
unsafe code. To understand drop checking, let’s put ourselves in the Rust
compiler’s shoes for a second and look at two code snippets. First, take a
look at the little three-liner in Listing 9-9 that takes a mutable reference to
a variable and then mutates that same variable right after.
let mut x = true;
let foo = Foo(&mut x);
x = false;
Listing 9-9: The implementation of Foo dictates whether this code should compile
Without knowing the definition of Foo, can you say whether this code
should compile or not? When we set x = false, there is still a foo hanging
around that will be dropped at the end of the scope. We know that foo contains
a mutable borrow of x, which would indicate that the mutable borrow
that’s necessary to modify x is illegal. But what’s the harm in allowing it? It
turns out that allowing the mutation of x is problematic only if Foo implements
Drop—if Foo doesn’t implement Drop, then we know that Foo won’t
touch the reference to x after its last use. Since that last use is before we
need the exclusive reference for the assignment, we can allow the code! On
the other hand, if Foo does implement Drop, we can’t allow this code, since
the Drop implementation may use the reference to x.
Now that you’re warmed up, take a look at Listing 9-10. In this not-sostraightforward
code snippet, the mutable reference is buried even deeper.
fn barify<’a>(_: &’a mut i32) -> Bar<Foo<’a>> { .. }
let mut x = true;
let foo = barify(&mut x);
x = false;
Listing 9-10: The implementations of both Foo and Bar dictate whether this code should
compile
Again, without knowing the definitions of Foo and Bar, can you say
whether this code should compile or not? Let’s consider what happens if
Foo implements Drop but Bar does not, since that’s the most interesting case.
Usually, when a Bar goes out of scope, or otherwise gets dropped, it’ll still
have to drop Foo, which in turn means that the code should be rejected for
the same reason as before: Foo::drop might access the reference to x. However,
Bar may not contain a Foo directly at all, but instead just a PhantomData<Foo<'a>>
or a &'static Foo<'a>, in which case the code is actually okay—even though
the Bar is dropped, Foo::drop is never invoked, and the reference to x is never
accessed. This is the kind of code we want the compiler to accept because a
human will be able to identify that it’s okay, even if the compiler finds it difficult
to detect that this is the case.
The logic we’ve just walked through is the drop check. Normally it
doesn’t affect unsafe code too much as its default behavior matches user
expectations, with one major exception: dangling generic parameters.
162 Chapter 9
Imagine that you’re implementing your own Box<T> type, and someone
places a &mut x into it as we did in Listing 9-9. Your Box type needs to implement
Drop to free memory, but it doesn’t access T beyond dropping it. Since
dropping a &mut does nothing, it should be entirely fine for code to access
&mut x again after the last time the Box is accessed but before it’s dropped!
To support types like this, Rust has an unstable feature called dropck_eyepatch
(because it makes the drop check partially blind). The feature is likely
to remain unstable forever and is intended to serve only as a temporary
escape hatch until a proper mechanism is devised. The dropck_eyepatch feature
adds a #[may_dangle] attribute, which you can add as a prefix for generic
lifetimes and types in a type’s Drop implementation to tell the drop check
machinery that you won’t use the annotated lifetime or type beyond dropping
it. You use it by writing:
unsafe impl<#[may_dangle] T> Drop for ..
This escape hatch allows a type to declare that a given generic parameter
isn’t used in Drop, which enables use cases like Box<&mut T>. However,
it also introduces a new problem if your Box<T> holds a raw heap pointer,
*mut T, and allows T to dangle using #[may_dangle]. Specifically, the *mut T
makes Rust’s drop check think that your Box<T> doesn’t own a T, and thus
that it doesn’t call T::drop either. Combined with the may_dangle assertion
that we don’t access T when the Box<T> is dropped, the drop check now concludes
that it’s fine to have a Box<T> where the T doesn’t live until the Box
is dropped (like our shortened &mut x in Listing 9-10). But that’s not true,
since we do call T::drop, which may itself access, say, a reference to said x.
Luckily, the fix is simple: we add a PhantomData<T> to tell the drop check
that even though the Box<T> doesn’t hold any T, and won’t access T on drop,
it does still own a T and will drop one when the Box is dropped. Listing 9-11
shows what our hypothetical Box type would look like.
struct Box<T> {
t: NonNull<T>, // NonNull not *mut for covariance (Chapter 1)
_owned: PhantomData<T>, // For drop check to realize we drop a T
}
unsafe impl<#[may_dangle] T> for Box<T> { /* ... */ }
Listing 9-11: A definition for Box that is maximally flexible in terms of the drop check
This interaction is subtle and easy to miss, but it arises only when you
use the unstable #[may_dangle] attribute. Hopefully this subsection will serve
as a warning so that when you see unsafe impl Drop in the wild in the future,
you’ll know to look for a PhantomData<T> as well!
NOTE Another consideration for unsafe code concerning Drop is to make sure that you have
a Type<T> that lets T continue to live after self is dropped. For example, if you’re
implementing delayed garbage collection, you need to also add T: 'static. Otherwise,
if T = WriteOnDrop<&mut U>, the later access or drop of T could trigger undefined
behavior!
Unsafe Code 163
Coping with Fear
With this chapter mostly behind you, you may now be more afraid of unsafe
code than you were before you started. While that is understandable, it’s
important to stress that it’s not only possible to write safe unsafe code, but
most of the time it’s not even that difficult. The key is to make sure that you
handle unsafe code with care; that’s half the struggle. And be really sure
that there isn’t a safe implementation you can use instead before resorting
to unsafe.
In the remainder of this chapter, we’ll look at some techniques and
tools that can help you be more confident in the correctness of your unsafe
code when there’s no way around it.
Manage Unsafe Boundaries
It’s tempting to reason about unsafety locally; that is, to consider whether
the code in the unsafe block you just wrote is safe without thinking too
much about its interaction with the rest of the codebase. Unfortunately,
that kind of local reasoning often comes back to bite you. A good example
of this is the Unpin trait—you may write some code for your type that uses
Pin::new_unchecked to produce a pinned reference to a field of the type, and
that code may be entirely safe when you write it. But then at some later
point in time, you (or someone else) might add a safe implementation of
Unpin for said type, and suddenly the unsafe code is no longer safe, even
though it’s nowhere near the new impl!
Safety is a property that can be checked only at the privacy boundary of
all code that relates to the unsafe block. Privacy boundary here isn’t so much
a formal term as an attempt at describing “any part of your code that can
fiddle with the unsafe bits.” For example, if you declare a public type Foo in
a module bar that is marked pub or pub(crate), then any other code in the
same crate can implement methods on and traits for Foo. So, if the safety
of your unsafe code depends on Foo not implementing particular traits or
methods with particular signatures, you need to remember to recheck the
safety of that unsafe block any time you add an impl for Foo. If, on the other
hand, Foo is not visible to the entire crate, then a much smaller set of scopes
is able to add problematic implementations, and thus, the risk of accidentally
adding an implementation that breaks the safety invariants goes down
accordingly. If Foo is private, then only the current module and any submodules
can add such implementations.
The same rule applies to access to fields: if the safety of an unsafe block
depends on certain invariants over a type’s fields, then any code that can
touch those fields (including safe code) falls within the privacy boundary
of the unsafe block. Here, too, minimizing the privacy boundary is the
best approach—code that cannot get to the fields cannot mess up your
invariants!
Because unsafe code often requires this wide-reaching reasoning,
it’s best practice to encapsulate the unsafety in your code as best you can.
Provide the unsafety in the form of a single module, and strive to give that
164 Chapter 9
module an interface that is entirely safe. That way you only need to audit the
internals of that module for your invariants. Or better yet, stick the unsafe
bits in their own crate so that you can’t leave any holes open by accident!
It’s not always possible to fully encapsulate complex unsafe interactions
to a single, safe interface, however. When that’s the case, try to narrow
down the parts of the public interface that have to be unsafe so that you
have only a very small number of them, give them names that clearly communicate
that care is needed, and then document them rigorously.
It is sometimes tempting to remove the unsafe marker on internal APIs
so that you don’t have to stick unsafe {} throughout your code. After all,
inside your code you know never to invoke frobnify if you’ve previously
called bazzify, right? Removing the unsafe annotation can lead to cleaner
code but is usually a bad decision in the long run. A year from now, when
your codebase has grown, you’ve paged out some of the safety invariants,
and you “ just want to hack together this one feature real quick,” chances
are that you’ll inadvertently violate one of those invariants. And since
you don’t have to type unsafe, you won’t even think to check. Plus, even
if you never make mistakes, what about other contributors to your code?
Ultimately, cleaner code is not a good enough argument to remove the
intentionally noisy unsafe marker.
Read and Write Documentation
It goes without saying that if you write an unsafe function, you must document
the conditions under which that function is safe to call. Here, both
clarity and completeness are important. Don’t leave any invariants out, even
if you’ve already written them somewhere else. If you have a type or module
that requires certain global invariants—invariants that must always hold for
all uses of the type—then remind the reader that they must also uphold the
global invariants in every unsafe function’s documentation too. Developers
often read documentation in an ad hoc, on-demand manner, so you can
assume they have probably not read your carefully written module-level
documentation and need to be given a nudge to do so.
What may be less obvious is that you should also document all unsafe
implementations and blocks—think of this as providing proof that you
do indeed uphold the contract the operation in question requires. For
example, slice::get_unchecked requires that the provided index is within the
bounds of the slice; when you call that method, put a comment just above
it explaining how you know that the index is in fact guaranteed to be in
bounds. In some cases, the invariants that the unsafe block requires are
extensive, and your comments may get long. That’s a good thing. I have
caught mistakes many times by trying to write the safety comment for an
unsafe block and realizing halfway through that I actually don’t uphold
a key invariant. You’ll also thank yourself a year down the road when you
have to modify this code and ensure it’s still safe. And so will the contributor
to your project who just stumbled across this unsafe call and wants to
understand what’s going on.
Unsafe Code 165
Before you get too deep into writing unsafe code, I also highly recommend
that you go read the Rustonomicon (https://doc.rust-lang.org/nomicon/)
cover to cover. There are so many details that are easy to miss, and will
come back to bite you if you’re not aware of them. We’ve covered many
of them in this chapter, but it never hurts to be more aware. You should
also make liberal use of the Rust reference whenever you’re in doubt. It’s
added to regularly, and chances are that if you’re even slightly unsure about
whether some assumption you have is right, the reference will call it out. If
it doesn’t, consider opening an issue so that it’ll be added!
Check Your Work
Okay, so you’ve written some unsafe code, you’ve double- and triple-checked
all the invariants, and you think it’s ready to go. Before you put it into production,
there are some automated tools that you should run your test suite
through (you have a test suite, right?).
The first of these is Miri, the mid-level intermediate representation
interpreter. Miri doesn’t compile your code into machine code but instead
interprets the Rust code directly. This provides Miri with far more visibility
into what your program is doing, which in turn allows it to check that your
program doesn’t do anything obviously bad, like read from uninitialized
memory. Miri can catch a lot of very subtle and Rust-specific bugs and is a
lifesaver for anyone writing unsafe code.
Unfortunately, because Miri has to interpret the code to execute it, code
run under Miri often runs orders of magnitude slower than its compiled
counterpart. For that reason, Miri should really be used only to execute your
test suite. It can also check only the code that actually runs, and thus won’t
catch issues in code paths that your test suite doesn’t reach. You should think
of Miri as an extension of your test suite, not a replacement for it.
There are also tools known as sanitizers, which instrument machine code
to detect erroneous behavior at runtime. The overhead and fidelity of these
tools vary greatly, but one widely loved tool is Google’s AddressSanitizer.
It detects a large number of memory errors, such as use-after-free, buffer
overflows, and memory leaks, all of which are common symptoms of incorrect
unsafe code. Unlike Miri, these tools operate on machine code and thus
tend to be fairly fast—usually within the same order of magnitude. But like
Miri, they are constrained to analyzing the code that actually runs, so here
too a solid test suite is vital.
The key to using these tools effectively is to automate them through
your continuous integration pipeline so they’re run for every change, and
to ensure that you add regression tests over time as you discover errors.
The tools get better at catching problems as the quality of your test suite
improves, so by incorporating new tests as you fix known bugs, you’re earning
double points back, so to speak!
Finally, don’t forget to sprinkle assertions generously through unsafe
code. A panic is always better than triggering undefined behavior! Check
all of your assumptions with assertions if you can—even things like the size
166 Chapter 9
of a usize if you rely on that for safety. If you’re concerned about runtime
cost, make use of the debug_assert* macros and the if cfg!(debug_assertions)
|| cfg!(test) construct to execute them only in debug and test contexts.
A HOUSE OF CARDS?
Unsafe code can violate all of Rust’s safety guarantees, and this is often touted
as a reason why Rust’s whole safety argument is a charade. The concern is
that it takes only one bit of incorrect unsafe code for the whole house to come
crashing down and all safety to be lost. Proponents of this argument then
sometimes argue that at the very least only unsafe code should be able to call
unsafe code, so that the unsafety is visible all the way to the highest level of the
application.
The argument is understandable—it is true that the safety of Rust code
relies on the safety of all the transitive unsafe code it ends up invoking. And
indeed, if some of that unsafe code is incorrect, it may have implications for
the safety of the program overall. However, what this argument misses is that
all successful safe languages provide a facility for language extensions that are
not expressible in the (safe) surface language, usually in the form of code written
in C or assembly. Just as Rust relies on the correctness of its unsafe code,
the safety of those languages relies on the correctness of those extensions.
Rust is different in that it doesn’t have a separate extension language, but
instead allows extensions to be written in what amounts to a dialect of Rust
(unsafe Rust). This allows much closer integration between the safe and unsafe
code, which in turn reduces the likelihood of errors due to impedance mismatches
at the interface between the two, or due to developers being familiar
with one but not the other. The closer integration also makes it easier to write
tools that analyze the correctness of the unsafe code’s interaction with the safe
code, as exemplified by tools like Miri. And since unsafe Rust continues to be
subject to the borrow checker for any operation that isn’t explicitly unsafe, there
remain many safety checks in place that aren’t present when developers must
drop down to a language like C.
Summary
In this chapter, we’ve walked through the powers that come with the unsafe
keyword and the responsibilities we accept by leveraging those powers. We
also talked about the consequences of writing unsafe unsafe code, and how
you really should be thinking about unsafe as a way to swear to the compiler
that you’ve manually checked that the indicated code is still safe. In the
next chapter, we’ll jump into concurrency in Rust and see how you can get
all those cores on your shiny new computer to pull in the same direction!

### 10 CONCURRENCY (ANdD PARALLELISM)

With this chapter I hope to provide you
with all the information and tools you’ll
need to take effective advantage of concurrency
in your Rust programs, to implement
support for concurrent use in your libraries, and to
use Rust’s concurrency primitives correctly. I won’t
directly teach you how to implement a concurrent
data structure or write a high-performance concurrent
application. Instead, my goal is to give you sufficient
understanding of the underlying mechanisms
that you’re equipped to wield them yourself for whatever
you may need them for.
Concurrency comes in three flavors: single-thread concurrency (like
with async/await, as we discussed in Chapter 8), single-core multithreaded
concurrency, and multicore concurrency, which yields true parallelism.
168 Chapter 10
Each flavor allows the execution of concurrent tasks in your program to be
interleaved in different ways. There are even more subflavors if you take the
details of operating system scheduling and preemption into account, but we
won’t get too deep into that.
At the type level, Rust represents only one aspect of concurrency: multithreading.
Either a type is safe for use by more than one thread, or it is not.
Even if your program has multiple threads (and so is concurrent) but only
one core (and so is not parallel), Rust must assume that if there are multiple
threads, there may be parallelism. Most of the types and techniques we’ll be
talking about apply equally whether two threads actually execute in parallel
or not, so to keep the language simple, I’ll be using the word concurrency
in the informal sense of “things running more or less at the same time”
throughout this chapter. When the distinction is important, I’ll call that out.
What’s particularly neat about Rust’s approach to type-based safe
multithreading
is that it is not a feature of the compiler, but rather a library
feature that developers can extend to develop sophisticated concurrency
contracts. Since thread safety is expressed in the type system through Send
and Sync implementations and bounds, which propagate all the way out
to application code, the thread safety of the entire program is checked
through type checking alone.
The Rust Programming Language already covers most of the basics when it
comes to concurrency, including the Send and Sync traits, Arc and Mutex, and
channels. I therefore won’t reiterate much of that here, except where it’s
worth repeating something specifically in the context of some other topic.
Instead, we’ll look at what makes concurrency difficult and some common
concurrency patterns intended to deal with those difficulties. We’ll also
explore how concurrency and asynchrony interact (and how they don’t)
before diving into how to use atomic operations to implement lower-level
concurrent operations. Finally, I’ll close out the chapter with some advice
for how to retain your sanity when working with concurrent code.
The Trouble with Concurrency
Before we dive into good patterns for concurrent programming and the
details of Rust’s concurrency mechanisms, it’s worth taking some time to
understand why concurrency is challenging in the first place. That is, why
do we need special patterns and mechanisms for concurrent code?
Correctness
The primary difficulty in concurrency is coordinating access—in particular,
write access—to a resource that is shared among multiple threads. If lots of
threads want to share a resource solely for the purposes of reading it, then
that’s usually easy: you stick it in an Arc or place it in something you can
get a &'static to, and you’re all done. But once any thread wants to write,
all sorts of problems arise, usually in the form of data races. Briefly, a data
race occurs when one thread updates shared state while a second thread is
also accessing that state, either to read it or to update it. Without additional
Concurrency (and Parallelism) 169
safeguards in place, the second thread may read partially overwritten state,
clobber parts of what the first thread wrote, or fail to see the first thread’s
write at all! In general, all data races are considered undefined behavior.
Data races are a part of a broader class of problems that primarily,
though not exclusively, occur in a concurrent setting: race conditions. A race
condition occurs whenever multiple outcomes are possible from a sequence
of instructions, depending on the relative timing of other events in the
system. These events can be threads executing a particular piece of code,
a timer going off, a network packet coming in, or any other time-variable
occurrence. Race conditions, unlike data races, are not inherently bad,
and are not considered undefined behavior. However, they are a breeding
ground for bugs when particularly peculiar races occur, as you’ll see
throughout this chapter.
Performance
Often, developers introduce concurrency into their programs in the
hope of increasing performance. Or, to be more precise, they hope that
concurrency will enable them to perform more operations per second in
aggregate by taking advantage of more hardware resources. This can be
done on a single core by having one thread run while another is waiting,
or across multiple cores by having threads do work simultaneously, one on
each core, that would otherwise happen serially on one core. Most developers
are referring to the latter kind of performance gain when they talk
about concurrency, which is often framed in terms of scalability. Scalability
in this context means “the performance of this program scales with the
number of cores,” implying that if you give your program more cores, its
performance improves.
While achieving such a speedup is possible, it’s harder than it seems.
The ultimate goal in scalability is linear scalability, where doubling the number
of cores doubles the amount of work your program completes per unit
of time. Linear scalability is also often called perfect scalability. However, in
reality, few concurrent programs achieve such speedups. Sublinear scaling
is more common, where the throughput increases nearly linearly as you go
from one core to two, but adding more cores yields diminishing returns.
Some programs even experience negative scaling, where giving the program
access to more cores reduces throughput, usually because the many threads
are all contending for some shared resource.
It might help to think of a group of people trying to pop all the bubbles
on a piece of bubble wrap—adding more people helps initially, but at some
point you get diminishing returns as the crowding makes any one person’s
job harder. If the humans involved are particularly ineffective, your group
may end up standing around discussing who should pop next and pop no
bubbles at all! This kind of interference among tasks that are supposed to
execute in parallel is called contention and is the archnemesis of scaling well.
Contention can arise in a number of ways, but the primary offenders are
mutual exclusion, shared resource exhaustion, and false sharing.

170 Chapter 10
Mutual Exclusion
When only a single concurrent task is allowed to execute a particular piece
of code at any one time, we say that execution of that segment of code is
mutually exclusive—if one thread executes it, no other thread can do so
at the same time. The archetypal example of this is a mutual exclusion
lock, or mutex, which explicitly enforces that only one thread gets to enter
a particular critical section of your program code at any one time. Mutual
exclusion can also happen implicitly, however. For example, if you spin up
a thread to manage a shared resource and send jobs to it over an mpsc channel,
that thread effectively implements mutual exclusion, since only one
such job gets to execute at a time.
Mutual exclusion can also occur when invoking operating system or
library calls that internally enforce single-threaded access to a critical section.
For example, for many years, the standard memory allocator required
mutual exclusion for some allocations, which made memory allocation an
operation that incurred significant contention in otherwise highly parallel
programs. Similarly, many operating system operations that may seem like
they should be independent, such as creating two files with different names
in the same directory, may end up having to happen sequentially inside the
kernel.
NOTE Scalable concurrent allocations is the raison d’être for the jemalloc memory allocator!
Mutual exclusion is the most obvious barrier to parallel speedup since,
by definition, it forces serial execution of some portion of your program.
Even if you make the remainder of your program scale with the number of
cores perfectly, the total speedup you can achieve is limited by the length of
the mutually exclusive, serial section. Be mindful of your mutually exclusive
sections, and seek to restrict them to only where strictly necessary.
NOTE For the theoretically minded, the limits on the achievable speedup as a result of mutually
exclusive sections of code can be computed using Amdahl’s law.
Shared Resource Exhaustion
Unfortunately, even if you achieve perfect concurrency within your tasks,
the environment those tasks need to interact with may itself not be perfectly
scalable. The kernel can handle only so many sends on a given TCP socket
per second, the memory bus can do only so many reads at once, and your
GPU has a limited capacity for concurrency. There’s no cure for this. The
environment is usually where perfect scalability falls apart in practice, and
fixes for such cases tend to require substantial re-engineering (or even new
hardware!), so we won’t talk much more about this topic in this chapter. Just
remember that scalability is rarely something you can “achieve,” and more
something you just strive for.





Concurrency (and Parallelism) 171
False Sharing
False sharing occurs when two operations that shouldn’t contend with one
another contend anyway, preventing efficient simultaneous execution. This
usually happens because the two operations happen to intersect on some
shared resource even though they use unrelated parts of that resource.
The simplest example of this is lock oversharing, where a lock guards
some composite state, and two operations that are otherwise independent
both need to take the lock to update their particular parts of the state.
This in turn means the operations must execute serially instead of in parallel.
In some cases it’s possible to split the single lock into two, one for each
of the disjoint parts, which enables the operations to proceed in parallel.
However, it’s not always straightforward to split a lock like this—the state
may share a single lock because some third operation needs to lock over
all the parts of the state. Usually you can still split the lock, but you have to
be careful about the order in which different threads take the split locks to
avoid deadlocks that can occur when two operations attempt to take them
in different orders (look up the “dining philosophers problem,” if you’re
curious). Alternatively, for some problems, you may be able to avoid the
critical section entirely by using a lock-free version of the underlying algorithm,
though those are also tricky to get right. Ultimately, false sharing is a
hard problem to solve, and there isn’t a single catchall solution—but identifying
the problem is a good start.
A more subtle example of false sharing occurs on the CPU level, as we
discussed briefly in Chapter 2. The CPU internally operates on memory in
terms of cache lines—longer sequences of consecutive bytes in memory—
rather than individual bytes, to amortize the cost of memory accesses. For
example, on most Intel processors, the cache line size is 64 bytes. This
means that every memory operation really ends up reading or writing some
multiple of 64 bytes. The false sharing comes into play when two cores
want to update the value of two different bytes that happen to fall on the
same cache line; those updates must execute sequentially even though the
updates are logically disjoint.
This might seem too low-level to matter, but in practice this kind of
false sharing can decimate the parallel speedup of an application. Imagine
that you allocate an array of integer values to indicate how many operations
each thread has completed, but the integers all fall within the same cache
line—now, all your otherwise parallel threads will contend on that one
cache line for every operation they perform. If the operations are relatively
quick, most of your execution time may end up being spent contending on
those counters!
The trick to avoiding false cache line sharing is to pad your values so
that they are the size of a cache line. That way, two adjacent values always
fall on different cache lines. But of course, this also inflates the size of your
data structures, so use this approach only when benchmarks indicate a
problem.
172 Chapter 10
THE COST OF SCALABILITY
A somewhat orthogonal aspect of concurrency that you should be mindful of is
the cost of introducing concurrency in the first place. Compilers are really good
at optimizing single-threaded code—they’ve been doing it for a long time, after
all—and single-threaded code tends to get away with fewer expensive safeguards
(like locks, channels, or atomic instructions) than concurrent code can.
In aggregate, the various costs of concurrency can make a parallel program
slower than its single-threaded counterpart, given any number of cores! This is
why it’s important to measure both before and after you optimize and parallelize:
the results may surprise you.
If you’re curious about this topic, I highly recommend you read Frank
McSherry’s 2015 paper “Scalability! But at what COST?” (https://www
.frankmcsherry.org/assets/COST.pdf), which uncovers some particularly
egregious examples of “costly scaling.”
Concurrency Models
Rust has three patterns for adding concurrency to your programs that
you’ll come across fairly often: shared memory concurrency, worker pools,
and actors. Going through every way you could add concurrency in detail
would take a book of its own, so here I’ll focus on just these three patterns.
Shared Memory
Shared memory concurrency is, conceptually, very straightforward: the
threads cooperate by operating on regions of memory shared between
them. This might take the form of state guarded by a mutex or stored in
a hash map with support for concurrent access from many threads. The
many threads may be doing the same task on disjoint pieces of data, such
as if many threads perform some function over disjoint subranges of a Vec,
or they may be performing different tasks that require some shared state,
such as in a database where one thread handles user queries to a table
while another optimizes the data structures used to store that table in the
background.
When you use shared memory concurrency, your choice of data structures
is significant, especially if the threads involved need to cooperate
very closely. A regular mutex might prevent scaling beyond a very small
number of cores, a reader/writer lock might allow many more concurrent
reads at the cost of slower writes, and a sharded reader/writer lock might
allow perfectly scalable reads at the cost of making writes highly disruptive.
Similarly, some concurrent hash maps aim for good all-round performance
while others specifically target, say, concurrent reads where writes are rare.
In general, in shared memory concurrency, you want to use data structures
Concurrency (and Parallelism) 173
that are specifically designed for something as close to your target use case
as possible, so that you can take advantage of optimizations that trade off
performance aspects your application does not care about for those it does.
Shared memory concurrency is a good fit for use cases where threads
need to jointly update some shared state in a way that does not commute.
That is, if one thread has to update the state s with some function f, and
another has to update the state with some function g, and f(g(s)) != g(f(s)),
then shared memory concurrency is likely necessary. If that is not the case,
the other two patterns are likely better fits, as they tend to lead to simpler
and more performant designs.
NOTE Some problems have known algorithms that can provide concurrent shared memory
operations without the use of locks. As the number of cores grows, these lock-free
algorithms may scale better than lock-based algorithms, though they also often have
slower per-core performance due to their complexity. As always with performance matters,
benchmark first, then look for alternative solutions.
Worker Pools
In the worker pool model, many identical threads receive jobs from a
shared job queue, which they then execute entirely independently. Web
servers, for example, often have a worker pool handling incoming connections,
and multithreaded runtimes for asynchronous code tend to use a
worker pool to collectively execute all of an application’s futures (or, more
accurately, its top-level tasks).
The lines between shared memory concurrency and worker pools are
often blurry, as worker pools tend to use shared memory concurrency to
coordinate how they take jobs from the queue and how they return incomplete
jobs back to the queue. For example, say you’re using the data parallelism
library rayon to perform some function over every element of a vector
in parallel. Behind the scenes rayon spins up a worker pool, splits the vector
into subranges, and then hands out subranges to the threads in the pool.
When a thread in the pool finishes a range, rayon arranges for it to start
working on the next unprocessed subrange. The vector is shared among all
the worker threads, and the threads coordinate through a shared memory
queue–like data structure that supports work stealing.
Work stealing is a key feature of most worker pools. The basic premise
is that if one thread finishes its work early, and there’s no more unassigned
work available, that thread can steal jobs that have already been assigned to
a different worker thread but haven’t been started yet. Not all jobs take the
same amount of time to complete, so even if every worker is given the same
number of jobs, some workers may end up finishing their jobs more quickly
than others. Rather than sit around and wait for the threads that drew
longer-running jobs to complete, those threads that finish early should help
the stragglers so the overall operation is completed sooner.
It’s quite a task to implement a data structure that supports this kind
of work stealing without incurring significant overhead from threads constantly
trying to steal work from one another, but this feature is vital to a
174 Chapter 10
high-performance worker pool. If you find yourself in need of a worker
pool, your best bet is usually to use one that has already seen a lot of work
go into it, or at least reuse data structures from an existing one, rather than
to write one yourself from scratch.
Worker pools are a good fit when the work that each thread performs is
the same, but the data it performs it on varies. In a rayon parallel map operation,
every thread performs the same map computation; they just perform
it on different subsets of the underlying data. In a multithreaded asynchronous
runtime, each thread simply calls Future::poll; they just call it on different
futures. If you start having to distinguish between the threads in your
thread pool, a different design is probably more appropriate.
CONNECTION POOLS
A connection pool is a shared memory construct that keeps a set of established
connections and hands them out to threads that need a connection. It’s a common
design pattern in libraries that manage connections to external services.
If a thread needs a connection but one isn’t available, either a new connection
is established or the thread is forced to block. When a thread is done with a
connection, it returns that connection to the pool, and thus makes it available to
other threads that may be waiting.
Usually, the hardest task for a connection pool is managing connection
life cycles. A connection can be returned to the pool in whatever state it was
put in by the last thread that used it. The connection pool therefore has to make
sure any state associated with the connection, whether on the client or on the
server, has been reset so that when the connection is subsequently used by
another thread, that thread can act as though it was given a fresh, dedicated
connection.
Actors
The actor concurrency model is, in many ways, the opposite of the worker
pool model. Whereas the worker pool has many identical threads that
share a job queue, the actor model has many separate job queues, one
for each job “topic.” Each job queue feeds into a particular actor, which
handles all jobs that pertain to a subset of the application’s state. That state
might be a database connection, a file, a metrics collection data structure,
or any other structure that you can imagine many threads may need to be
able to access. Whatever it is, a single actor owns that state, and if some
task wants to interact with that state, it needs to send a message to the
owning actor summarizing the operation it wishes to perform. When the
owning actor receives that message, it performs the indicated action and
responds to the inquiring task with the result of the operation, if relevant.
Concurrency (and Parallelism) 175
Since the actor has exclusive access to its inner resource, no locks or other
synchronization mechanisms are required beyond what’s needed for the
messaging.
A key point in the actor pattern is that actors all talk to one another.
If, say, an actor that is responsible for logging needs to write to a file and a
database table, it might send off messages to the actors responsible for each
of those, asking them to perform the respective actions, and then proceed
to the next log event. In this way, the actor model more closely resembles a
web than spokes on a wheel—a user request to a web server might start as a
single request to the actor responsible for that connection but might transitively
spawn tens, hundreds, or even thousands of messages to actors deeper
in the system before the user’s request is satisfied.
Nothing in the actor model requires that each actor is its own thread. To
the contrary, most actor systems suggest that there should be a large number
of actors, and so each actor should map to a task rather than a thread. After
all, actors require exclusive access to their wrapped resources only when
they execute, and do not care whether they are on a thread of their own
or not. In fact, very frequently, the actor model is used in conjunction with
the worker pool model—for example, an application that uses the multithreaded
asynchronous runtime Tokio can spawn an asynchronous task for
each actor, and Tokio will then make the execution of each actor a job in its
worker pool. Thus, the execution of a given actor may move from thread to
thread in the worker pool as the actor yields and resumes, but every time the
actor executes it maintains exclusive access to its wrapped resource.
The actor concurrency model is well suited for when you have many
resources that can operate relatively independently, and where there is
little or no opportunity for concurrency within each resource. For example,
an operating system might have an actor responsible for each hardware
device, and a web server might have an actor for each backend database
connection. The actor model does not work so well if you need only a few
actors, if work is skewed significantly among the actors, or if some actors
grow large—in all of those cases, your application may end up being bottlenecked
on the execution speed of a single actor in the system. And since
actors each expect to have exclusive access to their little slice of the world,
you can’t easily parallelize the execution of that one bottleneck actor.
Asynchrony and Parallelism
As we discussed in Chapter 8, asynchrony in Rust enables concurrency
without parallelism—we can use constructs like selects and joins to have
a single thread poll multiple futures and continue when one, some, or all
of them complete. Because there is no parallelism involved, concurrency
with futures does not fundamentally require those futures to be Send. Even
spawning a future to run as an additional top-level task does not fundamentally
require Send, since a single executor thread can manage the polling of
many futures at once.
176 Chapter 10
However, in most cases, applications want both concurrency and parallelism.
For example, if a web application constructs a future for each incoming
connection and so has many active connections at once, it probably wants the
asynchronous executor to be able to take advantage of more than one core
on the host computer. That won’t happen naturally: your code has to explicitly
tell the executor which futures can run in parallel and which cannot.
In particular, two pieces of information must be given to the executor
to let it know that it can spread the work in the futures across a worker pool
of threads. The first is that the futures in question are Send—if they aren’t,
the executor is not allowed to send the futures to other threads for processing,
and no parallelism is possible; only the thread that constructed such
futures can poll them.
The second piece of information is how to split the futures into tasks
that can operate independently. This ties back to the discussion of tasks versus
futures from Chapter 8: if one giant Future contains a number of Future
instances that themselves correspond to tasks that can run in parallel, the
executor must still call poll on the top-level Future, and it must do so from
a single thread, since poll requires &mut self. Thus, to achieve parallelism
with futures, you have to explicitly spawn the futures you want to be able to
run in parallel. Also, because of the first requirement, the executor function
you use to do so will require that the passed-in Future is Send.
ASYNCHRONOUS SYNCHRONIZATION PRIMITIVES
Most of the synchronization primitives that exist for blocking code (think
std::sync) also have asynchronous counterparts. There are asynchronous variants
of channels, mutexes, reader/writer locks, barriers, and all sorts of other
similar constructs. We need these because, as discussed in Chapter 8, blocking
inside a future will hold up other work the executor may need to do, and so is
inadvisable.
However, the asynchronous versions of these primitives are often slower
than their synchronous counterparts because of the additional machinery
needed to perform the necessary wake-ups. For that reason, you may want to
use synchronous synchronization primitives even in asynchronous contexts whenever
the use does not risk blocking the executor. For example, while it’s generally
true that acquiring a Mutex might block for a long time, that might not be true for
a particular Mutex that, perhaps, is acquired only rarely, and only ever for short
periods of time. In that case, blocking for the short time until the Mutex becomes
available again might not actually cause any problems. You will want to make
sure that you never yield or perform other long-running operations while holding
the MutexGuard, but barring that you shouldn’t run into problems.
As always with such optimizations, though, make sure you measure first,
and choose only the synchronous primitive if it nets you significant performance
improvements. If it does not, the additional footguns introduced by using a synchronous
primitive in an asynchronous context are probably not worth it.

Concurrency (and Parallelism) 177
Lower-Level Concurrency
The standard library provides the std::sync::atomic module, which provides
access to the underlying CPU primitives, higher-level constructs like
channels and mutexes are built with. These primitives come in the form
of atomic types with names starting with Atomic—AtomicUsize, AtomicI32,
AtomicBool, AtomicPtr, and so on—the Ordering type, and two functions
called fence and compiler_fence. We’ll look at each of these over the next few
sections.
These types are the blocks used to build any code that has to communicate
between threads. Mutexes, channels, barriers, concurrent hash tables,
lock-free stacks, and all other synchronization constructs ultimately rely on
these few primitives to do their jobs. They also come in handy on their own
for lightweight cooperation between threads where heavyweight synchronization
like a mutex is excessive—for example, to increment a shared counter
or set a shared Boolean to true.
The atomic types are special in that they have defined semantics for
what happens when multiple threads try to access them concurrently. These
types all support (mostly) the same API: load, store, fetch_*, and compare_
exchange. In the rest of this section, we’ll look at what those do, how to use
them correctly, and what they’re useful for. But first, we have to talk about
low-level memory operations and memory ordering.
Memory Operations
Informally, we often refer to accessing variables as “reading from” or “writing
to” memory. In reality, a lot of machinery between code uses a variable
and the actual CPU instructions that access your memory hardware. It’s
important to understand that machinery, at least at a high level, in order to
understand how concurrent memory accesses behave.
The compiler decides what instructions to emit when your program
reads the value of a variable or assigns a new value to it. It is permitted to
perform all sorts of transformations and optimizations on your code and
may end up reordering your program statements, eliminating operations
it deems redundant, or using CPU registers rather than actual memory to
store intermediate computations. The compiler is subject to a number of
restrictions on these transformations, but ultimately only a subset of your
variable accesses actually end up as memory access instructions.
At the CPU level, memory instructions come in two main shapes: loads
and stores. A load pulls bytes from a location in memory into a CPU register,
and a store stores bytes from a CPU register into a location in memory.
Loads and stores operate on small chunks of memory at a time: usually
8 bytes or less on modern CPUs. If a variable access spans more bytes than
can be accessed with a single load or store, the compiler automatically turns
it into multiple load or store instructions, as appropriate. The CPU also has
some leeway in how it executes a program’s instructions to make better use
of the hardware and improve program performance. For example, modern
CPUs often execute instructions in parallel, or even out of order, when they
don’t have dependencies on each other. There are also several layers of caches
178 Chapter 10
between each CPU and your computer’s DRAM, which means that a load
of a given memory location may not necessarily see the latest store to that
memory location, going by wall-clock time.
In most code, the compiler and CPU are permitted to transform the
code only in ways that don’t affect the semantics of the resulting program,
so these transformations are invisible to the programmer. However, in the
context of parallel execution, these transformations can have a significant
impact on application behavior. Therefore, CPUs typically provide multiple
different variations of the load and store instructions, each with different
guarantees about how the CPU may reorder them and how they may be
interleaved with parallel operations on other CPUs. Similarly, compilers (or
rather, the language the compiler compiles) provide different annotations
you can use to force particular execution constraints for some subset of
their memory accesses. In Rust, those annotations come in the form of the
atomic types and their methods, which we’ll spend the rest of this section
picking apart.
Atomic Types
Rust’s atomic types are so called because they can be accessed atomically—
that is, the value of an atomic-type variable is written all at once and will
never be written using multiple stores, guaranteeing that a load of that variable
cannot observe that only some of the bytes composing the value have
changed while others have not (yet). This is easiest understood by way of
contrast with non-atomic types. For example, reassigning a new value to a
tuple of type (i64, i64) typically requires two CPU store instructions, one
for each 8-byte value. If one thread were to perform both of those stores,
another thread could (if we ignore the borrow checker for a second) read
the tuple’s value after the first store but before the second, and thus end up
with an inconsistent view of the tuple’s value. It would end up reading the
new value for the first element and the old value for the second element, a
value that was never actually stored by any thread.
The CPU can atomically access values only of certain sizes, so there are
only a few atomic types, all of which live in the atomic module. Each atomic
type is of one of the sizes the CPU supports atomic access to, with multiple
variations for things like whether the value is signed and to differentiate
between an atomic usize and a pointer (which is of the same size as usize).
Furthermore, the atomic types have explicit methods for loading and storing
the values they hold, and a handful of more complex methods we’ll get
back to later, so that the mapping between the code the programmer writes
and the resulting CPU instructions is clearer. For example, AtomicI32::load
performs a single load of a signed 32-bit value, and AtomicPtr::store performs
a single store of a pointer-sized (64 bits on a 64-bit platform) value.
Memory Ordering
Most of the methods on the atomic types take an argument of type Ordering,
which dictates the memory ordering restrictions the atomic operation is
subject to. Across different threads, loads and stores of an atomic value
Concurrency (and Parallelism) 179
may be sequenced by the compiler and CPU only in interleavings that are
compatible with the requested memory ordering of each of the atomic
operations on that atomic value. Over the next few sections, we’ll see some
examples of why control over the ordering is important and necessary to get
the expected semantics out of the compiler and CPU.
Memory ordering often comes across as counterintuitive, because we
humans like to read programs from top to bottom and imagine that they
execute line by line—but that’s not how the code actually executes when
it hits the hardware. Memory accesses can be reordered, or even entirely
elided, and writes on one thread may not immediately be visible to other
threads, even if later writes in program order have already been observed.
Think of it like this: each memory location sees a sequence of modifications
coming from different threads, and the sequences of modifications for
different memory locations are independent. If two threads T1 and T2 both
write to memory location M, then even if T1 executed first as measured by
a user with a stopwatch, T2’s write to M may still appear to have happened
first for M absent any other constraints between the two threads’ execution.
Essentially, the computer does not take wall-clock time into account when it determines
the value of a given memory location—all that matter are the execution
constraints the programmer puts on what constitutes a valid execution.
For example, if T1 writes to M and then spawns thread T2, which then
writes to M, the computer must recognize T1’s write as having happened
first because T2’s existence depends on T1.
If that’s hard to follow, don’t fret—memory ordering can be mindbending,
and language specifications tend to use very precise but not very
intuitive wording to describe it. We can construct a mental model that’s
easier to grasp, if a little simplified, by instead focusing on the underlying
hardware architecture. Very basically, your computer memory is structured
as a treelike hierarchy of storage where the leaves are CPU registers and
the roots are the storage on your physical memory chips, often called main
memory. Between the two are several layers of caches, and different layers
of the hierarchy can reside on different pieces of hardware. When a
thread performs a store to a memory location, what really happens is that
the CPU starts a write request for the value in a given CPU register that
then has to make its way up the memory hierarchy toward main memory.
When a thread performs a load, the request flows up the hierarchy until it
hits a layer that has the value available, and returns from there. Herein lies
the problem: writes aren’t visible everywhere until all caches of the written
memory location have been updated, but other CPUs can execute instructions
against the same memory location at the same time, and weirdness
ensues. Memory ordering, then, is a way to request precise semantics for
what happens when multiple CPUs access a particular memory location for
a particular operation.
With this in mind, let’s take a look at the Ordering type, which is the
primary mechanism by which we, as programmers, can dictate additional
constraints on what concurrent executions are valid.
Ordering is defined as an enum with the variants shown in Listing 10-1.
180 Chapter 10
enum Ordering {
Relaxed,
Release,
Acquire,
AcqRel,
SeqCst
}
Listing 10-1: The definition of Ordering
Each of these places different restrictions on the mapping from source
code to execution semantics, and we’ll explore each one in turn in the
remainder of this section.
Relaxed Ordering
Relaxed ordering essentially guarantees nothing about concurrent
access to the value beyond the fact that the access is atomic. In particular,
relaxed ordering gives no guarantees about the relative ordering of memory
accesses across different threads. This is the weakest form of memory
ordering. Listing 10-2 shows a simple program in which two threads access
two atomic variables using Ordering::Relaxed.
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
1 let r1 = Y.load(Ordering::Relaxed);
2 X.store(r1, Ordering::Relaxed);
});
let t2 = spawn(|| {
3 let r2 = X.load(Ordering::Relaxed);
4 Y.store(true, Ordering::Relaxed)
});
Listing 10-2: Two racing threads with Ordering::Relaxed
Looking at the thread spawned as t2, you might expect that r2 can
never be true, since all values are false until the same thread assigns true to
Y on the line after reading X. However, with a relaxed memory ordering, that
outcome is completely possible. The reason is that the CPU is allowed to
reorder the loads and stores involved. Let’s walk through exactly what happens
here to make r2 = true possible.
First, the CPU notices that 4 doesn’t have to happen after 3, since 4
doesn’t use any output or side effect of 3. That is, 4 has no execution dependency
on 3. So, the CPU decides to reorder them for *waves hands* reasons
that’ll make your program go faster. The CPU thus goes ahead and executes 4
first, setting Y = true, even though 3 hasn’t run yet. Then, t2 is put to sleep
by the operating system and thread t1 executes a few instructions, or t1 simply
executes on another core. In t1, the compiler must indeed run 1 first and
then 2, since 2 depends on the value read in 1. Therefore, t1 reads true from
Concurrency (and Parallelism) 181
Y (written by 4) into r1 and then writes that back to X. Finally, t2 executes 3,
which reads X and gets true, as was written by 2.
The relaxed memory ordering allows this execution because it imposes
no additional constraints on concurrent execution. That is, under relaxed
memory ordering, the compiler must ensure only that execution dependencies
on any given thread are respected (just as if atomics weren’t involved);
it need not make any promises about the interleaving of concurrent operations.
Reordering 3 and 4 is permitted for a single-threaded execution, so
it is permitted under relaxed ordering as well.
In some cases, this kind of reordering is fine. For example, if you have a
counter that just keeps track of metrics, it doesn’t really matter when exactly
it executes relative to other instructions, and Ordering::Relaxed is fine. In
other cases, this could be disastrous: say, if your program uses r2 to figure
out if security protections have already been set up, and thus ends up erroneously
believing that they already have been.
You don’t generally notice this reordering when writing code that
doesn’t make fancy use of atomics—the CPU has to promise that there is
no observable difference between the code as written and what each thread
actually executes, so everything seems like it runs in order just as you wrote
it. This is referred to as respecting program order or evaluation order; the
terms are synonyms.
Acquire/Release Ordering
At the next step up in the memory ordering hierarchy, we have
Ordering::Acquire, Ordering::Release, and Ordering::AcqRel (acquire plus
release). At a high level, these establish an execution dependency between
a store in one thread and a load in another and then restrict how operations
can be reordered with respect to that load and store. Crucially, these
dependencies not only establish a relationship between a store and a load of
a single value, but also put ordering constraints on other loads and stores in
the threads involved. This is because every execution must respect the program
order; if a load in thread B has a dependency on some store in thread
A (the store in A must execute before the load in B), then any read or write
in B after that load must also happen after that store in A.
NOTE The Acquire memory ordering can be applied only to loads, Release only to stores,
and AcqRel only to operations that both load and store (like fetch_add).
Concretely, these memory orderings place the following restrictions on
execution:
1. Loads and stores cannot be moved forward past a store with
Ordering::Release.
2. Loads and stores cannot be moved back before a load with
Ordering::Acquire.
3. An Ordering::Acquire load of a variable must see all stores that happened
before an Ordering::Release store that stored what the load loaded.
182 Chapter 10
To see how these memory orderings change things, Listing 10-3 shows
Listing 10-2 again but with the memory ordering swapped out for Acquire
and Release.
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
let r1 = Y.load(Ordering::Acquire);
X.store(r1, Ordering::Release);
});
let t2 = spawn(|| {
1 let r2 = X.load(Ordering::Acquire);
2 Y.store(true, Ordering::Release)
});
Listing 10-3: Listing 10-2 with Acquire/Release memory ordering
These additional restrictions mean that it is no longer possible for t2 to
see r2 = true. To see why, consider the primary cause of the weird outcome
in Listing 10-2: the reordering of 1 and 2. The very first restriction, on
stores with Ordering::Release, dictates that we cannot move 1 below 2, so
we’re all good!
But these rules are useful beyond this simple example. For example,
imagine that you implement a mutual exclusion lock. You want to make
sure that any loads and stores a thread runs while it holds the lock are executed
only while it’s actually holding the lock, and visible to any thread that
takes the lock later. This is exactly what Release and Acquire enable you to
do. By performing a Release store to release the lock and an Acquire load to
acquire the lock, you can guarantee that the loads and stores in the critical
section are never moved to before the lock was actually acquired or to after
the lock was released!
NOTE On some CPU architectures, like x86, Acquire/Release ordering is guaranteed
by the hardware, and there is no additional cost to using Ordering::Release and
Ordering::Acquire over Ordering::Relaxed. On other architectures that is not the
case, and your program may see speedups if you switch to Relaxed for atomic operations
that can tolerate the weaker memory ordering guarantees.
Sequentially Consistent Ordering
Sequentially consistent ordering (Ordering::SeqCst) is the strongest memory
ordering we have access to. Its exact guarantees are somewhat hard to nail
down, but very broadly, it requires not only that each thread sees results
consistent with Acquire/Release, but also that all threads see the same ordering
as one another. This is best seen by way of contrast with the behavior of
Acquire and Release. Specifically, Acquire/Release ordering does not guarantee
that if two threads A and B atomically load values written by two other
threads X and Y, A and B will see a consistent pattern of when X wrote
relative to Y. That’s fairly abstract, so consider the example in Listing 10-4,
Concurrency (and Parallelism) 183
which shows a case where Acquire/Release ordering can produce unexpected
results. Afterwards, we’ll see how sequentially consistent ordering avoids
that particular unexpected outcome.
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
static Z: AtomicI32 = AtomicI32::new(0);
let t1 = spawn(|| {
X.store(true, Ordering::Release);
});
let t2 = spawn(|| {
Y.store(true, Ordering::Release);
});
let t3 = spawn(|| {
while (!X.load(Ordering::Acquire)) {}
1 if (Y.load(Ordering::Acquire)) {
Z.fetch_add(1, Ordering::Relaxed); }
});
let t4 = spawn(|| {
while (!Y.load(Ordering::Acquire)) {}
2 if (X.load(Ordering::Acquire)) {
Z.fetch_add(1, Ordering::Relaxed); }
});
Listing 10-4: Weird results with Acquire/Release ordering
The two threads t1 and t2 set X and Y to true, respectively. Thread t3
waits for X to be true; once X is true, it checks if Y is true and, if so, adds 1 to
Z. Thread t4 instead waits for Y to become true, and then checks if X is true
and, if so, adds 1 to Z. At this point the question is: what are the possible
values for Z after all the threads terminate? Before I show you the answer,
try to work your way through it given the definitions of Release and Acquire
ordering in the previous section.
First, let’s recap the conditions under which Z is incremented. Thread t3
increments Z if it sees that Y is true after it observes that X is true, which can
happen only if t2 runs before t3 evaluates the load at 1. Conversely, thread
t4 increments Z if it sees that X is true after it observes that Y is true, so only if
t1 runs before t4 evaluates the load at 2. To simplify the explanation, let’s
assume for now that each thread runs to completion once it runs.
Logically, then, Z can be incremented twice if the threads run in the
order 1, 2, 3, 4—both X and Y are set to true, and then t3 and t4 run to find
that their conditions for incrementing Z are met. Similarly, Z can trivially
be incremented just once if the threads run in the order 1, 3, 2, 4. This satisfies
t4’s condition for incrementing Z, but not t3’s. Getting Z to be 0, however,
seems impossible: if we want to prevent t3 from incrementing Z, t2 has
to run after t3. Since t3 runs only after t1, that implies that t2 runs after t1.
However, t4 won’t run until after t2 has run, so t1 must have run and set X
to true by the time t4 runs, and so t4 will increment Z.
Our inability to get Z to be 0 stems mostly from our human inclination
for linear explanations; this happened, then this happened, then this
184 Chapter 10
happened. Computers aren’t limited in the same way and have no need to
box all events into a single global order. There’s nothing in the rules for
Release and Acquire that says that t3 must observe the same execution order
for t1 and t2 as t4 observes. As far as the computer is concerned, it’s fine
to let t3 observe t1 as having executed first, while having t4 observe t2 as
having executed first. With that in mind, an execution in which t3 observes
that Y is false after it observes that X is true (implying that t2 runs after t1),
while in the same execution t4 observes that X is false after it observes that
Y is true (implying that t2 runs before t1), is completely reasonable, even if
that seems outrageous to us mere humans.
As we discussed earlier, Acquire/Release requires only that an
Ordering::Acquire load of a variable must see all stores that happened before
an Ordering::Release store that stored what the load loaded. In the ordering
just discussed, the computer did uphold that property: t3 sees X == true,
and indeed sees all stores by t1 prior to it setting X = true—there are none.
It also sees Y == false, which was stored by the main thread at program
startup, so there aren’t any relevant stores to be concerned with. Similarly,
t4 sees Y = true and also sees all stores by t2 prior to setting Y = true—again,
there are none. It also sees X == false, which was stored by the main thread
and has no preceding store. No rules are broken, yet it just seems wrong
somehow.
Our intuitive expectation was that we could put the threads in some
global order to make sense of what every thread saw and did, but that was
not the case for Acquire/Release ordering in this example. To achieve something
closer to that intuitive expectation, we need sequential consistency.
Sequential consistency requires all the threads taking part in an atomic
operation to coordinate to ensure that what each thread observes corresponds
to (or at least appears to correspond to) some single, common execution
order. This makes it easier to reason about but also makes it costly.
Atomic loads and stores marked with Ordering::SeqCst instruct the compiler
to take any extra precautions (such as using special CPU instructions)
needed to guarantee sequential consistency for those loads and stores. The
exact formalism around this is fairly convoluted, but sequential consistency
essentially ensures that if you looked at all the related SeqCst operations
from across all your threads, you could put the thread executions in some
order so that the values that were loaded and stored would all match up.
If we replaced all the memory ordering arguments in Listing 10-4 with
SeqCst, Z could not possibly be 0 after all the threads have exited, just as we
originally expected. Under sequential consistency, it must be possible to say
either that t1 definitely ran before t2 or that t2 definitely ran before t1, so
the execution where t3 and t4 see different orders is not allowed, and thus Z
cannot be 0.
Compare and Exchange
In addition to load and store, all of Rust’s atomic types provide a method
called compare_exchange. This method is used to atomically and conditionally
replace a value. You provide compare_exchange with the last value you
Concurrency (and Parallelism) 185
observed for an atomic variable and the new value you want to replace the
original value with, and it will replace the value only if it is still the same as
it was when you last observed it. To see why this is important, take a look
at the (broken) implementation of a mutual exclusion lock in Listing 10-5.
This implementation keeps track of whether the lock is held in the static
atomic variable LOCK. We use the Boolean value true to represent that the
lock is held. To acquire the lock, a thread waits for LOCK to be false, then
sets it to true again; it then enters its critical section and sets LOCK to false
to release the lock when its work (f) is done.
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
// Wait for the lock to become free (false).
while LOCK.load(Ordering::Acquire)
{ /* .. TODO: avoid spinning .. */ }
// Store the fact that we hold the lock.
LOCK.store(true, Ordering::Release);
// Call f while holding the lock.
f();
// Release the lock.
LOCK.store(false, Ordering::Release);
}
Listing 10-5: An incorrect implementation of a mutual exclusion lock
This mostly works, but it has a terrible flaw—two threads might both
see LOCK == false at the same time and both leave the while loop. Then they
both set LOCK to true and both enter the critical section, which is exactly
what the mutex function was supposed to prevent!
The issue in Listing 10-5 is that there is a gap between when we load
the current value of the atomic variable and when we subsequently update
it, during which another thread might get to run and read or touch its
value. It is exactly this problem that compare_exchange solves—it swaps out the
value behind the atomic variable only if its value still matches the previous
read, and otherwise notifies you that the value has changed. Listing 10-6
shows the corrected implementation using compare_exchange.
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
// Wait for the lock to become free (false).
loop {
let take = LOCK.compare_exchange(
false,
true,
Ordering::AcqRel,
Ordering::Relaxed
);
match take {
Ok(false) => break,
Ok(true) | Err(false) => unreachable!(),
186 Chapter 10
Err(true) => { /* .. TODO: avoid spinning .. */ }
}
}
// Call f while holding the lock.
f();
// Release the lock.
LOCK.store(false, Ordering::Release);
}
Listing 10-6: A corrected implementation of a mutual exclusion lock
This time around, we use compare_exchange in the loop, and it takes care
of both checking that the lock is currently not held and storing true to take
the lock as appropriate. This happens through the first and second arguments
to compare_exchange, respectively: in this case, false and then true. You
can read the invocation as “Store true only if the current value is false.”
The compare_exchange method returns a Result that indicates either that the
value was successfully updated (Ok) or that it could not be updated (Err).
In either case, it also returns the current value. This isn’t too useful with
an AtomicBool since we know what the value must be if the operation failed,
but for something like an AtomicI32, the updated current value will let you
quickly recompute what to store and then try again without having to do
another load.
NOTE Note that compare_exchange checks only whether the value is the same as the one that
was passed in as the current value. If some other thread modifies the atomic variable’s
value and then resets it to the original value again, a compare_exchange on that variable
will still succeed. This is often referred to as the A-B-A problem.
Unlike simple loads and stores, compare_exchange takes two Ordering arguments.
The first is the “success ordering,” and it dictates what memory
ordering should be used for the load and store that the compare_exchange
represents in the case that the value was successfully updated. The second
is the “failure ordering,” and it dictates the memory ordering for the load
if the loaded value does not match the expected current value. These two
orderings are kept separate so that the developer can give the CPU leeway
to improve execution performance by reordering loads and stores on failure
when appropriate, but still get the correct ordering on success. In this
case, it’s okay to reorder loads and stores across failed iterations of the lock
acquisition loop, but it’s not okay to reorder loads and stores inside the critical
section in such a way that they end up outside of it.
Even though its interface is simple, compare_exchange is a very powerful
synchronization primitive—so much so that it’s been theoretically proven
that you can build all other distributed consensus primitives using only
compare_exchange! For that reason, it is the workhorse of many, if not most,
synchronization constructs when you really dig into the implementation
details.
Be aware, though, that a compare_exchange requires that a single CPU has
exclusive access to the underlying value, and it is therefore a form of mutual
exclusion at the hardware level. This in turn means that compare_exchange
Concurrency (and Parallelism) 187
can quickly become a scalability bottleneck: only one CPU can make progress
at a time, so there’s a portion of your code that will not scale with the
number of cores. In fact, it’s probably worse than that—the CPUs have to
coordinate to ensure that only one CPU succeeds at a compare_exchange for a
variable at a time (take a look at the MESI protocol if you’re curious about
how that works), and that coordination grows quadratically more costly the
more CPUs are involved!
COMPARE_EXCHANGE_WEAK
The careful documentation reader will notice that compare_exchange has a suspiciously
named cousin, compare_exchange_weak, and wonder what the difference
is. The weak variant of compare_exchange is allowed to fail even if the atomic
variable’s value does still match the expected value that the user passed in,
whereas the strong variant must succeed in this case.
This might seem odd—how could an atomic value swap fail except if the
value has changed? The answer lies in system architectures that do not have a
native compare_exchange operation. For example, ARM processors instead have
locked load and conditional store operations, where a conditional store will fail
if the value read by an associated locked load has not been written to since
the load. The Rust standard library implements compare_exchange on ARM by
calling this pair of instructions in a loop and returning only once the conditional
store succeeds. This makes the code in Listing 10-6 needlessly inefficient—we
end up with a nested loop, which requires more instructions and is harder to
optimize. Since we already have a loop in this case, we could instead use compare_
exchange_weak, remove the unreachable!() on Err(false), and get better
machine code on ARM and the same compiled code on x86!
The Fetch Methods
Fetch methods (fetch_add, fetch_sub, fetch_and, and the like) are designed to
allow more efficient execution of atomic operations that commute—that
is, operations that have meaningful semantics regardless of the order they
execute in. The motivation for this is that the compare_exchange method
is powerful, but also costly—if two threads both want to update a single
atomic variable, one will succeed, while the other will fail and have to retry.
If many threads are involved, they all have to mediate sequential access to
the underlying value, and there will be plenty of spinning while threads
retry on failure.
For simple operations that commute, rather than fail and retry just
because another thread modified the value, we can tell the CPU what
operation to perform on the atomic variable. It’ll then perform that operation
on whatever the current value happens to be when the CPU eventually
gets exclusive access. Think of an AtomicUsize that counts the number of
188 Chapter 10
operations a pool of threads has completed. If two threads both complete a
job at the same time, it doesn’t matter which one updates the counter first
as long as both their increments are counted.
The fetch methods implement these kinds of commutative operations.
They perform a read and a store operation in a single step and
guarantee that the store operation was performed on the atomic variable
when it held exactly the value returned by the method. As an example,
AtomicUsize::fetch_add(1, Ordering::Relaxed) never fails—it always adds 1 to
the current value of the AtomicUsize, no matter what it is, and returns the
value of the AtomicUsize precisely when this thread’s 1 was added.
The fetch methods tend to be more efficient than compare_exchange
because they don’t require threads to fail and retry when multiple threads
contend for access to a variable. Some hardware architectures even have
specialized fetch method implementations that scale much better as the
number of involved CPUs grows. Nevertheless, if enough threads try to
operate on the same atomic variable, those operations will begin to slow
down and exhibit sublinear scaling due to the coordination required. In
general, the best way to significantly improve the performance of a concurrent
algorithm is to split contended variables into more atomic variables
that are each less contended, rather than switching from compare_exchange to
a fetch method.
NOTE The fetch_update method is somewhat deceptively named—behind the scenes, it is
really just a compare_exchange_weak loop, so its performance profile will more closely
match that of compare_exchange than the other fetch methods.
Sane Concurrency
Writing correct and performant concurrent code is harder than writing
sequential code; you have to consider not only possible execution interleavings
but also how your code interacts with the compiler, the CPU, and the
memory subsystem. With such a wide array of footguns at your disposal, it’s
easy to want to throw your hands in the air and just give up on concurrency
altogether. In this section we’ll explore some techniques and tools that can
help ensure that you write correct concurrent code without (as much) fear.
Start Simple
It is a fact of life that simple, straightforward, easy-to-follow code is more
likely to be correct. This principle also applies to concurrent code—always
start with the simplest concurrent design you can think of, then measure,
and only if measurement reveals a performance problem should you optimize
your algorithm.
To follow this tip in practice, start out with concurrency patterns that
do not require intricate use of atomics or lots of fine-grained locks. Begin
with multiple threads that run sequential code and communicate over
channels, or that cooperate through locks, and then benchmark the resulting
performance with the workload you care about. You’re much less likely
Concurrency (and Parallelism) 189
to make mistakes this way than by implementing fancy lockless algorithms
or by splitting your locks into a thousand pieces to avoid false sharing. For
many use cases, these designs are plenty fast enough; it turns out a lot of
time and effort has gone into making channels and locks perform well!
And if the simple approach is fast enough for your use case, why introduce
more complex and error-prone code?
If your benchmarks indicate a performance problem, then figure out
exactly which part of your system scales poorly. Focus on fixing that bottleneck
in isolation where you can, and try to do so with small adjustments
where possible. Maybe it’s enough to split a lock in two rather than move
to a concurrent hash table, or to introduce another thread and a channel
rather than implement a lock-free work stealing queue. If so, do that.
Even when you do have to work directly with atomics and the like, keep
things simple until there’s a proven need to optimize—use Ordering::SeqCst
and compare_exchange at first, and then iterate if you find concrete evidence
that those are becoming bottlenecks that must be taken care of.
Write Stress Tests
As the author, you have a lot of insight into where bugs in your code
may hide, without necessarily knowing what those bugs are (yet, anyway).
Writing stress tests is a good way to shake out some of the hidden bugs.
Stress tests don’t necessarily perform a complex sequence of steps but
instead have lots of threads doing relatively simple operations in parallel.
For example, if you were writing a concurrent hash map, one stress test
might be to have N threads insert or update keys and M threads read keys
in such a way that those M+N threads are likely to often choose the same
keys. Such a test doesn’t test for a particular outcome or value but instead
tries to trigger many possible interleavings of operations in the hopes that
buggy interleavings might reveal themselves.
Stress tests resemble fuzz tests in many ways; whereas fuzzing generates
many random inputs to a given function, the stress test instead generates
many random thread and memory access schedules. Just like fuzzers,
stress tests are therefore only as good as the assertions in your code; they
can’t tell you about a bug that doesn’t manifest in some easy-to-spot way
like an assertion failure or some other kind of panic. For that reason, it’s a
good idea to litter your low-level concurrency code with assertions, or debug_
assert_* if you’re worried about runtime cost in particularly hot loops.
Use Concurrency Testing Tools
The primary challenge in writing concurrent code is to handle all the possible
ways the execution of different threads can interleave. As we saw in the
Ordering::SeqCst example in Listing 10-4, it’s not just the thread scheduling
that matters, but also which memory values are possible for a given thread
to observe at any given point in time. Writing tests that execute every possible
legal execution is not only tedious but also difficult—you need very
low-level control over which threads execute when and what values their
reads return, which the operating system likely doesn’t provide.
190 Chapter 10
Model Checking with Loom
Luckily, a tool already exists that can simplify this execution exploration
for you in the form of the loom crate. Given the relative release cycles of this
book and that of a Rust crate, I won’t give any examples of how to use Loom
here, as they’d likely be out of date by the time you read this book, but I will
give an overview of what it does.
Loom expects you to write dedicated test cases in the form of closures
that you pass into a Loom model. The model keeps track of all cross-thread
interactions and tries to intelligently explore all possible iterations of those
interactions by executing the test case closure multiple times. To detect
and control thread interactions, Loom provides replacement types for all
the types in the standard library that allow threads to coordinate with one
another; that includes most types under std::sync and std::thread as well
as UnsafeCell and a few others. Loom expects your application to use those
replacement types whenever you run the Loom tests. The replacement
types tie into the Loom executor and perform a dual function: they act as
rescheduling points so that Loom can choose which operation to run next
after each possible thread interaction point, and they inform Loom of new
possible interleavings to consider. Essentially, Loom builds up a tree of all
the possible future executions for each point at which multiple execution
interleavings are possible and then tries to execute all of them, one after
the other.
Loom attempts to fully explore all possible executions of the test
cases you provide it with, which means it can find bugs that occur only in
extremely rare executions that stress testing would not find in a hundred
years. While that’s great for smaller test cases, it’s generally not feasible
to apply that kind of rigorous testing to larger test cases that test more
involved sequences of operations or require many threads to run at once.
Loom would simply take too long to get decent coverage of the code. In
practice, you may therefore want to tell Loom to consider only a subset of
the possible executions, which Loom’s documentation has more details on.
Like with stress tests, Loom can catch only bugs that manifest as panics,
so that’s yet another reason to spend some time placing strategic assertions
in your concurrent code! In many cases, it may even be worthwhile to add
additional state tracking and bookkeeping instructions to your concurrent
code to give you better assertions.
Runtime Checking with ThreadSanitizer
For larger test cases, your best bet is to run the test through a couple of iterations
under Google’s excellent ThreadSanitizer, also known as TSan. TSan
automatically augments your code by placing extra bookkeeping instructions
prior to every memory access. Then, as your code runs, those bookkeeping
instructions update and check a special state machine that flags any concurrent
memory operations that indicate a problematic race condition. For
example, if thread B writes to some atomic value X, but has not synchronized
(lots of hand waving here) with the thread that wrote the previous value of X
that indicates a write/write race, which is nearly always a bug.
Concurrency (and Parallelism) 191
Since TSan only observes your code running and does not execute
it over and over again like Loom, it generally only adds a constant-factor
overhead to the runtime of your program. While that factor can be significant
(5–15 times at the time of writing), it’s still small enough that you can
execute even most complex test cases in a reasonable amount of time.
At the time of writing, to use TSan you need to use a nightly version of
the Rust compiler and pass in the -Zsanitizer=thread command-line argument
(or set it in RUSTFLAGS), though hopefully in time this will be a standard
supported option. Other sanitizers are also available that check things like
out-of-bounds memory accesses, use-after-free, memory leaks, and reads of
uninitialized memory, and you may want to run your concurrent test suite
through those too!
HEISENBUGS
Heisenbugs are bugs that seem to disappear when you try to study them. This
happens quite frequently when trying to debug highly concurrent code; the
additional instrumentation to debug the problem changes the relative timing of
concurrent events and might cause the execution interleaving that triggered the
bug to no longer happen.
A particularly common cause of disappearing concurrency bugs is using
print statements, which is by far one of the most common debugging techniques.
There are two reasons why print statements have such an outsized effect on
concurrency bugs. The first, and perhaps most obvious, is that relatively speaking,
printing something to the user’s terminal (or wherever standard output
points) takes quite a long time, especially if your program is producing a lot
of output. Writing to the terminal requires, at the very least, a round-trip to the
operating system kernel to perform the write, but the write may also have to
wait for the terminal itself to read from the process’s output into its own buffers.
All that extra time might so much delay the operation that previously raced with
an operation in some other thread that the race condition disappears.
The second reason why print statements disturb concurrent execution patterns
is that writing to standard output is (generally) guarded by a lock. If you look
inside the Stdout type in the standard library, you’ll see that it holds a Mutex that
guards access to the output stream. It does this so that the output isn’t garbled too
badly if multiple threads try to write at the same time—without a lock, a given
line might have characters interspersed from multiple thread writes, but with the
lock the threads will take turns writing instead. Unfortunately, acquiring the output
lock, is another thread synchronization point, and one that every printing thread is
involved in. This means that if your code was previously broken due to missing synchronization
between two threads, or just because a particular race between two
threads was possible, adding print statements might fix that bug as a side effect!
In general, when you spot what seems like a Heisenbug, try to find other
ways to narrow down the problem. That might involve using Loom or TSan,
(continued)

192 Chapter 10
using gdb or lldb, or using a per-thread in-memory log that you print only at the
end. Many logging frameworks also work hard to avoid synchronization points
on the critical path of issuing log events, so switching to one of those might
make your life easier. As an added bonus, good logging that you leave behind
after fixing a particular bug might come in handy later. Personally I’m a big fan
of the tracing crate, but there are many good options out there.
Summary
In this chapter, we first covered common correctness and performance pitfalls
in concurrent Rust, and some of the high-level concurrency patterns
that successful concurrent applications tend to use to work around them.
We also explored how asynchronous Rust enables concurrency without parallelism,
and how to explicitly introduce parallelism in asynchronous Rust
code. We then dove deeper into Rust’s many different lower-level concurrency
primitives, including how they work, how they differ, and what they’re
all for. Finally, we explored techniques for writing better concurrent code
and looked at tools like Loom and TSan that can help you vet that code. In
the next chapter we’ll continue our journey through the lower levels of Rust
by digging into foreign function interfaces, which allow Rust code to link
directly against code written in other languages.

### FOREIGN FUNCTION I NTERFACES

Not all code is written in Rust. It’s shocking,
I know. Every so often, you’ll need to interact
with code written in other languages,
either by calling into such code from Rust or
by allowing that code to call your Rust code. You can
achieve this through foreign function interfaces (FFI).
In this chapter we’ll first look at the primary mechanism Rust provides
for FFI: the extern keyword. We’ll see how to use extern both to expose Rust
functions and statics to other languages and to give Rust access to functions
and static variables provided from outside the Rust bubble. Then, we’ll
walk through how to align Rust types with types defined in other languages
and explore some of the intricacies of allowing data to flow across the FFI
boundary. Finally, we’ll talk about some of the tools you’ll likely want to use
if you’re doing any nontrivial amount of FFI.
194 Chapter 11
NOTE While I often refer to FFI as being about crossing the boundary between one language
and another, FFI can also occur entirely inside Rust-land. If one Rust program shares
memory with another Rust program but the two aren’t compiled together—say, if you’re
using a dynamically linked library in your Rust program that happens to be written in
Rust, but you just have the C-compatible .so file—the same complications arise.
Crossing Boundaries with extern
FFI is, ultimately, all about accessing bytes that originate somewhere outside
your application’s Rust code. For that, Rust provides two primary building
blocks: symbols, which are names assigned to particular addresses in a
given segment of your binary that allow you to share memory (be it for data
or code) between the external origin and your Rust code, and calling conventions
that provide a common understanding of how to call functions stored
in such shared memory. We’ll look at each of these in turn.
Symbols
Any binary artifact that the compiler produces from your code is filled with
symbols—every function or static variable you define has a symbol that
points to its location in the compiled binary. Generic functions may even
have multiple symbols, one for each monomorphization of the function the
compiler generates!
Normally, you don’t have to think about symbols—they’re used internally
by the compiler to pass around the final address of a function or
static variable in your binary. This is how the compiler knows what location
in memory each function call should target when it generates the final
machine code, or where to read from if your code accesses a static variable.
Since you don’t usually refer to symbols directly in your code, the compiler
defaults to choosing semirandom names for them—you may have two functions
called foo in different parts of your code, but the compiler will generate
distinct symbols from them so that there’s no confusion.
However, using random names for symbols won’t work when you want
to call a function or access a static variable that isn’t compiled at the same
time, such as code that’s written in a different language and thus compiled
by a different compiler. You can’t tell Rust about a static variable defined in
C if the symbol for that variable has a semirandom name that keeps changing.
Conversely, you can’t tell Python’s FFI interface about a Rust function if
you can’t produce a stable name for it.
To use a symbol with an external origin, we also need some way to tell
Rust about a variable or function in such a manner that the compiler will
look for that same symbol defined elsewhere rather than defining its own
(we’ll talk about how that search happens later). Otherwise, we would just
end up with two identical symbols for that function or static variable, and
no sharing would take place. In fact, in all likelihood, compilation would
fail since any code that referred to that symbol wouldn’t know which definition
(that is, which address) to use for it!
Foreign Function Interfaces 195
NOTE A quick note about terminology: a symbol can be declared multiple times but
defined only once. Every declaration of a symbol will link to the same single definition
for that symbol at linking time. If no definition for a declaration is found, or if
there are multiple definitions, the linker will complain.
An Aside on Compilation and Linking
Compiler crash course time! Having a rough idea of the complicated process
of turning code into a runnable binary will help you understand FFI
better. You see, the compiler isn’t one monolithic program but is (typically)
broken down into a handful of smaller programs that each perform distinct
tasks and run one after the other. At a high level, there are three distinct
phases to compilation—compilation, code generation, and linking—handled by
three different components.
The first phase is performed by what most people tend to think of
as “the compiler”; it deals with type checking, borrow checking, monomorphization,
and other features we associate with a given programming
language. This phase generates no machine code but rather a low-level
representation of the code that uses heavily annotated abstract machine
operations. That low-level representation is then passed to the code generation
tool, which is what produces machine code that can actually run on a
given CPU.
These two operations, taken together, do not have to be run in a single
big pass over the whole codebase all at once. Instead, the codebase can be
sliced into smaller chunks that are then run through compilation concurrently.
For example, Rust generally compiles different crates independently
and in parallel as long as there isn’t a dependency between them. It can also
invoke the code generation tool for independent crates separately to process
them in parallel. Rust can often even compile multiple smaller slices of
a single crate separately!
Once the machine code for every piece of the application has been
generated, those pieces can then be wired together. This is done in the
linking phase by, unsurprisingly, the linker. The linker’s primary job is to
take all the binary artifacts, called object files, produced by code generation,
stitch them together into a single file, and then replace every reference to a
symbol with the final memory address of that symbol. This is how you can
define a function in one crate and call it from another but still compile the
two crates separately.
The linker is what enables FFI to work. It doesn’t care how each of the
input object files were constructed; it just dutifully links together all the
object files and then resolves any shared symbols. One object file may originally
have been Rust code, one originally C code, and one may be a binary
blob downloaded from the internet; as long as they all use the same symbol
names, the linker will make sure that the resulting machine code uses the
correct cross-referenced addresses for any shared symbols.
A symbol can be linked either statically or dynamically. Static linking
is the simplest, as each reference to a symbol is simply replaced with the
address of that symbol’s definition. Dynamic linking, on the other hand,
196 Chapter 11
ties each reference to a symbol to a bit of generated code that tries to find
the symbol’s definition when the program runs. We’ll talk more about these
linking modes a little later. Rust generally defaults to static linking for Rust
code, and dynamic linking for FFI.
Using extern
The extern keyword is the mechanism that allows us to declare a symbol as
residing within a foreign interface. Specifically, it declares the existence of
a symbol that’s defined elsewhere. In Listing 11-1 we define a static variable
called RS_DEBUG in Rust that we make available to other code via FFI. We also
declare a static variable called FOREIGN_DEBUG whose definition is unspecified
but will be resolved at linking time.
#[no_mangle]
pub static RS_DEBUG: bool = true;
extern {
static FOREIGN_DEBUG: bool;
}
Listing 11-1: Exposing a Rust static variable, and accessing one declared elsewhere,
through FFI
The #[no_mangle] attribute ensures that RS_DEBUG retains that name during
compilation rather than having the compiler assign it another symbol
name to, for example, distinguish it from another (non-FFI) RS_DEBUG static
variable elsewhere in the program. The variable is also declared as pub since
it’s a part of the crate’s public API, though that annotation isn’t strictly
necessary on items marked #[no_mangle]. Note that we don’t use extern for
RS_DEBUG, since it’s defined here. It will still be accessible to link against from
other languages.
The extern block surrounding the FOREIGN_DEBUG static variable denotes
that this declaration refers to a location that Rust will learn at linking
time based on where the definition of the same symbol is located. Since
it’s defined elsewhere, we don’t give it an initialization value, just a type,
which should match the type used at the definition site. Because Rust
doesn’t know anything about the code that defines the static variable,
and thus can’t check that you’ve declared the correct type for the symbol,
FOREIGN_DEBUG can be accessed only inside an unsafe block.
NOTE Static variables in Rust aren’t mutable by default, regardless of whether they’re in an
extern block. These variables are always available from any thread, so mutable access
would pose a data race risk. You can declare a static as mut, but if you do, it becomes
unsafe to access.
The procedure to declare FFI functions is very similar. In Listing 11-2,
we make hello_rust accessible to non-Rust code and pull in the external
hello_foreign function.
Foreign Function Interfaces 197
#[no_mangle]
pub extern fn hello_rust(i: i32) { ... }
extern {
fn hello_foreign(i: i32);
}
Listing 11-2: Exposing a Rust function, and accessing one defined elsewhere, through FFI
The building blocks are all the same as in Listing 11-1 with the exception
that the Rust function is declared using extern fn, which we’ll explore
in the next section.
If there are multiple definitions of a given extern symbol like FOREIGN_
DEBUG or hello_foreign, you can explicitly specify which library the symbol
should link against using the #[link] attribute. If you don’t, the linker will
give you an error saying that it’s found multiple definitions for the symbol
in question. For example, if you prefix an extern block with #[link(name =
"crypto")], you’re telling the linker to resolve any symbols (whether statics
or functions) against a linked library named “crypto.” You can also rename
an external static or function in your Rust code by annotating its declaration
with #[link_name = "<actual_symbol_name>"], and then the item links to
whatever name you wish. Similarly, you can rename a Rust item for export
using #[export_name = "<export_symbol_name>"].
Link Kinds
#[link] also accepts the argument kind, which dictates how the items in the
block should be linked. The argument defaults to "dylib", which signifies
C-compatible dynamic linking. The alternative kind value is "static", which
indicates that the items in the block should be linked fully at compile time
(that is, statically). This essentially means that the external code is wired
directly into the binary produced by the compiler , and thus doesn’t need to
exist at runtime. There are a few other kinds as well, but they are much less
common and outside the scope of this book.
There are several trade-offs between static and dynamic linking, but the
main considerations are security, binary size, and distribution. First, dynamic
linking tends to be more secure because it makes it easier to upgrade libraries
independently. Dynamic linking allows whoever deploys a binary that contains
your code to upgrade libraries your code links against without having
to recompile your code. If, say, libcrypto gets a security update, the user can
update the crypto library on the host and restart the binary, and the updated
library code will be used automatically. With static compilation, the library’s
code is hardwired into the binary, so the user would have to recompile your
code against an upgraded version of the library to get the update.
Dynamic linking also tends to produce smaller binaries. Since static
compilation includes any linked code into the final binary output, and any
code that code in turn pulls in, it produces larger binaries. With dynamic
linking, each external item includes just a small bit of wrapper code that
loads the indicated library at runtime and then forwards the access.
198 Chapter 11
So far, static linking may not seem very attractive, but it has one big advantage
over dynamic linking: ease of distribution. With dynamic linking, anyone
who wants to run a binary that includes your code must also have any libraries
your code links against. Not only that, but they must make sure the version of
each such library they have is compatible with what your code expects. This
may be fine for libraries like glibc or OpenSSL that are available on most systems,
but it poses a problem for more obscure libraries. The user then needs
to be aware that they should install that library and must hunt for it in order to
run your code! With static linking, the library’s code is embedded directly into
the binary output, so the user doesn’t need to install it themselves.
Ultimately, there isn’t a right choice between static and dynamic linking.
Dynamic linking is usually a good default, but static compilation may be a
better option for particularly constrained deployment environments or for
very small or niche library dependencies. Use your best judgment!
Calling Conventions
Symbols dictate where a given function or variable is defined, but that’s not
enough to allow function calls across FFI boundaries. To call a foreign
function in any language, the compiler also needs to know its calling convention,
which dictates the assembly code to use to invoke the function. We
won’t get into the actual technical details of each calling convention here,
but as a general overview, the convention dictates:
• How the stack frame for the call is set up
• How arguments are passed (whether on the stack or in registers, in
order or in reverse)
• How the function is told where to jump back to when it returns
• How various CPU states, like registers, are restored in the caller after
the function completes
Rust has its own unique calling convention that isn’t standardized and
is allowed to be changed by the compiler over time. This works fine as long
as all function definitions and calls are compiled by the same Rust compiler,
but it is problematic if you want interoperability with external code
because that external code doesn’t know about the Rust calling convention.
Every Rust function is implicitly declared with extern "Rust" if you don’t
declare anything else. Using extern on its own, as in Listing 11-2, is shorthand
for extern "C", which means “use the standard C calling convention.”
The shorthand is there because the C calling convention is what you want
in nearly every case of FFI.
NOTE Unwinding generally works only with regular Rust functions. If you unwind
across the end of a Rust function that isn’t extern "Rust", your program will abort.
Unwinding across the FFI boundary into external code is undefined behavior. With
RFC 2945, Rust gained a new extern declaration, extern "C-unwind"; this permits
unwinding across FFI boundaries in particular situations, but if you wish to use it
you should read the RFC carefully.
Foreign Function Interfaces 199
Rust also supports a number of other calling conventions that you supply
as a string following the extern keyword (in both fn and block context). For
example, extern "system" says to use the calling convention of the operating
system’s standard library interface, which at the time of writing is the same
as "C" everywhere except on Win32, which uses the "stdcall" calling convention.
In general, you’ll rarely need to supply a calling convention explicitly
unless you’re working with particularly platform-specific or highly optimized
external interfaces, so just extern (which is extern "C") will be fine.
NOTE A function’s calling convention is part of its type. That is, the type extern "C" fn()
is not the same as fn() (or extern "Rust" fn()), which is different again from extern
"system" fn().
OTHER BINARY ARTIFACTS
Normally, you compile Rust code only to run its tests or build a binary that
you’re then going to distribute or run. Unlike in many other languages, you
don’t generally compile a Rust library to distribute it to others—if you run a command
like cargo publish, it just wraps up your crate’s source code and uploads
it to crates.io. This is mostly because it is difficult to distribute generic code as
anything but source code. Since the compiler monomorphizes each generic
function to the provided type arguments, and those types may be defined in
the caller’s crate, the compiler must have access to the function’s generic form,
which means no optimized machine code!
Technically speaking, Rust does compile binary library artifacts, called rlibs,
of each dependency that it combines in the end. These rlibs include the information
necessary to resolve generic types, but they are specific to the exact compiler
used and can’t generally be distributed in any meaningful way.
So what do you do if you want to write a library in Rust that you then
want to interface with from another programming language? The solution is to
produce C-compatible library files in the form of dynamically linked libraries
(.so files on Unix, .dylib files on macOS, and .dll files on Windows) and statically
linked libraries (.a files on Unix/macOS and .lib files on Windows). Those
files look like files produced by C code, so they can also be used by other languages
that know how to interact with C.
To produce these C-compatible binary artifacts, you set the crate-type
field of the [lib] section of your Cargo.toml file. The field takes an array of values,
which would normally just be "lib" to indicate a standard Rust library (an
rlib). Cargo applies some heuristics that will set this value automatically if your
crate is clearly not a library (for example, if it's a procedural macro), but best
practice is to set this value explicitly if you’re producing anything but a good ol’
Rust library.
There are a number of different crate types, but the relevant ones here are
"cdylib" and "staticlib", which produce C-compatible library files that are
dynamically and statically linked, respectively. Keep in mind that when you
(continued)
200 Chapter 11
produce one of these artifact types, only publicly available symbols are available—
that is, public and #[no_mangle] static variables and functions. Things like
types and constants won’t be available, even if they’re marked pub, since they
have no meaningful representation in a binary library file.
Types Across Language Boundaries
With FFI, type layout is crucial; if one language lays out the memory for
some shared data one way but the language on the other side of the FFI
boundary expects it to be laid out differently, then the two sides will interpret
the data inconsistently. In this section, we’ll look at how to make types
match up over FFI, and other aspects of types to be aware of when you cross
the boundaries between languages.
Type Matching
Types aren’t shared across the FFI boundary. When you declare a type in
Rust, that type information is lost entirely upon compilation. All that’s communicated
to the other side is the bits that make up values of that type.
You therefore need to declare the type for those bits on both sides of the
boundary. When you declare the Rust version of the type, you first must
make sure the primitives contained within the type match up. For example,
if C is used on the other side of the boundary, and the C type uses an int,
the Rust code had better use the exact Rust equivalent: an i32. To take
some of the guesswork out of that process, for interfaces that use C-like
types the Rust standard library provides you with the correct C types in
the std::os::raw module, which defines type c_int = i32, type c_char = i8/
u8 depending on whether char is signed, type c_long = i32/i64 depending on
the target pointer width, and so on.
NOTE Take particular note of quirky integer types in C like __be32. These often do not translate
directly to Rust types and may be best left as something like [u8; 4]. For example,
__be32 is always encoded as big-endian, whereas Rust’s i32 uses the endianness of the
current platform.
With more complex types like vectors and strings, you usually need
to do the mapping manually. For example, since C tends to represent
a string as a sequence of bytes terminated with a 0 byte, rather than a
UTF-8–encoded string with the length stored separately, you cannot generally
use Rust’s string types over FFI. Instead, assuming the other side
uses a C-style string representation, you should use the std::ffi::CStr and
std::ffi::CString types for borrowed and owned strings, respectively. For
vectors, you’ll likely want to use a raw pointer to the first element and then
pass the length separately—the Vec::into_raw_parts method may come in
handy for that.
Foreign Function Interfaces 201
For types that contain other types, such as structs and unions, you also
need to deal with layout and alignment. As we discussed in Chapter 2, Rust
lays out types in an undefined way by default, so at the very least you will
want to use #[repr(C)] to ensure that the type has a deterministic layout and
alignment that mirrors what’s (likely and hopefully) used across the FFI
boundary. If the interface also specifies other configurations for the type,
such as manually setting its alignment or removing padding, you’ll need to
adjust your #[repr] accordingly.
A Rust enum has multiple possible C-style representations depending
on whether the enum contains data or not. Consider an enum without data,
like this:
enum Foo { Bar, Baz }
With #[repr(C)], the type Foo is encoded using just a single integer of
the same size that a C compiler would choose for an enum with the same
number of variants. The first variant has the value 0, the second the value 1,
and so on. You can also manually assign values to each variant, as shown in
Listing 11-3.
#[repr(C)]
enum Foo {
Bar = 1,
Baz = 2,
}
Listing 11-3: Defining explicit variant values for a dataless enum
NOTE Technically, the specification says that the first variant’s value is 0 and every subsequent
variant’s value is one greater than that of the previous one. This makes a difference
if you manually set the value for some variants but not others—those you do
not set will continue from the last one you did set.
You should be careful about mapping enum-like types in C to Rust
this way, however, as only the values for defined variants are valid for an
instance of the enum type. This tends to get you into trouble with C-style
enumerations that often function more like bitsets, where variants can be
bitwise ORed together to produce a value that encapsulates multiple variants
at once. In the example from Listing 11-3, for instance, a value of 3 produced
by taking Bar | Baz would not be valid for Foo in Rust! If you need to
model a C API that uses an enumeration for a set of bitflags that can be set
and unset individually, consider using a newtype wrapper around an integer
type, with associated constants for each variant and implementations of
the various Bit* traits for improved ergonomics. Or use the bitflags crate.
NOTE For fieldless enums, you can also pass a numeric type to #[repr] to use a different
type than isize for the discriminator. For example, #[repr(u8)] will encode the discriminator
using a single unsigned byte. For a data-carrying enum, you can pass
#[repr(C, u8)] to get the same effect.
202 Chapter 11
On an enum that contains data, the #[repr(C)] attribute causes the enum
to be represented using a tagged union. That is, it is represented in memory
by a #[repr(C)] struct with two fields, where the first is the discriminator as
it would be encoded if none of the variants had fields, and the second is a
union of the data structures for each variant. For a concrete example, consider
the enum and associated representation in Listing 11-4.
#[repr(C)]
enum Foo {
Bar(i32),
Baz { a: bool, b: f64 }
}
// is represented as
#[repr(C)]
enum FooTag { Bar, Baz }
#[repr(C)]
struct FooBar(i32);
#[repr(C)]
struct FooBaz{ a: bool, b: f64 }
#[repr(C)]
union FooData {
bar: FooBar,
baz: FooBaz,
}
#[repr(C)]
struct Foo {
tag: FooTag,
data: FooData
}
Listing 11-4: Rust enums with #[repr(C)] are represented as tagged unions.
THE NICHE OPTIMIZATION IN FFI
In Chapter 9 we talked about the niche optimization, where the Rust compiler
uses invalid bit patterns to represent enum variants that hold no data. The fact
that this optimization is guaranteed leads to an interesting interaction with FFI.
Specifically, it means that nullable pointers can always be represented in FFI types
using an Option-wrapped pointer type. For example, a nullable function pointer
can be represented as Option<extern fn(...)>, and a nullable data pointer can
be represented as Option<*mut T>. These will transparently do the right thing if an
all-zero bit pattern value is provided, and will represent it as None in Rust.
Allocations
When you allocate memory, that allocation belongs to its allocator and can
be freed only by that same allocator. This is the case if you use multiple


Foreign Function Interfaces 203
allocators within Rust and also if you are allocating memory both in Rust
and with some allocator on the other side of the FFI boundary. You’re
free to send pointers across the boundary and access that memory to your
heart’s content, but when it comes to releasing the memory again, it needs
to be returned to the appropriate allocator.
Most FFI interfaces will have one of two configurations for handling
allocation: either the caller provides data pointers to chunks of memory
or the interface exposes dedicated freeing methods to which any allocated
resources should be returned when they are no longer needed. Listing 11-5
shows an example of Rust declarations of some signatures from the
OpenSSL library that use implementation-managed memory.
// One function allocates memory for a new object.
extern fn ECDSA_SIG_new() -> *mut ECDSA_SIG;
// And another accepts a pointer created by new
// and deallocates it when the caller is done with it.
extern fn ECDSA_SIG_free(sig: *mut ECDSA_SIG);
Listing 11-5: An implementation-managed memory interface
The functions ECDSA_SIG_new and ECDSA_SIG_free form a pair, where the
caller is expected to call the new function, use the returned pointer for as
long as it needs (likely by passing it to other functions in turn), and then
finally pass the pointer to the free function once it’s done with the referenced
resource. Presumably, the implementation allocates memory in
the new function and deallocates it in the free function. If these functions
were defined in Rust, the new function would likely use Box::new, and the
free function would invoke Box::from_raw and then drop the value to run its
destructor.
Listing 11-6 shows an example of caller-managed memory.
// An example of caller-managed memory.
// The caller provides a pointer to a chunk of memory,
// which the implementation then uses to instantiate its own types.
// No free function is provided, as that happens in the caller.
extern fn BIO_new_mem_buf(buf: *const c_void, len: c_int) -> *mut BIO
Listing 11-6: A caller-managed memory interface
Here, the BIO_new_mem_buf function instead has the caller supply the
backing memory. The caller can choose to allocate memory on the heap,
or use whatever other mechanism it deems fit for obtaining the required
memory, and then passes it to the library. The onus is then on the caller to
ensure that the memory is later deallocated, but only once it is no longer
needed by the FFI implementation!
You can use either of these approaches in your FFI APIs or even mix
and match them if you wish. As a general rule of thumb, allow the caller to
pass in memory when doing so is feasible, since it gives the caller more freedom
to manage memory as it deems appropriate. For example, the caller
may be using a highly specialized allocator on some custom operating
204 Chapter 11
system, and may not want to be forced to use the standard allocator your
implementation would use. If the caller can pass in the memory, it might
even avoid allocations entirely if it can instead use stack memory or reuse
already allocated memory. However, keep in mind that the ergonomics of a
caller-managed interface are often more convoluted, since the caller must
now do all the work to figure out how much memory to allocate and then
set that up before it can call into your library.
In some instances, it may even be impossible for the caller to know
ahead of time how much memory to allocate—for example, if your library’s
types are opaque (and thus not known to the caller) or can change over
time, the caller won’t be able to predict the size of the allocation. Similarly,
if your code has to allocate more memory while it is running, such as if
you’re building a graph on the fly, the amount of memory needed may vary
dynamically at runtime. In such cases, you will have to use implementationmanaged
memory.
When you’re forced to make a trade-off, go with caller-allocated memory
for anything that is either large or frequent. In those cases the caller is
likely to care the most about controlling the allocations itself. For anything
else, it’s probably okay for your code to allocate and then expose destructor
functions for each relevant type.
Callbacks
You can pass function pointers across the FFI boundary and call the referenced
function through those pointers as long as the function pointer’s
type has an extern annotation that matches the function’s calling convention.
That is, you can define an extern "C" fn(c_int) -> c_int in Rust and
then pass a reference to that function to C code as a callback that the C
code will eventually invoke.
You do need to be careful using callbacks around panics, as having a
panic unwind past the end of a function that is anything but extern "Rust"
is undefined behavior. The Rust compiler will currently automatically abort
if it detects such a panic, but that may not always be the behavior you want.
Instead, you may want to use std::panic::catch_unwind to detect the panic in
any function marked extern, and then translate the panic into an error that
is FFI-compatible.
Safety
When you write Rust FFI bindings, most of the code that actually interfaces
with the FFI will be unsafe and will mainly revolve around raw pointers.
However, your goal should be to ultimately present a safe Rust interface on
top of the FFI. Doing so mainly comes down to reading carefully through
the invariants of the unsafe interface you are wrapping and then ensuring
you uphold them all through the Rust type system in the safe interface. The
three most important elements of safely encapsulating a foreign interface
are capturing & versus &mut accurately, implementing Send and Sync appropriately,
and ensuring that pointers cannot be accidentally confused. I’ll go
over how to enforce each of these next.
Foreign Function Interfaces 205
References and Lifetimes
If there’s a chance external code will modify data behind a given pointer,
make sure that the safe Rust interface has an exclusive reference to the
relevant data by taking &mut. Otherwise a user of your safe wrapper might
accidentally read from memory that the external code is simultaneously
modifying, and all hell will break loose!
You’ll also want to make good use of Rust lifetimes to ensure that all
pointers live for as long as the FFI requires. For example, imagine an external
interface that lets you create a Context and then lets you create a Device from
that Context with the requirement that the Context remain valid for as long as
the Device lives. In that case, any safe wrapper for the interface should enforce
that requirement in the type system by having Device hold a lifetime associated
with the borrow of Context that the Device was created from.
Send and Sync
Do not implement Send and Sync for types from an external library unless
that library explicitly documents that those types are thread-safe! It is the
safe Rust wrapper’s job to ensure that safe Rust code cannot violate the
invariants of the external code and thus trigger undefined behavior.
Sometimes, you may even want to introduce dummy types to enforce
external invariants. For example, say you have an event loop library with the
interface given in Listing 11-7.
extern fn start_main_loop();
extern fn next_event() -> *mut Event;
Listing 11-7: A library that expects single-threaded use
Now suppose that the documentation for the external library states that
next_event may be called only by the same thread that called start_main_loop.
However, here we have no type that we can avoid implementing Send for!
Instead, we can take a page out of Chapter 3 and introduce additional
marker state to enforce the invariant, as shown in Listing 11-8.
pub struct EventLoop(std::marker::PhantomData<*const ()>);
pub fn start() -> EventLoop {
unsafe { ffi::start_main_loop() };
EventLoop(std::marker::PhantomData)
}
impl EventLoop {
pub fn next_event(&self) -> Option<Event> {
let e = unsafe { ffi::next_event() };
// ...
}
}
Listing 11-8: Enforcing an FFI invariant by introducing auxiliary types
The empty type EventLoop doesn’t actually connect with anything in the
underlying external interface but rather enforces the contract that you call
206 Chapter 11
next_event only after calling start_main_loop, and only on the same thread.
You enforce the “same thread” part by making EventLoop neither Send nor
Sync, by having it hold a phantom raw pointer (which itself is neither Send
nor Sync).
Using PhantomData<*const ()> to “undo” the Send and Sync auto-traits as
we do here is a bit ugly and indirect. Rust does have an unstable compiler
feature that enables negative trait implementations like impl !Send for
EventLoop {}, but it’s surprisingly difficult to get its implementation right,
and it likely won’t stabilize for some time.
You may have noticed that nothing prevents the caller from invoking
start_main_loop multiple times, either from the same thread or from another
thread. How you’d handle that would depend on the semantics of the
library in question, so I’ll leave it to you as an exercise.
Pointer Confusion
In many FFI APIs, you don’t necessarily want the caller to know the internal
representation for each and every chunk of memory you give it pointers to.
The type might have internal state that the caller shouldn’t fiddle with, or
the state might be difficult to express in a cross-language-compatible way.
For these kinds of situations, C-style APIs usually expose void pointers, written
out as the C type void*, which is equivalent to *mut std::ffi::c_void in
Rust. A type-erased pointer like this is, effectively, just a pointer, and does
not convey anything about the thing it points to. For that reason, these
kinds of pointers are often referred to as opaque.
Opaque pointers effectively serve the role of visibility modifiers for
types across FFI boundaries—since the method signature does not say
what’s being pointed to, the caller has no option but to pass around the
pointer as is and use any available FFI methods to provide visibility into
the referenced data. Unfortunately, since one *mut c_void is indistinguishable
from another, there’s nothing stopping a user from taking an opaque
pointer as is returned from one FFI method and supplying it to a method
that expects a pointer to a different opaque type.
We can do better than this in Rust. To mitigate this kind of pointer
type confusion, we can avoid using *mut c_void directly for opaque pointers
in FFI, even if the actual interface calls for a void*, and instead construct
different empty types for each distinct opaque type. For example,
in Listing 11-9 I use two distinct opaque pointer types that cannot be
confused.
#[non_exhaustive] #[repr(transparent)] pub struct Foo(c_void);
#[non_exhaustive] #[repr(transparent)] pub struct Bar(c_void);
extern {
pub fn foo() -> *mut Foo;
pub fn take_foo(arg: *mut Foo);
pub fn take_bar(arg: *mut Bar);
}
Listing 11-9: Opaque pointer types that cannot be confused
Foreign Function Interfaces 207
Since Foo and Bar are both zero-sized types, they can be used in place of
() in the extern method signatures. Even better, since they are now distinct
types, Rust won’t let you use one where the other is required, so it’s now
impossible to call take_bar with a pointer you got back from foo. Adding the
#[non_exhaustive] annotation ensures that the Foo and Bar types cannot be
constructed outside of this crate.
bindgen and Build Scripts
Mapping out the Rust types and externs for a larger external library can be
quite a chore. Big libraries tend to have a large enough number of type and
method signatures to match up that writing out all the Rust equivalents is
time-consuming. They also have enough corner cases and C oddities that
some patterns are bound to require more careful thought to translate.
Luckily, the Rust community has developed a tool called bindgen that
significantly simplifies this process as long as you have C header files available
for the library you want to interface with. bindgen essentially encodes all
the rules and best practices we’ve discussed in this chapter, plus a number
of others, and wraps them up in a configurable code generator that takes in
C header files and spits out appropriate Rust equivalents.
bindgen provides a stand-alone binary that generates the Rust code for C
headers once, which is convenient when you want to check in the bindings.
This process allows you to hand-tune the generated bindings, should that
be necessary. If, on the other hand, you want to generate the bindings automatically
on every build and just include the C header files in your source
code, bindgen also ships as a library that you can invoke in a custom build
script for your package.
NOTE If you check in the bindings directly, keep in mind that they will be correct only on
the platform they were generated for. Generating the bindings in a build script will
generate them specifically for the current target platform, which is less likely to cause
platform-related layout inconsistencies.
You declare a build script by adding build = "<some-file.rs>" to the
[package] section of your Cargo.toml. This tells Cargo that, before compiling
your crate, it should compile <some-file.rs> as a stand-alone Rust program
and run it; only then should it compile the source code of your crate. The
build script also gets its own dependencies, which you declare in the [builddependencies]
section of your Cargo.toml.
NOTE If you name your build script build.rs, you don’t need to declare it in your Cargo.toml.
Build scripts come in very handy with FFI—they can compile a bundled
C library from source, dynamically discover and declare additional build
flags to be passed to the compiler, declare additional files that Cargo
should check for changes for the purposes of recompilation, and, you
guessed it, generate additional source files on the fly!
208 Chapter 11
Though build scripts are very versatile, beware of making them too
aware of the environment they run in. While you can use a build script
to detect if the Rust compiler version is a prime or if it’s going to rain in
Istanbul tomorrow, making your compilation dependent on such conditions
may make builds fail unexpectedly for other developers, which leads to a
poor development experience.
The build script can write files to a special directory supplied through
the OUT_DIR environment variable. The same directory and environment
variable are also accessible in the Rust source code at compile time so that
it can pick up files generated by the build script. To generate and use Rust
types from a C header, you first have your build script use the library version
of bindgen to read in a .h file and turn it into a file called, say, bindings.rs
inside OUT_DIR. You then add the following line to any Rust file in your crate
to include bindings.rs at compilation time:
include!(concat!(env!("OUT_DIR"), "/bindings.rs"));
Since the code in bindings.rs is autogenerated, it’s generally best practice
to place the bindings in their own crate and give the crate the same
name as the library the bindings are for, with the suffix -sys (for example,
openssl-sys). If you don’t follow this practice, releasing new versions of your
library will be much more painful, as it is illegal for two crates that link
against the same external library through the links key in Cargo.toml to
coexist in a given build. You would essentially have to upgrade the entire
ecosystem to the new major version of your library all at once. Separating
just the bindings into their own crate allows you to issue new major versions
of the wrapper crate that can be adopted incrementally. The separation
also allows you to cut a breaking release of the crate with those bindings if
the Rust bindings change—say, if the header files themselves are upgraded
or a bindgen upgrade causes the generated Rust code to change slightly—
without also having to cut a breaking release of the crate that safely wraps
the FFI bindings.
NOTE Remember that if you include any of the types from the -sys crate in the public interface
of your main library crate, changing the dependency on the -sys crate to a new
major version still constitutes a breaking change for your main library!
If your crate instead produces a library file that you intend others to
use through FFI, you should also publish a C header file for its interface
to make it easier to generate native bindings to your library from other
languages. However, that C header file then needs to be kept up to date as
your crate changes, which can become cumbersome as your library grows in
size. Fortunately, the Rust community has also developed a tool to automate
this task: cbindgen. Like bindgen, cbindgen is a build tool, and it also comes
as both a binary and a library for use in build scripts. Instead of taking
in a C header file and producing Rust, it takes Rust in and produces a C
header file. Since the C header file represents the main computer-readable
Foreign Function Interfaces 209
description of your crate’s FFI, I recommend manually looking it over to
make sure the autogenerated C code isn’t too unwieldy, though in general
cbindgen tends to produce fairly reasonable code. If it doesn’t, file a bug!
C++
I’ve mainly focused on C in this chapter as it’s the language most commonly
used to describe cross-language interfaces for libraries you can link against.
Nearly every programming language provides some way to interact with C
libraries, since they are so ubiquitous. While C++ feels closely related to C,
and many high-profile libraries are written in C++, it’s a very different beast
when it comes to FFI. Generating types and signatures to match a C header is
relatively straightforward, but that is not at all the case for C++. At the time of
writing, bindgen has decent support for generating bindings to C++, but they
are often lacking in ergonomics. For example, you generally have to manually
call constructors, destructors, overloaded operators, and the like. Some C++
features like template specialization also aren’t supported at all. If you do have
to interface with C++, I recommend you give the cxx crate a try.
Summary
In this chapter, we’ve covered how to use the extern keyword to call out of
Rust into external code, as well as how to use it to make Rust code accessible
to external code. We’ve also discussed how to align Rust types with types on
the other side of the FFI boundary, and some of the common pitfalls in trying
to get code written in two different languages to mesh well. Finally, we
talked about the bindgen and cbindgen tools, which make the experience of
keeping FFI bindings up to date much more pleasant. In the next chapter,
we’ll look at how to use Rust in more restricted environments, like embedded
devices, where the standard library may not be available and where
even a simple operation like allocating memory may not be possible.

### 12 RUST WITHOUT THE STANDARD LIBRARY

Rust is intended to be a language for systems
programming, but it isn’t always clear
what that really means. At the very least,
a systems programming language is usually
expected to allow the programmer to write programs
that do not rely on the operating system and can run
directly on the hardware, whether that is a thousandcore
supercomputer or an embedded device with
a single-
core ARM processor with a clock speed of
72MHz and 256KiB of memory.
In this chapter, we’ll take a look at how you can use Rust in unorthodox
environments, such as those without an operating system, or those that
don’t even have the ability to dynamically allocate memory! Much of our
discussion will focus on the #![no_std] attribute, but we’ll also investigate
212 Chapter 12
Rust’s alloc module, the Rust runtime (yes, Rust does technically have a
runtime), and some of the tricks you have to play to write up a Rust binary
for use in such an environment.
Opting Out of the Standard Library
As a language, Rust consists of multiple independent pieces. First there’s
the compiler, which dictates the grammar of the Rust language and implements
type checking, borrow checking, and the final conversion into
machine-runnable code. Then there’s the standard library, std, which
implements all the useful common functionality that most programs
need—things like file and network access, a notion of time, facilities for
printing and reading user input, and so on. But std itself is also a composite,
building on top of two other, more fundamental libraries called core
and alloc. In fact, many of the types and functions in std are just re-exports
from those two libraries.
The core library sits at the bottom of the standard library pyramid and
contains any functionality that depends on nothing but the Rust language
itself and the hardware the resulting program is running on—things like
sorting algorithms, marker types, fundamental types such as Option and
Result, low-level operations such as atomic memory access methods, and
compiler hints. The core library works as if the operating system does
not exist, so there is no standard input, no filesystem, and no network.
Similarly, there is no memory allocator, so types like Box, Vec, and HashMap
are nowhere to be seen.
Above core sits alloc, which holds all the functionality that depends
on dynamic memory allocation, such as collections, smart pointers, and
dynamically allocated strings (String). We’ll get back to alloc in the next
section.
Most of the time, because std re-exports everything in core and
alloc, developers do not need to know about the differences among the
three libraries. This means that even though Option technically lives in
core::option::Option, you can access it through std::option::Option.
However, in an unorthodox environment, such as on an embedded
device where there is no operating system, the distinction is crucial. While
it’s fine to use an Iterator or to sort a list of numbers, an embedded device
may simply have no meaningful way to access a file (as that requires a filesystem)
or print to the terminal (as that requires a terminal)—so there’s
no File or println!. Furthermore, the device may have so little memory that
dynamic memory allocation is a luxury you can’t afford, and thus anything
that allocates memory on the fly is a no-go—say goodbye to Box and Vec.
Rather than force developers to carefully avoid those basic constructs
in such environments, Rust provides a way to opt out of anything but the
core functionality of the language: the #![no_std] attribute. This is a cratelevel
attribute (#!) that switches the prelude (see the box on page 213) for
the crate from std::prelude to core::prelude so that you don’t accidentally
depend on anything outside of core that might not work in your target
environment.
Rust Without the Standard Library 213
However, that is all the #![no_std] attribute does—it does not prevent
you from bringing in the standard library explicitly with extern std. This
may be surprising, as it means a crate marked #![no_std] may in fact not be
compatible with a target environment that does not support std, but this
design decision was intentional: it allows you to mark your crate as being
no_std-compatible but to still use features from the standard library when
certain features are enabled. For example, many crates have a feature named
std that, when enabled, gives access to more sophisticated APIs and integrations
with types that live in std. This allows crate authors to both supply the
core implementation for constrained use cases and add bells and whistles
for consumers on more standard platforms.
NOTE Since features should be additive, prefer an std-enabling feature to an std-disabling
one. Otherwise, if any crate in a consumer’s dependency graph enables the no-std
feature, all consumers will be given access only to the bare-bones API without std support,
which may then mean that APIs they depend on aren’t available, causing them
to no longer compile.
THE PRELUDE
Have you ever wondered why there are some types and traits—like Box, Iterator,
Option, and Clone—that are available in every Rust file without you needing to
use them? Or why you don’t need to use any of the macros in the standard library
(like vec![])? The reason is that every Rust module automatically imports the Rust
standard prelude with an implicit use std::prelude::rust_2021::* (or similar
for other editions), which brings all the exports from the crate’s chosen edition’s
prelude into scope. The prelude modules themselves aren’t special beyond this
auto-
inclusion—they are merely collections of pub use statements for key types,
traits, and macros that the Rust developers expect to be commonly used.
Dynamic Memory Allocation
As we discussed in Chapter 1, a machine has many different regions of
memory, and each one serves a distinct purpose. There’s static memory for
your program code and static variables, there’s the stack for function-local
variables and function arguments, and there’s the heap for, well, everything
else. The heap supports allocating variably sized regions of memory
at runtime, and those allocations stick around for however long you want
them to. This makes heap memory extremely versatile, and as a result, you
find it used everywhere. Vec, String, Arc and Rc, and the collection types are
all implemented in heap memory, which allows them to grow and shrink
over time and to be returned from functions without the borrow checker
complaining.
214 Chapter 12
Behind the scenes, the heap is really just a huge chunk of contiguous
memory that is managed by an allocator. It’s the allocator that provides the
illusion of distinct allocations in the heap, ensuring that those allocations do
not overlap and that regions of memory that are no longer in use are reused.
By default Rust uses the system allocator, which is generally the one dictated
by the standard C library. This works well for most use cases, but if necessary,
you can override which allocator Rust will use through the GlobalAlloc trait
combined with the #[global_allocator] attribute, which requires an implementation
of an alloc method for allocating a new segment of memory and
dealloc for returning a past allocation to the allocator to reuse.
In environments without an operating system, the standard C library
is also generally not available, and so neither is the standard system allocator.
For that reason, #![no_std] also excludes all types that rely on dynamic
memory allocation. But since it’s entirely possible to implement a memory
allocator without access to a full-blown operating system, Rust allows you
to opt back into just the part of the Rust standard library that requires an
allocator without opting into all of std through the alloc crate. The alloc
crate comes with the standard Rust toolchain (just like core and std) and
contains most of your favorite heap-allocation types, like Box, Arc, String,
Vec, and BTreeMap. HashMap is not among them, since it relies on random number
generation for its key hashing, which is an operating system facility. To
use types from alloc in a no_std context, all you have to do is replace any
imports of those types that previously had use std:: with use alloc:: instead.
Do keep in mind, though, that depending on alloc means your #![no_std]
crate will no longer be usable by any program that disallows dynamic memory
allocation, either because it doesn’t have an allocator or because it has
too little memory to permit dynamic memory allocation in the first place.
NOTE Some programming domains, like the Linux kernel, may allow dynamic memory
allocation only if out-of-memory errors are handled gracefully (that is, without panicking).
For such use cases, you’ll want to provide try_ versions of any methods you
expose that might allocate. The try_ methods should use fallible methods of any inner
types (like the currently unstable Box::try_new or Vec::try_reserve) rather than ones
that just panic (like Box::new or Vec::reserve) and propagate those errors out to the
caller, who can then handle them appropriately.
It might strike you as odd that it’s possible to write nontrivial crates that
use only core. After all, they can’t use collections, the String type, the network,
or the filesystem, and they don’t even have a notion of time! The trick
to core-only crates is to utilize the stack and static allocations. For example,
for a heapless vector, you allocate enough memory up front—either in static
memory or in a function’s stack frame—for the largest number of elements
you expect the vector to be able to hold, and then augment it with a usize
that tracks how many elements it currently holds. To push to the vector, you
write to the next element in the (statically sized) array and increment a variable
that tracks the number of elements. If the vector’s length ever reaches
the static size, the next push fails. Listing 12-1 gives an example of such a
heapless vector type implemented using const generics.
Rust Without the Standard Library 215
struct ArrayVec<T, const N: usize> {
values: [Option<T>; N],
len: usize,
}
impl<T, const N: usize> ArrayVec<T, N> {
fn try_push(&mut self, t: T) -> Result<(), T> {
if self.len == N {
return Err(t);
}
self.values[self.len] = Some(t);
self.len += 1;
return Ok(());
}
}
Listing 12-1: A heapless vector type
We make ArrayVec generic over both the type of its elements, T, and the
maximum number of elements, N, and then represent the vector as an array
of N optional Ts. This structure always stores N Option<T>, so it has a size known
at compile time and can be stored on the stack, but it can still act like a vector
by using runtime information to inform how we access the array.
NOTE We could have implemented ArrayVec using [MaybeUninit<T>; N] to avoid the overhead
of the Option, but that would require using unsafe code, which isn’t warranted
for this example.
The Rust Runtime
You may have heard the claim that Rust doesn’t have a runtime. While
that’s true at a high level—it doesn’t have a garbage collector, an interpreter,
or a built-in user-level scheduler—it’s not really true in the strictest
sense. Specifically, Rust does have some special code that runs before your
main function and in response to certain special conditions in your code,
which really is a form of bare-bones runtime.
The Panic Handler
The first bit of such special code is Rust’s panic handler. When Rust code
panics by invoking panic! or panic_any, the panic handler dictates what
happens next. When the Rust runtime is available—as is the case on most
targets that supply std—the panic handler first invokes the panic hook set
via std::panic::set_hook, which prints a message and optionally a backtrace
to standard error by default. It then either unwinds the current thread’s
stack or aborts the process, depending on the panic setting chosen for current
compilation (either through Cargo configuration or arguments passed
directly to rustc).
However, not all targets provide a panic handler. For example, most
embedded targets do not, as there isn’t necessarily a single implementation
that makes sense across all the uses for such a target. For targets that don’t
216 Chapter 12
supply a panic handler, Rust still needs to know what to do when a panic
occurs. To that end, we can use the #[panic_handler] attribute to decorate a
single function in the program with the signature fn(&PanicInfo) -> !. That
function is called whenever the program invokes a panic, and it is passed
information about the panic in the form of a core::panic::PanicInfo. What
the function does with that information is entirely unspecified, but it can
never return (as indicated by the ! return type). This is important, since the
Rust compiler assumes that no code that follows a panic is run.
There are many valid ways for a panic handler to avoid returning. The
standard panic handler unwinds the thread’s stack and then terminates the
thread, but a panic handler can also halt the thread using loop {}, abort the
program, or do anything else that makes sense for the target platform, even
as far as resetting the device.
Program Initialization
Contrary to popular belief, the main function is not the first thing that runs
in a Rust program. Instead, the main symbol in a Rust binary actually points
to a function in the standard library called lang_start. That function performs
the (fairly minimal) setup for the Rust runtime, including stashing
the program’s command-line arguments in a place where std::env::args can
get to them, setting the name of the main thread, handling panics in the
main function, flushing standard output on program exit, and setting up signal
handlers. The lang_start function in turn calls the main function defined
in your crate, which then doesn’t need to think about how, for example,
Windows and Linux differ in how command-line arguments are passed in.
This arrangement works well on platforms where all of that setup is sensible
and supported, but it presents a problem on embedded platforms where
main memory may not even be accessible when the program starts. On such
platforms, you’ll generally want to opt out of the Rust initialization code
entirely using the #![no_main] crate-level attribute. This attribute completely
omits lang_start, meaning you as the developer must figure out how the program
should be started, such as by declaring a function with #[export_name =
"main"] that matches the expected launch sequence for the target platform.
NOTE On platforms that truly run no code before they jump to the defined start symbol,
like most embedded devices, the initial values of static variables may not even match
what’s specified in the source code. In such cases, your initialization function will
need to explicitly initialize the various static memory segments with the initial data
values specified in your program binary.
The Out-of-Memory Handler
If you write a program that wishes to use alloc but is built for a platform
that does not supply an allocator, you must dictate which allocator to use
using the #[global_allocator] attribute mentioned earlier in the chapter.
But you also have to specify what happens if that global allocator fails
Rust Without the Standard Library 217
to allocate memory. Specifically, you need to define an out-of-memory handler
to say what should happen if an infallible operation like Vec::push needs to
allocate more memory, but the allocator cannot supply it.
The default behavior of the out-of-memory handler on std-enabled
platforms is to print an error message to standard error and then abort the
process. However, on a platform that, for example, doesn’t have standard
error, that obviously won’t work. At the time of writing, on such platforms
your program must explicitly define an out-of-memory handler using the
unstable attribute #[lang = "oom"]. Keep in mind that the handler should
almost certainly prevent future execution, as otherwise the code that tried
to allocate will continue executing without knowing that it did not receive
the memory it asked for!
NOTE By the time you read this, the out-of-memory handler may already have been stabilized
under a permanent name (#[alloc_error_handler], most likely). Work is also underway
to give the default std out-of-memory handler the same kind of “hook” functionality
as Rust’s panic handler, so that code can change the out-of-memory behavior on
the fly through a method like set_alloc_error_hook.
Low-Level Memory Accesses
In Chapter 10, we discussed the fact that the compiler is given a fair amount
of leeway in how it turns your program statements into machine instructions,
and that the CPU is allowed some wiggle room to execute instructions
out of order. Normally, the shortcuts and optimizations that the
compiler and CPU can take advantage of are invisible to the semantics of
the program—you can’t generally tell whether, say, two reads have been
reordered relative to each other or whether two reads from the same memory
location actually result in two CPU load instructions. This is by design.
The language and hardware designers carefully specified what semantics
programmers commonly expect from their code when it runs so that your
code generally does what you expect it to.
However, no_std programming sometimes takes you beyond the usual
border of “invisible optimizations.” In particular, you’ll often communicate
with hardware devices through memory mapping, where the internal state
of the device is made available in carefully chosen regions in memory. For
example, while your computer starts up, the memory address range 0xA0000–
0xBFFFF maps to a crude graphics rendering pipeline; writes to individual
bytes in that range will change particular pixels (or blocks, depending on
the mode) on the screen.
When you’re interacting with device-mapped memory, the device
may implement custom behavior for each memory access to that region of
memory, so the assumptions your CPU and compiler make about regular
memory loads and stores may no longer hold. For instance, it is common
for hardware devices to have memory-mapped registers that are modified
218 Chapter 12
when they’re read, meaning the reads have side effects. In such cases, the
compiler can’t safely elide a memory store operation if you read the same
memory address twice in a row!
A similar issue arises when program execution is suddenly diverted in
ways that aren’t represented in the code and thus that the compiler cannot
expect. Execution might be diverted if there is no underlying operating
system to handle processor exceptions or interrupts, or if a process
receives a signal that interrupts execution. In those cases, the execution
of the active segment of code is stopped, and the CPU starts executing
instructions in the event handler for whatever event triggered the diversion
instead. Normally, since the compiler can anticipate all possible executions,
it arranges its optimizations so that executions cannot observe when operations
have been performed out of order or optimized away. However, since
the compiler can’t predict these exceptional jumps, it also cannot plan for
them to be oblivious to its optimizations, so these event handlers might
actually observe instructions that have run in a different order than those
in the original program code.
To deal with these exceptional situations, Rust provides volatile memory
operations that cannot be elided or reordered with respect to other volatile
operations. These operations take the form of std::ptr::read_volatile
and std::ptr::write_volatile. Volatile operations are exactly the right fit
for accessing memory-mapped hardware resources: they map directly to
memory access operations with no compiler trickery, and the guarantee
that volatile operations aren’t reordered relative to one another ensures
that hardware operations with possible side effects don’t happen out of
order even when they would normally look interchangeable (such as a load
of one address and a store to a different address). The no-reordering guarantee
also helps the exceptional execution situation, as long as any code
that touches memory accessed in an exceptional context uses only volatile
memory operations.
NOTE There is also a std::sync::atomic::compiler_fence function that prevents the compiler
from reordering non-volatile memory accesses. You’ll very rarely need a compiler
fence, but its documentation is an interesting read.
INCLUDING ASSEMBLY CODE
These days, you rarely need to drop down to writing assembly code to accomplish
any given task. But for low-level hardware programming where you need
to initialize CPUs at boot or issue strange instructions to manipulate memory
mappings, assembly code is still sometimes required. At the time of writing,
there is an RFC and a mostly complete implementation of inline assembly syntax
on nightly Rust, but nothing has been stabilized yet, so I won’t discuss the syntax
in this book.
Rust Without the Standard Library 219
It’s still possible to write assembly on stable Rust—you just need to get a little
creative. Specifically, remember build scripts from Chapter 11? Well, Cargo
build scripts can emit certain special directives to standard output to augment
Cargo’s standard build process, including cargo:rustc-link-lib=static=xyz
to link the static library file libxyz.a into the final binary, and cargo:rustc-linksearch:/
some/path to add /some/path to the search path for link objects.
Using those, we can add a build.rs to the project that compiles a standalone
assembly file (.s) to an object file (.o) using the target platform’s compiler and
then repackages it into a static archive (.a) using the appropriate archiving
tool (usually ar). The project then emits those two Cargo directives, pointing at
where it placed the static archive—probably in OUT_DIR—and we’re off to the
races! If the target platform doesn’t change, you can even include the precompiled
.a when publishing your crate so that consumers don’t need to rebuild it.
Misuse-Resistant Hardware Abstraction
Rust’s type system excels at encapsulating unsafe, hairy, and otherwise
unpleasant code behind safe, ergonomic interfaces. Nowhere is that
more important than in the infamously complex world of low-level systems
programming, littered with magic hardware-defined values pulled
from obscure manuals and mysterious undocumented assembly instruction
incantations to get devices into just the right state. And all that in a
space where a runtime error might crash more than just a user program!
In no_std programs, it is immensely important to use the type system to
make illegal states impossible to represent, as we discussed in Chapter 3. If
certain combinations of register values cannot occur at the same time, then
create a single type whose type parameters indicate the current state of the
relevant registers, and implement only legal transitions on it, like we did for
the rocket example in Listing 3-2.
NOTE Make sure to also review the advice from Chapter 3 on API design—all of that
applies in the context of no_std programs as well!
For example, consider a pair of registers where at most one register
should be “on” at any given point in time. Listing 12-2 shows how you can
represent that in a (single-threaded) program in a way makes it impossible
to write code that violates that invariant.
// raw register address -- private submodule
mod registers;
pub struct On;
pub struct Off;
pub struct Pair<R1, R2>(PhantomData<(R1, R2)>);
impl Pair<Off, Off> {
pub fn get() -> Option<Self> {
220 Chapter 12
static mut PAIR_TAKEN: bool = false;
if unsafe { PAIR_TAKEN } {
None
} else {
// Ensure initial state is correct.
registers::off("r1");
registers::off("r2");
unsafe { PAIR_TAKEN = true };
Some(Pair(PhantomData))
}
}
pub fn first_on(self) -> Pair<On, Off> {
registers::set_on("r1");
Pair(PhantomData)
}
// .. and inverse for -> Pair<Off, On>
}
impl Pair<On, Off> {
pub fn off(self) -> Pair<Off, Off> {
registers::set_off("r1");
Pair(PhantomData)
}
}
// .. and inverse for Pair<Off, On>
Listing 12-2: Statically ensuring correct operation
There are a few noteworthy patterns in this code. The first is that we
ensure only a single instance of Pair ever exists by checking a private static
Boolean in its only constructor and making all methods consume self. We
then ensure that the initial state is valid and that only valid state transitions
are possible to express, and therefore the invariant must hold globally.
The second noteworthy pattern in Listing 12-2 is that we use PhantomData
to take advantage of zero-sized types and represent runtime information
statically. That is, at any given point in the code the types tell us what the
runtime state must be, and therefore we don’t need to track or check any
state related to the registers at runtime. There’s no need to check that r2
isn’t already on when we’re asked to enable r1, since the types prevent writing
a program in which that is the case.
Cross-Compilation
Usually, you’ll write no_std programs on a computer with a full-fledged
operating system running and all the niceties of modern hardware, but ultimately
run it on a dinky hardware device with 93/4 bits of RAM and a sock
for a CPU. That calls for cross-compilation—you need to compile the code in
your development environment, but compile it for the sock. That’s not the
only context in which cross-compilation is important, though. For example,
it’s increasingly common to have one build pipeline produce binary
Rust Without the Standard Library 221
artifacts for all consumer platforms rather than trying to have a build pipeline
for every platform your consumers may be using, and that means using
cross-compilation.
NOTE If you’re actually compiling for something sock-like with limited memory, or even
something as fancy as a potato, you may want to set the opt-level Cargo configuration
to "s" to optimize for smaller binary sizes.
Cross-compiling involves two platforms: the host platform and the target
platform. The host platform is the one doing the compiling, and the target
platform is the one that will eventually run the output of the compilation.
We specify platforms as target triples, which take the form machine-vendor-
os.
The machine part dictates the machine architecture the code will run on,
such as x86_64, armv7, or wasm32, and tells the compiler what instruction set to
use for the emitted machine code. The vendor part generally takes the value
of pc on Windows, apple on macOS and iOS, and unknown everywhere else,
and doesn’t affect compilation in any meaningful way; it’s mostly irrelevant
and can even be left out. The os part tells the compiler what format to use
for the final binary artifacts, so a value of linux dictates Linux .so files, windows
dictates Windows .dll files, and so on.
NOTE By default, Cargo assumes that the target platform is the same as the host platform,
which is why you generally never have to tell Cargo to, say, compile for Linux when
you’re already on Linux. Sometimes you may want to use --target even if the CPU
and OS of the target are the same, though, such as to target the musl implementation
of libc.
To tell Cargo to cross-compile, you simply pass it the --target <target
triple>
argument with your triple of choice. Cargo will then take care of
forwarding that information to the Rust compiler so that it generates binary
artifacts that will work on the given target platform. Cargo will also take care
to use the appropriate version of the standard library for that platform—after
all, the standard library contains a lot of conditional compilation directives
(using #[cfg(...)]) so that the right system calls get invoked and the right
architecture-specific implementations are used, so we can’t use the standard
library for the host platform on the target.
The target platform also dictates what components of the standard
library are available. For example, while x86_64-unknown-linux-gnu includes
the full std library, something like thumbv7m-none-eabi does no, and doesn’t
even define an allocator, so if you use alloc without defining one explicitly,
you’ll get a build error. This comes in handy for testing that code you write
actually doesn’t require std (recall that even with #![no_std] you can still have
use std::, since no_std opts out of only the std prelude). If you have your continuous
integration pipeline build your crate with --target thumbv7m-none-eabi,
any attempt to access components from anything but core will trigger a build
failure. Crucially, this will also check that your crate doesn’t accidentally
bring in dependencies that themselves use items from std (or alloc).
222 Chapter 12
PLATFORM SUPPORT
The standard Rust installer, Rustup, doesn’t install the standard library for all the
target triples that Rust supports by default. That would be a waste of space and
bandwidth. Instead, you have to use the command rustup target add to install
the appropriate standard library versions for additional targets. If no version of
the standard library exists for your target platform, you’ll have to compile it from
source yourself by adding the rust-src Rustup component and using Cargo’s
(currently unstable) build-std feature to also build std (and/or core and alloc)
when building any crate.
If your target is not supported by the Rust compiler—that is, if rustc doesn’t
even know about your target triple—you’ll have to go one step further and teach
rustc about the properties of the triple using a custom target specification. How
you do that is both currently unstable and beyond the scope of this book, but a
search for “custom target specification json” is a good place to start.
Summary
In this chapter, we’ve covered what lies beneath the standard library—or,
more precisely, beneath std. We’ve gone over what you get with core, how
you can extend your non-std reach with alloc, and what the (tiny) Rust runtime
adds to your programs to make fn main work. We’ve also taken a look
at how you can interact with device-mapped memory and otherwise handle
the unorthodox execution patterns that can happen at the very lowest level
of hardware programming, and how to safely encapsulate at least some of
the oddities of hardware in the Rust type system. Next, we’ll move from the
very small to the very large by discussing how to navigate, understand, and
maybe even contribute to the larger Rust ecosystem.

### 13 THE RUST ECOSYSTEM

Programming rarely happens in a vacuum
these days—nearly every Rust crate you
build is likely to take dependencies on some
code that wasn’t written by you. Whether this
trend is good, bad, or a little of both is a subject of
heavy debate, but either way, it’s a reality of today’s
developer experience.
In this brave new interdependent world, it’s more important than ever
to have a solid grasp of what libraries and tools are available and to stay up
to date on the latest and greatest of what the Rust community has to offer.
This chapter is dedicated to how you can leverage, track, understand, and
contribute back to the Rust ecosystem. Since this is the final chapter, in the
closing section I’ll also provide some suggestions of additional resources
you can explore to continue developing your Rust skills.
224 Chapter 13
What’s Out There?
Despite its relative youth, Rust already has an ecosystem large enough that
it’s hard to keep track of everything that’s available. If you know what you
want, you may be able to search your way to a set of appropriate crates and
then use download statistics and superficial vibe-checks on each crate’s
repository to determine which may make for reasonable dependencies.
However, there’s also a plethora of tools, crates, and general language features
that you might not necessarily know to look for that could potentially
save you countless hours and difficult design decisions.
In this section, I’ll go through some of the tools, libraries, and Rust features
I have found helpful over the years in the hopes that they may come
in useful for you at some point too!
Tools
First off, here are some Rust tools I find myself using regularly that you
should add to your toolbelt:
cargo-deny
Provides a way to lint your dependency graph. At the time of writing,
you can use cargo-deny to allow only certain licenses, deny-list crates or
specific crate versions, detect dependencies with known vulnerabilities
or that use Git sources, and detect crates that appear multiple times
with different versions in the dependency graph. By the time you’re
reading this, there may be even more handy lints in place.
cargo-expand
Expands macros in a given crate and lets you inspect the output, which
makes it much easier to spot mistakes deep down in macro transcribers
or procedural macros. cargo-expand is an invaluable tool when you’re
writing your own macros.
cargo-hack
Helps you check that your crate works with any combination of features
enabled. The tool presents an interface similar to that of Cargo itself
(like cargo check, build, and test) but gives you the ability to run a given
command with all possible combinations (the powerset) of the crate’s
features.
cargo-llvm-lines
Analyzes the mapping from Rust code to the intermediate representation
(IR) that’s passed to the part of the Rust compiler that actually
generates machine code (LLVM), and tells you which bits of Rust code
produce the largest IR. This is useful because a larger IR means longer
compile times, so identifying what Rust code generates a bigger IR (due
The Rust Ecosystem 225
to, for example, monomorphization) can highlight opportunities for
reducing compile times.
cargo-outdated
Checks whether any of your dependencies, either direct or transitive,
have newer versions available. Crucially, unlike cargo update, it even
tells you about new major versions, so it’s an essential tool for checking
if you’re missing out on newer versions due to an outdated major
version specifier. Just keep in mind that bumping the major version of
a dependency may be a breaking change for your crate if you expose
that dependency’s types in your interface!
cargo-udeps
Identifies any dependencies listed in your Cargo.toml that are never actually
used. Maybe you used them in the past but they’ve since become
redundant, or maybe they should be moved to dev-dependencies; whatever
the case, this tool helps you trim down bloat in your dependency
closure.
While they’re not specifically tools for developing Rust, I highly recommend
fd and ripgrep too—they’re excellent improvements over their predecessors
find and grep and also happen to be written in Rust themselves. I use
both every day.
Libraries
Next up are some useful but lesser-known crates that I reach for regularly,
and that I suspect I will continue to depend on for a long time:
bytes
Provides an efficient mechanism for passing around subslices of a single
piece of contiguous memory without having to copy or deal with lifetimes.
This is great in low-level networking code where you may need
multiple views into a single chunk of bytes, and copying is a no-no.
criterion
A statistics-driven benchmarking library that uses math to eliminate
noise from benchmark measurements and reliably detect changes in
performance over time. You should almost certainly be using it if you’re
including micro-benchmarks in your crate.
cxx
Provides a safe and ergonomic mechanism for calling C++ code from
Rust and Rust code from C++. If you’re willing to invest some time into
declaring your interfaces more thoroughly in advance in exchange for
much nicer cross-language compatibility, this library is well worth your
attention.
226 Chapter 13
flume
Implements a multi-producer, multi-consumer channel that is faster,
more flexible, and simpler than the one included with the Rust standard
library. It also supports both asynchronous and synchronous operation
and so is a great bridge between those two worlds.
hdrhistogram
A Rust port of the High Dynamic Range (HDR) histogram data structure,
which provides a compact representation of histograms across a
wide range of values. Anywhere you currently track averages or min/
max values, you should most likely be using an HDR histogram instead;
it can give you much better insight into the distribution of your metrics.
heapless
Supplies data structures that do not use the heap. Instead, heapless’s
data structures are all backed by static memory, which makes them
perfect for embedded contexts or other situations in which allocation is
undesirable.
itertools
Extends the Iterator trait from the standard library with lots of new
convenient methods for deduplication, grouping, and computing powersets.
These extension methods can significantly reduce boilerplate in
code, such as where you manually implement some common algorithm
over a sequence of values, like finding the min and max at the same
time (Itertools::minmax), or where you use a common pattern like checking
that an iterator has exactly one item (Itertools::exactly_one).
nix
Provides idiomatic bindings to system calls on Unix-like systems,
which allows for a much better experience than trying to cobble
together the C-compatible FFI types yourself when working with
something like libc directly.
pin-project
Provides macros that enforce the pinning safety invariants for annotated
types, which in turn provide a safe pinning interface to those
types. This allows you to avoid most of the hassle of getting Pin and
Unpin right for your own types. There’s also pin-project-lite, which
avoids the (currently) somewhat heavy dependency on the procedural
macro machinery at the cost of slightly worse ergonomics.
ring
Takes the good parts from the cryptography library BoringSSL,
written in C, and brings them to Rust through a fast, simple, and
The Rust Ecosystem 227
hard-to-misuse interface. It’s a great starting point if you need to use
cryptography in your crate. You’ve already most likely come across this
in the rustls library, which uses ring to provide a modern, secure-bydefault
TLS stack.
slab
Implements an efficient data structure to use in place of HashMap<Token, T>,
where Token is an opaque type used only to differentiate between entries in
the map. This kind of pattern comes up a lot when managing resources,
where the set of current resources must be managed centrally but individual
resources must also be accessible somehow.
static_assertions
Provides static assertions—that is, assertions that are evaluated at, and
thus may fail at, compile time. You can use it to assert things like that
a type implements a given trait (like Send) or is of a given size. I highly
recommend adding these kinds of assertions for code where those
guarantees are likely to be important.
structopt
Wraps the well-known argument parsing library clap and provides a way
to describe your application’s command line interface entirely using the
Rust type system (plus macro annotations). When you parse your application’s
arguments, you get a value of the type you defined, and you
thus get all the type checking benefits, like exhaustive matching and
IDE auto-complete.
thiserror
Makes writing custom enumerated error types, like the ones we discussed
in Chapter 4, a joy. It takes care of implementing the recommended traits
and following the established conventions and leaves you to define just
the critical bits that are unique to your application.
tower
Effectively takes the function signature async fn(Request) -> Response and
implements an entire ecosystem on top of it. At its core is the Service
trait, which represents a type that can turn a request into a response
(something I suspect may make its way into the standard library one
day). This is a great abstraction to build anything that looks like a service
on top of.
tracing
Provides all the plumbing needed to efficiently trace the execution of
your applications. Crucially, it is agnostic to the types of events you’re
tracing and what you want to do with those events. This library can be
228 Chapter 13
used for logging, metrics collection, debugging, profiling, and obviously
tracing, all with the same machinery and interfaces.
Rust Tooling
The Rust toolchain has a few features up its sleeve that you may not know
to look for. These are usually for very specific use cases, but if they match
yours, they can be lifesavers!
Rustup
Rustup, the Rust toolchain installer, does its job so efficiently that it tends
to fade into the background and get forgotten about. You’ll occasionally
use it to update your toolchain, set a directory override, or install a component,
but that’s about it. However, Rustup supports one very handy trick
that it’s worthwhile to know about: the toolchain override shorthand. You
can pass +toolchain as the first argument to any Rustup-managed binary,
and the binary will work as if you’d set an override for the given toolchain,
run the command, and then reset the override back to what it was previously.
So, cargo +nightly miri will run Miri using the nightly toolchain, and
cargo +1.53.0 check will check if the code compiles with Rust 1.53.0. The latter
comes in particularly handy for checking that you haven’t broken your
minimum supported Rust version contract.
Rustup also has a neat subcommand, doc, that opens a local copy of the
Rust standard library documentation for the current version of the Rust
compiler in your browser. This is invaluable if you’re developing on the go
without an internet connection!
Cargo
Cargo also has some handy features that aren’t always easy to discover.
The first of these is cargo tree, a Cargo subcommand built right into Cargo
itself for inspecting a crate’s dependency graph. This command’s primary
purpose is to print the dependency graph as a tree. This can be useful on
its own, but where cargo tree really shines is through the --invert option:
it takes a crate identifier and produces an inverted tree showing all the
dependency paths from the current crate that bring in that dependency.
So, for example, cargo tree -i rand will print all of the ways in which the
current crate depends on any version of rand, including through transitive
dependencies. This is invaluable if you want to eliminate a dependency, or
a particular version of a dependency, and wonder why it still keeps being
pulled in. You can also pass the -e features option to include information
about why each Cargo feature of the crate in question is enabled.
Speaking of Cargo subcommands, it’s really easy to write your own,
whether for sharing with other people or just for your own local development.
When Cargo is invoked with a subcommand it doesn’t recognize, it
checks whether a program by the name cargo-$subcommand exists. If it does,
Cargo invokes that program and passes it any arguments that were passed
The Rust Ecosystem 229
on the command line—so, cargo foo bar will invoke cargo-foo with the argument
bar. Cargo will even integrate this command with cargo help by translating
cargo help foo into a call to cargo-foo --help.
As you work on more Rust projects, you may notice that Cargo (and
Rust more generally) isn’t exactly forgiving when it comes to disk space.
Each project gets its own target directory for its compilation artifacts, and
over time you end up accumulating several identical copies of compiled
artifacts for common dependencies. Keeping artifacts for each project separate
is a sensible choice, as they aren’t necessarily compatible across projects
(say, if one project uses different compiler flags than another). But in most
developer environments, sharing build artifacts is entirely reasonable and
can save a fair amount of compilation time when switching between projects.
Luckily, configuring Cargo to share build artifacts is simple: just set
[build] target in your ~/.cargo/config.toml file to the directory you want those
shared artifacts to go in, and Cargo will take care of the rest. No more target
directories in sight! Just make sure you clean out that directory every
now and again too, and be aware that cargo clean will now clean all of your
projects’ build artifacts.
NOTE Using a shared build directory can cause problems for projects that assume that compiler
artifacts will always be under the target/ subdirectory, so watch out for that.
Also note that if a project does use different compiler flags, you’ll end up recompiling
affected dependencies every time you move into or out of that project. In such cases,
you’re best off overriding the target directory in that project’s Cargo configuration to a
distinct location.
Finally, if you ever feel like Cargo is taking a suspiciously long time to
build your crate, you can reach for the currently unstable Cargo -Ztimings
flag. Running Cargo with that flag outputs information about how long it
took to process each crate, how long build scripts took to run, what crates
had to wait for what other crates to finish compiling, and tons of other
useful metrics. This might highlight a particularly slow dependency chain
that you can then work to eliminate, or reveal a build script that compiles
a native dependency from scratch that you can make use system libraries
instead. If you want to dive even deeper, there’s also rustc -Ztime-passes,
which emits information about where time is spent inside of the compiler
for each crate—though that information is likely only useful if you’re looking
to contribute to the compiler itself.
rustc
The Rust compiler also has some lesser-known features that can prove useful
to enterprising developers. The first is the currently unstable -Zprinttype-
sizes argument, which prints the sizes of all the types in the current
crate. This produces a lot of information for all but the tiniest crates but
is immensely valuable when trying to determine the source of unexpected
time spent in calls to memcpy or to find ways to reduce memory use when allocating
lots of objects of a particular type. The -Zprint-type-sizes argument
230 Chapter 13
also displays the computed alignment and layout for each type, which may
point you to places where turning, say, a usize into a u32 could have a significant
impact on a type’s in-memory representation. After you debug a
particular type’s size, alignment, and layout, I recommend adding static
assertions to make sure that they don’t regress over time. You may also be
interested in the variant_size_differences lint, which issues a warning if a
crate contains enum types whose variants significantly differ in size.
NOTE To call rustc with particular flags, you have a few options: you can either set them in
the RUSTFLAGS environment variable or [build] rustflags in your .cargo/config.toml
to have them apply to every invocation of rustc from Cargo, or you can use cargo
rustc, which will pass any arguments you provide only to the rustc invocation for the
current crate.
If your profiling samples look weird, with stack frames reordered or
entirely missing, you could also try -Cforce-frame-pointers = yes. Frame pointers
provide a more reliable way to unwind the stack—which is done a lot
during profiling—at the cost of an extra register being used for function
calls. Even though stack unwinding should work fine with just regular debug
symbols enabled (remember to set debug = true when using the release profile),
that’s not always the case, and frame pointers may take care of any
issues you do encounter.
The Standard Library
The Rust standard library is generally considered to be small compared
to those of other programming languages, but what it lacks in breadth, it
makes up for in depth; you won’t find a web server implementation or an
X.509 certificate parser in Rust’s standard library, but you will find more
than 40 different methods on the Option type alongside over 20 trait implementations.
For the types it does include, Rust does its best to make available
any relevant functionality that meaningfully improves ergonomics, so
you avoid all that verbose boilerplate that can so easily arise otherwise. In
this section, I’ll present some types, macros, functions, and methods from
the standard library that you may not have come across before, but that can
often simplify or improve (or both) your code.
Macros and Functions
Let’s start off with a few free-standing utilities. First up is the write! macro,
which lets you use format strings to write into a file, a network socket, or anything
else that implements Write. You may already be familiar with it—but
one little-known feature of write! is that it works with both std::io::Write and
std::fmt::Write, which means you can use it to write formatted text directly into
a String. That is, you can write use std::fmt::Write; write!(&mut s, "{}+1={}", x,
x + 1); to append the formatted text to the String s!
The iter::once function takes any value and produces an iterator that
yields that value once. This comes in handy when calling functions that take
The Rust Ecosystem 231
iterators if you don’t want to allocate, or when combined with Iterator::chain
to append a single item to an existing iterator.
We briefly talked about mem::replace in Chapter 1, but it’s worth bringing
up again in case you missed it. This function takes an exclusive reference
to a T and an owned T, swaps the two so that the referent is now the
owned T, and returns ownership of the previous referent. This is useful
when you need to take ownership of a value in a situation where you have
only an exclusive reference, such as in implementations of Drop. See also
mem::take for when T: Default.
Types
Next, let’s look at some handy standard library types. The BufReader and
BufWriter types are a must for I/O operations that issue many small read or
write calls to the underlying I/O resource. These types wrap the respective
underlying Read or Write and implement Read and Write themselves, but
they additionally buffer the operations to the I/O resource such that many
small reads do only one large read, and many small writes do only one large
write. This can significantly improve performance as you don’t have to cross
the system call barrier into the operating system as often.
The Cow type, mentioned in Chapter 3, is useful when you want flexibility
in what types you hold or need flexibility in what you return. You’ll
rarely use Cow as a function argument (recall that you should let the caller
allocate if necessary), but it’s invaluable as a return type as it allows you
to accurately represent the return types of functions that may or may not
allocate. It’s also a perfect fit for types that can be used as inputs or outputs,
such as core types in RPC-like APIs. Say we have a type EntityIdentifier like
in Listing 13-1 that is used in an RPC service interface.
struct EntityIdentifier {
namespace: String,
name: String,
}
Listing 13-1: A representation of a combined input/output type that requires allocation
Now imagine two methods: get_entity takes an EntityIdentifier as an
argument, and find_by returns an EntityIdentifier based on some search
parameters. The get_entity method requires only a reference since the
identifier will (presumably) be serialized before being sent to the server.
But for find_by, the entity will be deserialized from the server response and
must therefore be represented as an owned value. If we make get_entity
take &EntityIdentifier, it will mean callers must still allocate owned Strings
to call get_entity even though that’s not required by the interface, since
it’s required to construct an EntityIdentifier in the first place! We could
instead introduce a separate type for get_entity, EntityIdenifierRef, that
holds only &str types, but then we’d have two types to represent one thing.
Cow to the rescue! Listing 13-2 shows an EntityIdentifier that instead holds
Cows internally.
232 Chapter 13
struct EntityIdentifier<'a> {
namespace: Cow<'a, str>,
name: Cow<'a str>,
}
Listing 13-2: A representation of a combined input/output type that does not require
allocation
With this construction, get_entity can take any EntityIdentifier<'_>,
which allows the caller to use just references to call the method. And find_
by can return EntityIdentifier<'static>, where all the fields are Cow::Owned.
One type shared across both interfaces, with no unnecessary allocation
requirements!
NOTE If you implement a type this way, I recommend you also provide an into_owned
method that turns an <'a> instance into a <'static> instance by calling Cow::into_
owned on all the fields. Otherwise, users will have no way to make longer-lasting
clones of your type when all they have is an <'a>.
The std::sync::Once type is a synchronization primitive that lets you run
a given piece of code exactly once, at initialization time. This is great for
initialization that’s part of an FFI where the library on the other side of the
FFI boundary requires that the initialization is performed only once.
The VecDeque type is an oft-neglected member of std::collections that
I find myself reaching for surprisingly often—basically, whenever I need
a stack or a queue. Its interface is similar to that of a Vec, and like Vec its
in-memory representation is a single chunk of memory. The difference is
that VecDeque keeps track of both the start and end of the actual data in that
single allocation. This allows constant-time push and pop from either side of
the VecDeque, meaning it can be used as a stack, as a queue, or even both at
the same time. The cost you pay is that the values are no longer necessarily
contiguous in memory (they may have wrapped around), which means that
VecDeque<T> does not implement AsRef<[T]>.
Methods
Let’s round off with a rapid-fire look at some neat methods. First up is
Arc::make_mut, which takes a &mut Arc<T> and gives you a &mut T. If the Arc is
the last one in existence, it gives you the T that was behind the Arc; otherwise,
it allocates a new Arc<T> that holds a clone of the T, swaps that in for
the currently referenced Arc, and then gives &mut to the T in the new singleton
Arc.
The Clone::clone_from method is an alternative form of .clone() that
lets you reuse an instance of the type you clone rather than allocate a new
one. In other words, if you already have an x: T, you can do x.clone_from(y)
rather than x = y.clone(), and you might save yourself some allocations.
std::fmt::Formatter::debug_* is by far the easiest way to implement Debug
yourself if #[derive(Debug)] won’t work for your use case, such as if you want
to include only some fields or expose information that isn’t exposed by the
The Rust Ecosystem 233
Debug implementations of your type’s fields. When implementing the fmt
method of Debug, simply call the appropriate debug_ method on the Formatter
that’s passed in (debug_struct or debug_map, for example), call the included
methods on the resulting type to fill in details about the type (like field to
add a field or entries to add a key/value entry), and then call finish.
Instant::elapsed returns the Duration since an Instant was created. This
is much more concise than the common approach of creating a new Instant
and subtracting the earlier instance.
Option::as_deref takes an Option<P> where P: Deref and returns
Option<&P::Target> (there’s also an as_deref_mut method). This simple operation
can make functional transformation chains that operate on Option
much cleaner by avoiding the inscrutable .as_ref().map(|r| &**r).
Ord::clamp lets you take any type that implements Ord and clamp it
between two other values of a given range. That is, given a lower limit min
and an upper limit max, x.clamp(min, max) returns min if x is less than min, max
if x is greater than max, and x otherwise.
Result::transpose and its counterpart Option::transpose invert types that
nest Result and Option. That is, transposing a Result<Option<T>, E> gives an
Option<Result<T, E>>, and vice versa. When combined with ?, this operation
can make for cleaner code when working with Iterator::next and similar
methods in fallible contexts.
Vec::swap_remove is Vec::remove’s faster twin. Vec::remove preserves the
order of the vector, which means that to remove an element in the middle,
it must shift all the later elements in the vector down by one. This can be
very slow for large vectors. Vec::swap_remove, on the other hand, swaps the
to-be-removed element with the last element and then truncates the vector’s
length by one, which is a constant-time operation. Be aware, though, that it
will shuffle your vector around and thus invalidate old indexes!
Patterns in the Wild
As you start exploring codebases that aren’t your own, you’ll likely come
across a couple of common Rust patterns that we haven’t discussed in the
book so far. Knowing about them will make it easier to recognize them, and
thus understand their purpose, when you do encounter them. You may even
find use for them in your own codebase one day!
Index Pointers
Index pointers allow you to store multiple references to data within a data
structure without running afoul of the borrow checker. For example, if you
want to store a collection of data so that it can be efficiently accessed in
more than one way, such as by keeping one HashMap keyed by one field and
one keyed by a different field, you don’t want to store the underlying data
multiple times too. You could use Arc or Rc, but they use dynamic reference
counting that introduces unnecessary overhead, and the extra bookkeeping
requires you to store additional bytes per entry. You could use references,
but the lifetimes become difficult if not impossible to manage because the
234 Chapter 13
data and the references live in the same data structure (it’s a self-referential
data structure, as we discussed in Chapter 8). You could use raw pointers
combined with Pin to ensure the pointers remain valid, but that introduces
a lot of complexity as well as unsafety you then need to carefully consider.
Most crates use index pointers—or, as I like to call them, indeferences—
instead. The idea is simple: store each data entry in some indexable data
structure like a Vec, and then store just the index in a derived data structure.
To then perform an operation, first use the derived data structure
to efficiently find the data index, and then use the index to retrieve the
referenced data. No lifetimes needed—and you can even have cycles in the
derived data representation if you wish!
The indexmap crate, which provides a HashMap implementation where the
iteration order matches the map insertion order, provides a good example
of this pattern. The implementation has to store the keys in two places,
both in the map of keys to values and in the list of all the keys, but it obviously
doesn’t want to keep two copies in case the key type itself is large. So,
it uses index pointers. Specifically, it keeps all the key/value pairs in a single
Vec and then keeps a mapping from key hashes to Vec indexes. To iterate
over all the elements of the map, it just walks the Vec. To look up a given
key, it hashes that key, looks that hash up in the mapping, which yields the
key’s index in the Vec (the index pointer), and then uses that to get the key’s
value from the Vec.
The petgraph crate, which implements graph data structures and algorithms,
also uses this pattern. The crate stores one Vec of all node values
and another of all edge values and then only ever uses the indexes into
those Vecs to refer to a node or edge. So, for example, the two nodes associated
with an edge are stored in that edge simply as two u32s, rather than as
references or reference-counted values.
The trick lies in how you support deletions. To delete a data entry, you
first need to search for its index in all of the derived data structures and
remove the corresponding entries, and then you need to remove the data
from the root data store. If the root data store is a Vec, removing the entry
will also change the index of one other data entry (when using swap_remove),
so you then need to go update all the derived data structures to reflect the
new index for the entry that moved.
Drop Guards
Drop guards provide a simple but reliable way to ensure that a bit of code
runs even in the presence of panics, which is often essential in unsafe code.
An example is a function that takes a closure f: FnOnce and executes it under
mutual exclusion using atomics. Say the function uses compare_exchange (discussed
in Chapter 10) to set a Boolean from false to true, calls f, and then
sets the Boolean back to false to end the mutual exclusion. But consider
what happens if f panics—the function will never get to run its cleanup, and
no other call will be able to enter the mutual exclusion section ever again.
It’s possible to work around this using catch_unwind, but drop guards
provide an alternative that is often more ergonomic. Listing 13-3 shows
The Rust Ecosystem 235
how, in our current example, we can use a drop guard to ensure the
Boolean always gets reset.
fn mutex(lock: &AtomicBool, f: impl FnOnce()) {
// .. while lock.compare_exchange(false, true).is_err() ..
struct DropGuard<'a>(&'a AtomicBool);
impl Drop for DropGuard<'_> {
fn drop(&mut self) {
lock.store(true, Ordering::Release);
}
}
let _guard = DropGuard(lock);
f();
}
Listing 13-3: Using a drop guard to ensure code gets run after an unwinding panic
We introduce the local type DropGuard that implements Drop and place
the cleanup code in its implementation of Drop::drop. Any necessary state
can be passed in through the fields of DropGuard. Then, we construct an
instance of the guard type just before we call the function that might
panic, which is f here. When f returns, whether due to a panic or because
it returns normally, the guard is dropped, its destructor runs, the lock is
released, and all is well.
It’s important that the guard is assigned to a variable that is dropped
at the end of the scope, after the user-provided code has been executed.
This means that even though we never refer to the guard’s variable again, it
needs to be given a name, as let _ = DropGuard(lock) would drop the guard
immediately—before the user-provided code even runs!
NOTE Like catch_unwind, drop guards work only when panics unwind. If the code is compiled
with panic=abort, no code gets to run after the panic.
This pattern is frequently used in conjunction with thread locals,
when library code may wish to set the thread local state so that it’s valid
only for the duration of the execution of the closure, and thus needs to
be cleared afterwards. For example, at the time of writing, Tokio uses this
pattern to provide information about the executor calling Future::poll to
leaf resources like TcpStream without having to propagate that information
through function signatures that are visible to users. It’d be no good if the
thread local state continued to indicate that a particular executor thread
was active even after Future::poll returned due to a panic, so Tokio uses a
drop guard to ensure that the thread local state is reset.
NOTE You’ll often see Cell or Rc<RefCell> used in thread locals. This is because thread
locals are accessible only through shared references, since a thread might access a
thread local again that it is already referencing somewhere higher up in the call stack.
Both types provide interior mutability without incurring much overhead because
they’re intended only for single-threaded use, and so are ideal for this use case.
236 Chapter 13
Extension Traits
Extension traits allow crates to provide additional functionality to types
that implement a trait from a different crate. For example, the itertools
crate provides an extension trait for Iterator, which adds a number of convenient
shortcuts for common (and not so common) iterator operations. As
another example, tower provides ServiceExt, which adds several more ergonomic
operations to wrap the low-level interface in the Service trait from
tower-service.
Extension traits tend to be useful either when you do not control the
base trait, as with Iterator, or when the base trait lives in a crate of its own
so that it rarely sees breaking releases and thus doesn’t cause unnecessary
ecosystem splits, as with Service.
An extension trait extends the base trait it is an extension of (trait
ServiceExt: Service) and consists solely of provided methods. It also comes
with a blanket implementation for any T that implements the base trait
(impl<T> ServiceExt for T where T: Service {}). Together, these conditions
ensure that the extension trait’s methods are available on anything that
implements the base trait.
Crate Preludes
In Chapter 12, we talked about the standard library prelude that makes a
number of types and traits automatically available without you having to
write any use statements. Along similar lines, crates that export multiple
types, traits, or functions that you’ll often use together sometimes define
their own prelude in the form of a module called prelude, which re-exports
some particularly common subset of those types, traits, and functions.
There’s nothing magical about that module name, and it doesn’t get used
automatically, but it serves as a signal to users that they likely want to add
use somecrate::prelude::* to files that want to use the crate in question. The *
is a glob import and tells Rust to use all publicly available items from the indicated
module. This can save quite a bit of typing when the crate has a lot of
items you’ll usually need to name.
NOTE Items used through * have lower precedence than items that are used explicitly by
name. This is what allows you to define items in your own crate that overlap with
what’s in the standard library prelude without having to specify which one to use.
Preludes are also great for crates that expose a lot of extension traits,
since trait methods can be called only if the trait that defines them is in
scope. For example, the diesel crate, which provides ergonomic access to
relational databases, makes extensive use of extension traits so you can
write code like:
posts.filter(published.eq(true)).limit(5).load::<Post>(&connection)
This line will work only if all the right traits are in scope, which the prelude
takes care of.
The Rust Ecosystem 237
In general, you should be careful when adding glob imports to your
code, as they can potentially turn additions to the indicated module into
backward-incompatible changes. For example, if someone adds a new trait
to a module you glob-import from, and that new trait makes a method foo
available on a type that already had some other foo method, code that calls
foo on that type will no longer compile as the call to foo is now ambiguous.
Interestingly enough, while the existence of glob imports makes any module
addition a technically breaking change, the Rust RFC on API evolution
(RFC 1105; see https://rust-lang.github.io/rfcs/1105-api-evolution.html) does not
require a library to issue a new major version for such a change. The RFC
goes into great detail about why, and I recommend you read it, but the gist
is that minor releases are allowed to require minimally invasive changes to
dependents, like having to add type annotations in edge cases, because otherwise
a large fraction of changes would require new major versions despite
being very unlikely to actually break any consumers.
Specifically in the case of preludes, using glob imports is usually fine
when recommended by the vending crate, since its maintainers know that
their users will use glob imports for the prelude module and thus will take
that into account when deciding whether a change requires a major version
bump.
Staying Up to Date
Rust, being such a young language, is evolving rapidly. The language itself,
the standard library, the tooling, and the broader ecosystem are all still in
their infancy, and new developments happen every day. While staying on
top of all the changes would be infeasible, it’s worth your time to keep up
with significant developments so that you can take advantage of the latest
and greatest features in your projects.
For monitoring improvements to Rust itself, including new language
features, standard library additions, and core tooling upgrades, the official
Rust blog at https://blog.rust-lang.org/ is a good, low-volume place to start. It
mainly features announcements for each new Rust release. I recommend
you make a habit of reading these, as they tend to include interesting tidbits
that will slowly but surely deepen your knowledge of the language. To
dig a little deeper, I highly recommend reading the detailed changelogs
for Rust and Cargo as well (links can usually be found near the bottom of
each release announcement). The changelogs surface changes that weren’t
large enough to warrant a paragraph in the release notes but that may be
just what you need two weeks from now. For a less frequently updated news
source, check in on The Edition Guide at https://doc.rust-lang.org/edition-guide/,
which outlines what’s new in each Rust edition. Rust editions tend to be
released every three years.
NOTE Clippy is often able to tell you when you can take advantage of a new language or
standard library feature—always enable Clippy!
238 Chapter 13
If you’re curious about how Rust itself is developed, you may also want
to subscribe to the Inside Rust blog at https://blog.rust-lang.org/inside-rust/. It
includes updates from the various Rust teams, as well as incident reports,
larger change proposals, edition planning information, and the like. To get
involved in Rust development yourself—which I highly encourage, as it’s
a lot of fun and a great learning experience—you can check out the various
Rust working groups at https://www.rust-lang.org/governance/, which each
focus on improving a specific aspect of Rust. Find one that appeals to you,
check in with the group wherever it meets and ask how you may be able to
help. You can also join the community discussion about Rust internals over
at https://internals.rust-lang.org/; this is another great way to get insight into
the thought that goes into every part of Rust’s design and development.
As is the case for most programming languages, much of Rust’s value
is derived from its community. Not only do the members of the Rust community
constantly develop new work-saving crates and discover new Rustspecific
techniques and design patterns, but they also collectively and
continuously help one another understand, document, and explain how
to take best advantage of the Rust language. Everything I have covered in
this book, and much more, has already been discussed by the community
in thousands of comment threads, blog posts, and Twitter and Discord conversations.
Dipping into these discussions even just once in a while is almost
guaranteed to show you new things about a language feature, a technique,
or a crate that you didn’t already know.
The Rust community lives in a lot of places, but some good places to
start are the Users forum (https://users.rust-lang.org/), the Rust subreddit
(https://www.reddit.com/r/rust/), the Rust Community Discord (https://discord
.gg/rust-lang-community), and the Rust Twitter account (https://twitter.com/
rustlang). You don’t have to engage with all of these, or all of the time—
pick one you like the vibe of, and check in occasionally!
A great single location for staying up to date with ongoing developments
is the This Week in Rust blog (https://this-week-in-rust.org/), a “weekly summary
of [Rust’s] progress and community.” It links to official announcements and
changelogs as well as popular community discussions and resources, interesting
new crates, opportunities for contributions, upcoming Rust events, and
Rust job opportunities. It even lists interesting language RFCs and compiler
PRs, so this site truly has it all! Discerning what information is valuable to
you and what isn’t may be a little daunting, but even just scrolling through
and clicking occasional links that appear interesting is a good way to keep a
steady stream of new Rust knowledge trickling into your brain.
NOTE Want to look up when a particular feature landed on stable? Can I Use…
(https://caniuse.rs/) has you covered.
What Next?
So, you’ve read this book front to back, absorbed all the knowledge it
imparts, and are still hungry for more? Great! There are a number of other
The Rust Ecosystem 239
excellent resources out there for broadening and deepening your knowledge and understanding of Rust, and in this very final section I’ll give you a survey of some of my favorites so that you can keep learning. I’ve divided them into subsections based on how different people prefer to learn so that you can find resources that’ll work for you.
NOTE A challenge with learning on your own, especially in the beginning, is that progress
is hard to perceive. Implementing even the simplest of things can take an outsized
amount of time when you have to constantly refer to documentation and other
resources, ask for help, or debug to learn how some aspect of Rust works. All of that
non-coding work can make it seem like you’re treading water and not really improving. But you’re learning, which is progress in and of itself—it’s just harder to notice
and appreciate.
Learn by Watching
Watching experienced developers code is essentially a life hack to remedy
the slow starting phase of solo learning. It allows you to observe the process of designing and building while utilizing someone else’s experience.
Listening to experienced developers articulate their thinking and explain
tricky concepts or techniques as they come up can be an excellent alternative to struggling through problems on your own. You’ll also pick up a
variety of auxiliary knowledge like debugging techniques, design patterns,
and best practices. Eventually you will have to sit down and do things yourself—it’s the only way to check that you actually understand what you’ve
observed—but piggybacking on the experience of others will almost certainly make the early stages more pleasant. And if the experience is interactive, that’s even better!
So, with that said, here are some Rust video channels that I recommend:
Perhaps unsurprisingly, my own channel: https://www.youtube.com/c/
JonGjengset/. I have a mix of long-form coding videos and short(er) code-
based theory/concept explanation videos, as well as occasional videos
that dive into interesting Rust coding stories.
The Awesome Rust Streaming listing: https://github.com/jamesmunns/awesome-rust-streaming/. This resource lists a wide variety of developers
who stream Rust coding or other Rust content.
The channel of Tim McNamara, the author of Rust in Action: https://
www.youtube.com/c/timClicks/. Tim’s channel, like mine, splits its time
between implementation and theory, though Tim has a particular
knack for creative visual projects, which makes for fun viewing.
Jonathan Turner’s Systems with JT channel: https://www.youtube.com/c/
SystemswithJT/. Jonathan’s videos document their work on Nushell, their take on a “new type of shell,” providing a great sense of what it’s
like to work on a nontrivial existing codebase.
Ryan Levick’s channel: https://www.youtube.com/c/RyanLevicksVideos/. Ryan mainly posts videos that tackle particular Rust concepts and walks
240 Chapter 13
through them using concrete code examples, but he also occasionally
does implementation videos (like FFI for Microsoft Flight Simulator!)
and deep dives into how well-known crates work under the hood.
Given that I make Rust videos, it should come as no surprise that I am
a fan of this approach to teaching. But this kind of receptive or interactive
learning doesn’t have to come in the form of videos. Another great avenue
for learning from experienced developers is pair programming. If you have
a colleague or friend with expertise in a particular aspect of Rust you’d like
to learn, ask if you can do a pair-programming session with them to solve a
problem together!
Learn by Doing
Since your ultimate goal is to get better at writing Rust, there’s no substitute
for programming experience. No matter what or how many resources you
learn from, you need to put that learning into practice. However, finding a
good place to start can be tricky, so here I’ll give some suggestions.
Before I dive into the list, I want to provide some general guidance on
how to pick projects. First, choose a project that you care about, without
worrying too much whether others care about it. While there are plenty
of popular and established Rust projects out there that would love to have
you as a contributor, and it’s fun to be able to say “I contributed to the wellknown
library X,” your first priority must be your own interest. Without
concrete motivation, you’ll quickly lose steam and find contributing to be
a chore. The very best targets are projects that you use yourself and have
experienced problems with—go fix them! Nothing is more satisfying than
getting rid of a long-standing personal nuisance while also contributing
back to the community.
Okay, so back to project suggestions. First and foremost, consider contributing
to the Rust compiler and its associated tools. It’s a high-quality
codebase with good documentation and an endless supply of issues (you
probably know of some yourself), and there are several great mentors who
can provide outlines for how to approach solving issues. If you look through
the issue tracker for issues marked E-easy or E-mentor, you’ll likely find a
good candidate quickly. As you gain more experience, you can keep leveling
up to contribute to trickier parts.
If that’s not your cup of tea, I recommend finding something you use
frequently that’s written in another language and porting it to Rust—not
necessarily with the intention of replacing the original library or tool, but
just because the experience will allow you to focus on writing Rust without
having to spend too much time coming up with all the functionality yourself.
If it turns out well, the fact that it already exists suggests that someone
else also needed it, so there may be a wider audience for your port too! Data
structures and command-line tools often make for great porting subjects,
but find a niche that appeals to you.
Should you be more of a “build it from scratch” kind of person, I recommend
looking back at your own development experience so far and thinking
about similar code you’ve ended up writing in multiple projects (whether
The Rust Ecosystem 241
in Rust or in other languages). Such repetition tends to be a good signal
that something is reusable and could be turned into a library. If nothing
comes to mind, David Tolnay maintains a list of smaller utility crates that
other Rust developers have requested at https://github.com/dtolnay/request-forimplementation/
that may provide a source of inspiration. If you’re looking for
something more substantial and ambitious, there’s also the Not Yet Awesome
list at https://github.com/not-yet-awesome-rust/not-yet-awesome-rust/ that lists
things that should exist in Rust but don’t (yet).
Learn by Reading
Although the state of affairs is constantly improving, finding good Rust
reading material beyond the beginner level can still be tricky. Here’s a collection
of pointers to some of my favorite resources that continue to teach
me new things or serve as good references when I have particularly niche or
nuanced questions.
First, I recommend looking through the official virtual Rust books
linked from https://www.rust-lang.org/learn/. Some, like the Cargo book, are
more reference-like while others, like the Embedded book, are more guidelike,
but they’re all deep sources of solid technical information about their
respective topics. The Rustonomicon (https://doc.rust-lang.org/nomicon/), in particular,
is a lifesaver when you’re writing unsafe code.
Two more books that are worth checking out are the Guide to rustc
Development (https://rustc-dev-guide.rust-lang.org/) and the Standard Library
Developers Guide (https://std-dev-guide.rust-lang.org/). These are fantastic
resources if you’re curious about how the Rust compiler does what it does
or how the standard library is designed, or if you want some pointers before
you try your hand at contributing to Rust itself. The official Rust guidelines
are also a treasure trove of information; I’ve already mentioned the Rust
API Guidelines (https://rust-lang.github.io/api-guidelines/) in the book, but a
Rust Unsafe Code Guidelines Reference is also available (https://rust-lang.github
.io/unsafe-code-guidelines/), and by the time you read this book there may
be more.
NOTE One of the resources listed at https://www.rust-lang.org/learn/ is the Rust
Reference, which is essentially a full specification for the Rust language. While parts
of it are quite dry, like the exact grammar used for parsing or basics about the inmemory
representations of the primitive types, some of it is fascinating reading, like
the section on type layout and the enumeration of behavior considered undefined.
There are also a number of unofficial virtual Rust books that are
enormously valuable collections of experience and knowledge. The Little
Book of Rust Macros (https://veykril.github.io/tlborm/), for example, is indispensable
if you want to write nontrivial declarative macros, and The Rust
Performance Book (https://nnethercote.github.io/perf-book/) is filled with tips and
tricks for improving the performance of Rust code both at the micro and
the macro level. Other great resources include the Rust Fuzz Book (https://
rust-fuzz.github.io/book/), which explores fuzz testing in more detail, and
242 Chapter 13
the Rust Cookbook (https://rust-lang-nursery.github.io/rust-cookbook/), which suggests
idiomatic solutions to common programming tasks. There’s even a
resource for finding more books, The Little Book of Rust Books (https://lborb.
github.io/book/unofficial.html)!
If you prefer more hands-on reading, the Tokio project has published
mini-redis (https://github.com/tokio-rs/mini-redis/), an incomplete but idiomatic
implementation of a Redis client and server that’s extremely well documented
and specifically written to serve as a guide to writing asynchronous
code. If you’re more of a data structures person, Learn Rust with Entirely Too
Many Linked Lists (https://rust-unofficial.github.io/too-many-lists/) is an enlightening
and fun read that gets into lots of gnarly details about ownership and
references. If you’re looking for something closer to the hardware, Philipp
Oppermann’s Writing an OS in Rust (https://os.phil-opp.com/) goes through
the whole operating system stack in great detail while teaching you good
Rust patterns in the process. I also highly recommend Amos’s collection of
articles (https://fasterthanli.me/tags/rust/) if you want a wide sampling of interesting
deep dives written in a conversational style.
When you feel more confident in your Rust abilities and need more of
a quick reference than a long tutorial, I’ve found the Rust Language Cheat
Sheet (https://cheats.rs/) great for looking things up quickly. It also provides
very nice visual explanations for most topics, so even if you’re looking up
something you’re not intimately familiar with already, the explanations are
pretty approachable.
And finally, if you want to put all of your Rust understanding to the
test, go give David Tolnay’s Rust Quiz (https://dtolnay.github.io/rust-quiz/) a try.
There are some real mind-benders in there, but each question comes with
a thorough explanation of what’s going on, so even if you get one wrong,
you’ll have learned from the experience!
Learn by Teaching
My experience has been that the best way to learn something well and
thoroughly, by far, is to try to teach it to others. I have learned an enormous
amount from writing this book, and I learn new things every time
I make a new Rust video or podcast episode. So, I wholeheartedly recommend
that you try your hand at teaching others about some of the things
you’ve learned from reading this book or that you learn from here on out.
It can take whatever form you prefer: in person, writing a blog post, tweeting,
making a video or podcast, or giving a talk. The important thing is
that you try to convey your newfound knowledge in your own words to
someone who doesn’t already understand the topic—in doing so, you also
give back to the community so that the next you that comes along has a
slightly easier time getting up to speed. Teaching is a humbling and deeply
educational experience, and I cannot recommend it highly enough.
NOTE Whether you’re looking to teach or be taught, make sure to visit Awesome Rust
Mentors (https://rustbeginners.github.io/awesome-rust-mentors/).
The Rust Ecosystem 243
Summary
In this chapter, we’ve covered Rust beyond what exists in your local workspace.
We surveyed useful tools, libraries, and Rust features; looked at how
to stay up to date as the ecosystem continues to evolve; and then discussed
how you can get your hands dirty and contribute back to the ecosystem
yourself. Finally, we discussed where you can go next to continue your Rust
journey now that this book has reached its end. And with that, there’s little
more to do than to declare:
}
